<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="rRxEGVoWcPPII6Ts-TzSapPe0XCHwuoRZsTwX9kKSEM">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wcjb.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="为什么在LLM模型预训练时，计算交叉熵损失函数时需要loss mask   在自回归训练中，模型需要根据前\(n-1\)个Token预测第\(n\)个Token。此时，输入序列为\(X &#x3D; [x_1,x_2,...,x_{n-1}]\)，目标序列为\(Y&#x3D; [y_1,y_2,...,y_m] \quad m \geq n\)，掩码规则为仅对目标序列中与当前预测位置匹配的Token计算损失（如第\">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM之模型训练Tricks">
<meta property="og:url" content="https://wcjb.github.io/posts/15775e35/index.html">
<meta property="og:site_name" content="老学庵">
<meta property="og:description" content="为什么在LLM模型预训练时，计算交叉熵损失函数时需要loss mask   在自回归训练中，模型需要根据前\(n-1\)个Token预测第\(n\)个Token。此时，输入序列为\(X &#x3D; [x_1,x_2,...,x_{n-1}]\)，目标序列为\(Y&#x3D; [y_1,y_2,...,y_m] \quad m \geq n\)，掩码规则为仅对目标序列中与当前预测位置匹配的Token计算损失（如第\">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wcjb.github.io/posts/15775e35/norm.png">
<meta property="og:image" content="https://wcjb.github.io/posts/15775e35/LLmbuild.png">
<meta property="article:published_time" content="2025-03-31T09:11:09.000Z">
<meta property="article:modified_time" content="2025-04-08T03:38:37.774Z">
<meta property="article:author" content="殉道者">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="模型训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wcjb.github.io/posts/15775e35/norm.png">

<link rel="canonical" href="https://wcjb.github.io/posts/15775e35/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>LLM之模型训练Tricks | 老学庵</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="老学庵" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">老学庵</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">天行健，君子以自强不息；地势坤，君子以厚德载物！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-rss">

    <a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

  
    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wcjb.github.io/posts/15775e35/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.png">
      <meta itemprop="name" content="殉道者">
      <meta itemprop="description" content="幼而学者,如日出之光；老而学者，如秉烛夜行，犹贤乎瞑目而无见者也！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老学庵">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM之模型训练Tricks
        </h1>

        <div class="post-meta">

          
            <i class="fa fa-thumb-tack"></i>
            <font color=7D26CD>置顶</font>
            <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-31 17:11:09" itemprop="dateCreated datePublished" datetime="2025-03-31T17:11:09+08:00">2025-03-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-08 11:38:37" itemprop="dateModified" datetime="2025-04-08T11:38:37+08:00">2025-04-08</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>
<h2 id="为什么在llm模型预训练时计算交叉熵损失函数时需要loss-mask">为什么在LLM模型预训练时，计算交叉熵损失函数时需要loss
mask</h2>
<p>  在自回归训练中，模型需要根据前<span class="math inline">\(n-1\)</span>个Token预测第<span class="math inline">\(n\)</span>个Token。此时，输入序列为<span class="math inline">\(X =
[x_1,x_2,...,x_{n-1}]\)</span>，目标序列为<span class="math inline">\(Y=
[y_1,y_2,...,y_m] \quad m \geq
n\)</span>，掩码规则为仅对目标序列中与当前预测位置匹配的Token计算损失（如第<span class="math inline">\(i\)</span>个预测位置仅计算<span class="math inline">\(y_i\)</span>的损失,其他位置（包括填充Token）设为0，不参与损失计算.
其主要作用如下：</p>
<ul>
<li><p>忽略无效Token</p>
<p>  预训练数据通常包含填充（Padding）标记或特殊符号（如<eos>），这些位置的真实值对模型无意义。若不加掩码，模型会错误学习生成这些无效Token，导致训练效率下降。</eos></p></li>
<li><p>聚焦有效生成</p>
<p>  通过掩码选择性计算损失，模型仅关注实际需要生成的Token（如自回归任务中预测下一个Token），避免被无关信息干扰。</p></li>
<li><p>保持损失归一化</p>
<p>  掩码确保不同批次、不同长度序列的损失计算公平，避免填充Token拉低整体损失值</p></li>
</ul>
<h2 id="深度学习中常用归一化技术">深度学习中常用归一化技术</h2>
<p>  归一化技术在深度学习中常用于对输入数据进行标准化处理的方法，归一化技术本质上是将输入空间映射到一个规范化空间的过程，此映射的常见形式如下：</p>
<ul>
<li><p>Min-Max</p>
<p><span class="math display">\[\bar{x} = \dfrac{x-x_{min}}{x_{max} -
x_{min}}\]</span></p></li>
</ul>
<p>  将数据缩放到<span class="math inline">\([0,1]\)</span>范围，适用于数据分布已知且无异常值的请况。</p>
<ul>
<li><p>Z-Score</p>
<p><span class="math display">\[\bar{x} = \dfrac{x -
\mu}{\delta}\]</span></p></li>
</ul>
<p><span class="math display">\[\mu = \frac{1}{m} \sum_{i=1}^{m} x_i,
\quad \sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2\]</span>
  将数据转换为均值为0，标准差为1的分布，适用于数据服从正态分布的情况</p>
<p>  在实际使用中，会针对性的做一些调整以适应不同任务，不同场景，主要有Batch
Norm、Layer Norm、RMS Norm等：</p>
<figure>
<img src="norm.png" alt="Norm">
<figcaption aria-hidden="true">Norm</figcaption>
</figure>
<ul>
<li><p>Batch Norm</p>
<p>  对每个特征通道（如图像的RGB通道）在批量数据上计算均值和方差，通过标准化和平移/缩放参数调整分布，公式如下：
<span class="math display">\[y = \gamma \cdot
\dfrac{x-\mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</span></p></li>
<li><p>Layer Norm</p>
<p>  对单个样本的所有特征维度（如序列模型的每个token向量）计算均值和方差，实现跨样本的归一化，公式同上，只是针对的输入不同</p></li>
<li><p>RMS Norm</p>
<p>  LayerNorm的变体，仅计算均方根（RMS）替代均值和方差，去除平移参数<span class="math inline">\(\beta\)</span>，简化计算，公式如下：</p>
<p><span class="math display">\[y = \gamma \cdot
\dfrac{x-\mu}{\sqrt{RMS(x)^2 + \epsilon}}\]</span></p>
<p>其中 <span class="math display">\[RMS(x) = \sqrt{\frac{1}{N}\sum_{i =
1}^Nx_i^2}\]</span></p></li>
</ul>
<p>在深度学习中归一化技术有如下意义：</p>
<ul>
<li><p>提高训练速度</p>
<ul>
<li>归一化可以使不同特征的数值范围相近，避免某些特征值过大或过小，影响梯度更新，导致训练过程缓慢。</li>
<li>通过减少梯度下降的震荡，使优化过程更稳定，加快收敛速度。</li>
</ul></li>
<li><p>防止梯度消失或梯度爆炸</p>
<ul>
<li>归一化可以保持数据分布的稳定性，防止某些层的输入值过大或过小，从而避免梯度消失或梯度爆炸问题，尤其是在深层网络中。</li>
</ul></li>
<li><p>提高模型的泛化能力</p>
<ul>
<li>归一化可以减少特征对不同尺度的依赖，使模型对输入数据的变化更具鲁棒性，提高泛化能力，降低过拟合风险。</li>
</ul></li>
<li><p>减少不同特征之间的影响</p>
<ul>
<li>由于不同特征的尺度不同，模型可能会过度依赖某些大数值特征，导致训练过程不稳定。归一化后，各特征的权重分布更加均衡，提高模型的稳定性。</li>
</ul></li>
</ul>
<p>归一化技术对比如下：</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 30%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>典型应用场景</th>
<th>核心优势</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch Norm</td>
<td>图像分类（CNN）、小批量训练场景</td>
<td>加速收敛、降低初始化敏感度</td>
<td>对批量大小敏感，无法处理序列数据（如NLP）</td>
</tr>
<tr>
<td>Layer Norm</td>
<td>自然语言处理（如BERT、GPT）、RNN、长序列模型</td>
<td>适应动态批量、保持序列位置不变性</td>
<td>计算开销较大，对噪声敏感</td>
</tr>
<tr>
<td>RMS Norm</td>
<td>超大规模语言模型（如LLaMA、DeepSeek）、高精度工业检测</td>
<td>计算高效、数值稳定性强、减少梯度爆炸风险</td>
<td>无法处理均值敏感任务（如某些CV任务）</td>
</tr>
</tbody>
</table>
<p>  需要注意的是，在训练和推理阶段，Batch
Norm的处理方式是截然不同的。</p>
<ul>
<li><p>训练阶段</p>
<p>  在训练阶段，BN会计算每个批次的均值和方差，同时维护全局均值和全局方差，并使用滑动平均来进行更新：</p>
<p><span class="math display">\[
\begin{cases}\mu_g = \alpha \mu_g + (1 - \alpha)\mu_{b} \\ \delta_g^2 =
\alpha \delta_g^2 + (1-\alpha)\delta_b^2
\end{cases}\]</span> 其中<span class="math inline">\(\alpha\)</span>为平滑系数。</p></li>
<li><p>推理阶段</p>
<p>  在推理阶段，会直接使用训练阶段得到的全局均值<span class="math inline">\(\mu_g\)</span>，全局方差<span class="math inline">\(\delta_g^2\)</span>进行数据归一化，确保输入数据特征分布的一致性，同时保证模型在推理阶段的稳定性。</p></li>
</ul>
<blockquote>
<p>  因此在进行模型推理时，若使用原生pytorch，在推理前，需调用模型的eval()方法，将模型设置为推理模式，以确保在使用BN的模型推理场景，能正确使用训练阶段的全局均值和方差。对于使用非BN标准化方法的场景，如LN，RN，IN，GN等在训练和推理时都是采用各自当前的均值和方差，但是也要在推理前将模型设置为推理模式，因为Dropout算法在训练和推理阶段也是完全不同的，在推理阶段，Dropout是静默的，随机冻结参数的比例为0。若使用第三方推理框架，如OnnxRuntime、TensorRT等，在使用torch到处模型参数前，也需要将模型设置为推理模式。</p>
</blockquote>
<h2 id="大模型构建流程">大模型构建流程</h2>
<p>  根据OpenAI联合创始人Andrej Karpathy在微软Build
2023大会上公开的信息，OpenAI使用的大模型构建流程如下图所示，主要包括以下四个阶段：
<img src="LLmbuild.png"></p>
<ul>
<li><p>预训练</p>
<p>  预训练阶段使用的主要模型结构是自回归（Autoregressive）模型GPT[1]，此外还有自编码（Autoencoding）模型BERT[2]、编码器-解码器模型BART[3]，以及融合上述三种方法的自回归填空（Autoregressive
Blank Infiling）模型GLM（General Language Model）[3]。</p></li>
<li><p>有监督微调（SFT）</p>
<p>  有监督微调分为全参数微调和轻量级参数微调。而常用的轻量级微调方法有LORA[4]、AdaLoRA[5]、QLoRA[6]等。</p></li>
<li><p>Reward Model</p>
<p>  练Reward
Model的目标是构建一个文本质量对比模型，用于对两个结果之间的优劣进行对比。Reward
Model主要用于下阶段的强化学习，强化学习会使最终生成的回答获得更高的Reward。InstructGPT中给出了如何构建Reward
Model[7]，可以阅读原论文。</p></li>
<li><p>人工反馈强化学习</p>
<p>  强化学习阶段根据数十万用户给的prompt，利用前一阶段训练的Reward
Model，使用近段策略优化PPO[8]对语言模型继续进行微调，使得模型更好的学习人类偏好。</p></li>
<li><p>参考文献</p>
<p><a href="https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">[1]
GPT-1技术报告 Improving Language Understanding by Generative
Pre-Training</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">[2]
GPT-2技术报告：Language Models are Unsupervised Multitask
Learners</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2005.14165.pdf">[3]
GPT-3技术报告：Language Models are Few-Shot Learners</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.02155.pdf">[4]
Instruct GPT技术报告：Training language models to follow instructions
with human feedback</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//cdn.openai.com/papers/gpt-4.pdf">[5]
GPT-4技术报告：GPT-4 Technical Report</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2302.13971.pdf">[6]
Llama-1技术报告：LLaMA: Open and Efficient Foundation Language
Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2307.09288.pdf">[7]
Llama-2技术报告：Llama 2: Open Foundation and Fine-Tuned Chat
Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2309.10305.pdf">[8]
Baichuan-2技术报告：Baichuan 2: Open Large-scale Language Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2103.10360.pdf">[9]
GLM技术报告：GLM: General Language Model Pretraining with Autoregressive
Blank</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.04652.pdf">[10]
InfillingYi-34B技术报告：Yi: Open Foundation Models by 01.AI</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf">[11]
Qwen-1技术报告：QWEN TECHNICAL REPORT</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//qwenlm.github.io/blog/qwen1.5/">[12]
Qwen1.5技术报告大模型综述：A Survey of Large Language
Models</a></p></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"> <i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/" rel="tag"> <i class="fa fa-tag"></i> 模型训练</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/781628f/" rel="prev" title="机器视觉之基本概念">
      <i class="fa fa-chevron-left"></i> 机器视觉之基本概念
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/1.5775e+40/" rel="next" title="LLM之位置编码">
      LLM之位置编码 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8llm%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83%E6%97%B6%E8%AE%A1%E7%AE%97%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%97%B6%E9%9C%80%E8%A6%81loss-mask"><span class="nav-number">1.</span> <span class="nav-text">为什么在LLM模型预训练时，计算交叉熵损失函数时需要loss
mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E5%BD%92%E4%B8%80%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="nav-number">2.</span> <span class="nav-text">深度学习中常用归一化技术</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">大模型构建流程</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="殉道者"
      src="/uploads/avatar.png">
  <p class="site-author-name" itemprop="name">殉道者</p>
  <div class="site-description" itemprop="description">幼而学者,如日出之光；老而学者，如秉烛夜行，犹贤乎瞑目而无见者也！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wcjb" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wcjb" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/wcjbyjx@gmail.com" title="E-Mail → wcjbyjx@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
      
      
        <div class="links-of-blogroll motion-element links-of-blogroll-block">
          <div class="links-of-blogroll-title">
            <!-- modify icon to fire by szw -->
            <i class="fa fa-history fa-" aria-hidden="true"></i>
            近期文章
          </div>
          <ul class="links-of-blogroll-list">
            
            
              <li class="recent_posts_li">
                <a href="/" title="" target="_blank"></a>
              </li>
            
              <li class="recent_posts_li">
                <a href="/" title="" target="_blank"></a>
              </li>
            
              <li class="recent_posts_li">
                <a href="/" title="" target="_blank"></a>
              </li>
            
              <li class="recent_posts_li">
                <a href="/" title="" target="_blank"></a>
              </li>
            
              <li class="recent_posts_li">
                <a href="/" title="" target="_blank"></a>
              </li>
            
          </ul>
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>



      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">殉道者</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">24k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:26</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
-->

    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'Ov23lijO0ypdZO6PpImE',
      clientSecret: '95acc21cc04144cce9fca13a46ca80ef78f03d8d',
      repo        : 'wcjb.github.io',
      owner       : 'wcjb',
      admin       : ['wcjb'],
      id          : 'c2f8051f4a6b1bdd19a548077c794982',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
