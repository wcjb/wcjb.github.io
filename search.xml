<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LLM之KVCache</title>
    <url>/posts/e16fb9d/</url>
    <content><![CDATA[<p>  在Transformer架构的生成式模型（如GPT系列）中，推理过程需要逐个生成token。传统方式每次生成都需重新计算所有历史token的注意力信息，导致计算复杂度达到O(n²)。<strong>KV
Cache技术通过缓存历史token的Key和Value矩阵</strong>，将后续生成的计算复杂度降至O(n)，实现推理加速，本质上是一种用空间来换取时间的加速策略。</p>
<span id="more"></span>
<h3 id="自注意力机制的冗余计算">1.自注意力机制的冗余计算</h3>
<p>  GPT系列模型是经典的自回归模型，在推理过程中，一次推理只输出一个token，输出token会与输入tokens拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。以GPT2为例，代码如下:
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel, GPT2Tokenizer</span><br><span class="line"></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(<span class="string">&quot;gpt2&quot;</span>, torchscript=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">tokenizer = GPT2Tokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">in_text = <span class="string">&quot;Hello, how  &quot;</span></span><br><span class="line">in_tokens = torch.tensor(tokenizer.encode(in_text))</span><br><span class="line"></span><br><span class="line">token_eos = torch.tensor([<span class="number">198</span>]) </span><br><span class="line">out_token = <span class="literal">None</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">while</span> out_token != token_eos:</span><br><span class="line">        logits, _ = model(in_tokens)</span><br><span class="line">        out_token = torch.argmax(logits[-<span class="number">1</span>, :], dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        in_tokens = torch.cat((in_tokens, out_token), <span class="number">0</span>)</span><br><span class="line">        text = tokenizer.decode(in_tokens)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;step <span class="subst">&#123;i&#125;</span> input: <span class="subst">&#123;text&#125;</span>&#x27;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">out_text = tokenizer.decode(in_tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Input: <span class="subst">&#123;in_text&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Output: <span class="subst">&#123;out_text&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure> ---</p>
<p>执行输出结果如下： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">step 0  input: Hello, how   </span><br><span class="line">step 1  input: Hello, how   <span class="keyword">do</span></span><br><span class="line">step 2  input: Hello, how   <span class="keyword">do</span> you</span><br><span class="line">step 3  input: Hello, how   <span class="keyword">do</span> you feel</span><br><span class="line">step 4  input: Hello, how   <span class="keyword">do</span> you feel about</span><br><span class="line">step 5  input: Hello, how   <span class="keyword">do</span> you feel about the</span><br><span class="line">step 6  input: Hello, how   <span class="keyword">do</span> you feel about the new</span><br><span class="line">step 7  input: Hello, how   <span class="keyword">do</span> you feel about the new season</span><br><span class="line">step 8  input: Hello, how   <span class="keyword">do</span> you feel about the new season of</span><br><span class="line">step 9  input: Hello, how   <span class="keyword">do</span> you feel about the new season of The</span><br><span class="line">step 10 input: Hello, how   <span class="keyword">do</span> you feel about the new season of The Walking</span><br><span class="line">step 11 input: Hello, how   <span class="keyword">do</span> you feel about the new season of The Walking Dead</span><br><span class="line">step 12 input: Hello, how   <span class="keyword">do</span> you feel about the new season of The Walking Dead?</span><br><span class="line">step 13 input: Hello, how   <span class="keyword">do</span> you feel about the new season of The Walking Dead?</span><br><span class="line"></span><br><span class="line">Input: Hello, how  </span><br><span class="line">Output: Hello, how   <span class="keyword">do</span> you feel about the new season of The Walking Dead?</span><br></pre></td></tr></table></figure></p>
<h3 id="kv-cache工作流程">2. KV Cache工作流程</h3>
<p>  显然上面的计算过程中存在大量重复计算，在上面的推理过程中，每step内，输入一个token序列，经过Embedding层将输入token序列变为一个三维张量<span class="math inline">\([b, s,
h]\)</span>，经过一通计算，最后经logits层将计算结果映射至词表空间，输出张量维度为<span class="math inline">\([b, s,
vocab_{size}]\)</span>。当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens，反复多次。可以看出第轮输入<span class="math inline">\(n\)</span>数据只比第<span class="math inline">\(n-1\)</span>轮输入数据新增了一个token，其他全部相同！因此第<span class="math inline">\(n\)</span>轮推理时必然包含了第<span class="math inline">\(n-1\)</span>轮的部分计算。   基于上面的观察，KV
Cache策略缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果，避免了大量的冗余计算。采用KV
Cache后，自回归模型的推理策略分为两个阶段：</p>
<ul>
<li><p>预填充阶段</p>
<p>  发生在计算第一个输出token过程中，此时Cache为空，需要为每个
transformer layer 计算并保存key cache和value
cache，在输出token时Cache完成填充；FLOPs同KV
Cache关闭一致，存在大量gemm操作，推理速度慢。</p></li>
<li><p>使用KV Cache阶段</p>
<p>  发生在计算非首个token的过程中，这时Cache已缓存历史kv，每轮推理只需读取Cache，同时将当前轮计算出的新的Key、Value追加写入至Cache；FLOPs降低，gemm变为gemv操作，推理速度相对第一阶段变快，这时属于Memory-bound类型计算。</p></li>
</ul>
<p>需要注意的时，由于自注意力机制的特性，在推理时仅需计算当前token的Q，并与缓存中的K/V进行注意力计算。</p>
<p>同样以GPT2为例，代码实现如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT2Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;cached_k&#x27;</span>, torch.zeros(...))</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;cached_v&#x27;</span>, torch.zeros(...))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, layer_past=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 计算当前Q/K/V</span></span><br><span class="line">        q, k, v = <span class="variable language_">self</span>.qkv_proj(x).split(...)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 存在历史缓存</span></span><br><span class="line">            past_k, past_v = layer_past</span><br><span class="line">            k = torch.cat([past_k, k], dim=-<span class="number">2</span>)  <span class="comment"># 序列维度拼接</span></span><br><span class="line">            v = torch.cat([past_v, v], dim=-<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> (k, v) <span class="keyword">if</span> use_cache <span class="keyword">else</span> output </span><br></pre></td></tr></table></figure>   KV
Cache是典型的以空间换时间的策略，显然其通过缓存历史KV的方式，避免了重复计算，使得自回归模型的推理速度提升了2～3倍，尤其在处理长序列时效果显著，显著减少了对计算资源的要求，同时缓存完整的历史Key和Value，确保模型在长文本生成中保持语义一致性。</p>
<h3 id="改良与优化">3.改良与优化</h3>
<p>  当然，由于KV
Cache的机制，随着序列长度的增加，缓存的Key和Value矩阵会占用大量显存。以一个层数为<span class="math inline">\(L\)</span>，隐藏层维度为<span class="math inline">\(H\)</span>，输入长度为<span class="math inline">\(S\)</span>，输出长度为<span class="math inline">\(N\)</span>的模型，KV Cache的显存占用为: <span class="math display">\[M = 4 * B * L * H * (S + N)\]</span>
其中B为批量大小。为了缓解推理过程中的显存压力，针对性的提出了一些优化策略：</p>
<ul>
<li><p>注意力结构改进</p>
<p>  采用多查询注意力(Multi-Query
Attention,MQA)或分组查询注意力(Grouped-Query
Attention,GQA)减少Key和Value矩阵的参数量。 ￼</p></li>
<li><p>滑动窗口机制</p>
<p>  仅缓存最近L个token的Key和Value，结合初始token的“注意力锚点”稳定计算，减少显存占用。
￼</p></li>
<li><p>量化压缩</p>
<p>  对KV Cache进行低精度存储（如FP8），进一步减少显存需求。 ￼</p></li>
</ul>
<p>  除了上面几种策略外，基于FlashAttention的优化注意力计算访存模式的技巧也进一步提升推理效率，KV
Cache通过空间换时间的策略，已成为大模型推理优化的标配技术。</p>
<h3 id="参考">参考</h3>
<p><a href="https://zhuanlan.zhihu.com/p/662498827">[1]
大模型推理加速：看图学KV Cache</a> <a href="https://huggingface.co/docs/transformers/main/en/main_classes/cache">[1]
Transformers官方文档</a></p>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>-- Trasformer -- 大模型推理 -- 性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM之位置编码</title>
    <url>/posts/15775ef36/</url>
    <content><![CDATA[<p>  在自然语言处理的任务中，位置编码是帮助模型理解序列中每个单词或词片（token）在序列中的位置的一种机制。这是因为像
Transformer
这样的架构本质上是无序的，它们通过注意力机制处理整个序列中的所有元素，但并不能直接感知这些元素在序列中的顺序。因此，我们需要将序列的位置信息编码进模型的输入，编码的方式有绝对位置编码和相对位置编码。</p>
<span id="more"></span>
<h2 id="绝对位置编码">绝对位置编码</h2>
<p>  绝对位置编码是一种直接为序列中的每个位置分配一个固定的编码。这种编码只依赖于位置本身，与具体的序列内容无关，以Sinusoidal位置编码为例，实现公式如下：</p>
<p><span class="math display">\[
\begin{cases}
P_{(p, 2i)} &amp;= \sin\left(\dfrac{p}{10000^{2i/d}}\right) \\
P_{(p, 2i+1)}&amp; = \cos\left(\dfrac{p}{10000^{2i/d}}\right)
\end{cases}
\]</span> 其中，<span class="math inline">\(d\)</span>是编码向量的维度,<span class="math inline">\(p\)</span>为位置，<span class="math inline">\(i\)</span>为维度索引。</p>
<p>  从公式来看，绝对位置编码实现简单，采用数学中的三角函数来编码位置信息，不含元素之间的相对位置关系。并且，由于其在编码关系在初期就已经固定，不会随着训练数据的变化而改变，因此无法很好的处理序列长度超过训练时最大长度的情况，扩展能力有限</p>
<h2 id="相对位置编码">相对位置编码</h2>
<p>  相对位置编码的目标是直接表示序列中不同元素之间的相对距离，而不是它们的绝对位置。这使得模型能够更好地捕捉元素之间的依赖关系。在注意力机制中，编码对序列中每一对元素的相对距离。例如，在Transformer的自注意力机制中，可以将注意力权重通过相对位置的偏置（bias）来修正，常用公式如下：
<span class="math display">\[
A_{ij} = Q_i \cdot K_j^T + b_{ij}
\]</span> 其中，<span class="math inline">\(b_{ij}\)</span>
是由相对位置决定的偏置。</p>
<p>  相较绝对位置编码，在自然语言处理中，相邻单词的关系往往比绝对位置更重要，例如主语和谓语的距离，因此这中编码方式会更符合语言特性。即使序列长度超出训练范围，相对位置编码仍然能很好地扩展，因为它关注的是元素之间的相对关系，而非绝对位置。相对位置编码能有效捕捉上下文中词之间的相对关系，对不同长度的序列具有更好的泛化能力。不过其实现比绝对位置复杂，在长序列位置编码中计算复杂度较高。</p>
<h2 id="旋转位置编码">旋转位置编码</h2>
<p>  <code>绝对位置编码具有实现简单、计算速度快等优点，而相对位置编码则直接地体现了相对位置信号，跟我们的直观理解吻合，实际性能往往也更好。由此可见，如果可以通过绝对位置编码的方式实现相对位置编码，那么就是“集各家之所长”、“鱼与熊掌兼得”了。Sinusoidal位置编码隐约做到了这一点，但并不够好。</code></p>
<p>  上段摘子RoPE的作者苏剑林的博客，其描述<code>旋转式位置编码（Rotary Position Embedding，RoPE）”，是一种配合Attention机制能达到“绝对位置编码的方式实现相对位置编码”的设计，而也正因为这种设计，它还是目前唯一一种可用于线性Attention的相对位置编码。</code></p>
<p>  RoPE的 self-attention操作流程如下：对于 token
序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量，然后对每个
token 位置都计算对应的旋转位置编码，接着对每个 token 位置的 query 和 key
向量的元素按照 两两一组 应用旋转变换，最后再计算 query 和 key
之间的内积得到 self-attention 的计算结果。总体流程如下图：</p>
<p><img src="RoPE.png"></p>
<p>RoPE的代码实现如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 预计算旋转位置编码（Positional Cis）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_pos_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span> = <span class="built_in">int</span>(<span class="params"><span class="number">32</span> * <span class="number">1024</span></span>), theta: <span class="built_in">float</span> = <span class="number">1e6</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算词向量元素两两分组之后，每组词向量的旋转角度</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成 token 序列索引 t = [0, 1,..., seq_len-1]</span></span><br><span class="line">    t = torch.arange(end, device=freqs.device)  <span class="comment"># type: ignore</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># freqs.shape = [seq_len, dim // 2] </span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()  <span class="comment"># type: ignore</span></span><br><span class="line">    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="comment"># 构造复数</span></span><br><span class="line">    <span class="keyword">return</span> pos_cis</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params">xq, xk, pos_cis</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unite_shape</span>(<span class="params">pos_cis, x</span>):</span><br><span class="line">        ndim = x.ndim</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">        <span class="keyword">assert</span> pos_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">        shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)]</span><br><span class="line">        <span class="keyword">return</span> pos_cis.view(*shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># xq的原始维度为[batch_size,sep_len,n_heads,head_dim],为进行RoPE计算，head_dim需为偶数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># xq.shape = [batch_size, seq_len, dim]</span></span><br><span class="line">    <span class="comment"># xq_.shape = [batch_size, seq_len, dim // 2, 2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造为复数</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 维度对齐</span></span><br><span class="line">    pos_cis = unite_shape(pos_cis, xq_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 复数乘法进行旋转变换</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(<span class="number">3</span>)   <span class="comment"># 执行复数乘法做旋转变换，使用flatten(3)，将最后的两个维度合并，恢复原始张量维度结构</span></span><br><span class="line">    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="参考文献">参考文献</h2>
<p><a href="https://kexue.fm/archives/8265">[1]
Transformer升级之路：2、博采众长的旋转式位置编码</a></p>
<p><a href="https://papers.cool/arxiv/2104.09864">[2] RoFormer: Enhanced
Transformer with Rotary Position Embedding</a></p>
<p><a href="https://www.zhihu.com/tardis/zm/art/647109286?source_id=1003">[3]
十分钟读懂旋转编码（RoPE)</a></p>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>— 位置编码 - LLM - 模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM之余弦退火学习率</title>
    <url>/posts/19c3723a/</url>
    <content><![CDATA[<p>  学习率这个概念在非线性优化中经常出现，在深度学习中模型在反向传播阶段严重依赖于损失函数梯度的链式传播，为了更好的控制参数更新的步长，引入了学习率的概念：
<span class="math display">\[
w_{new} = w_{old} - \eta \ \Delta J(w)
\]</span></p>
<span id="more"></span>
<p>  其中的<span class="math inline">\(\eta\)</span>为学习率，直观来说，非线性优化的过程就像下山，学习率就是步长，步子迈的太大或太小都不太理想。当步子迈得的太大了，你可能就直接跨过山脚，对应到模型训练中，就是学习率太高，每次更新参数时，跳过了最小值，导致在最优值附近来回振荡，无法收敛;步子太小，则需要走很久，才能到达山脚，对应到模型训练中，就是学习率太低，模型收敛的速度很慢，需要较长时间的训练才能收敛到最优值。举这个经典的下山的例子是为了说明学习率在模型训练中的重要意义。
  在深度学习的模型训练中学习率有很多使用策略，包括不限于：</p>
<ul>
<li>固定学习率</li>
</ul>
<p>  顾名思义就是在模型训练过程中使用固定学习率来进行模型训练，这样会难以平衡初期的收敛速度和后期的精细调整，一般很少使用，只在一些简单问题上应用。</p>
<ul>
<li>阶梯式衰减学习率（step decay）</li>
</ul>
<p>  指学习率在模型训练的特定步骤中衰减，会导致学习率变化不平滑，模型训练不稳定。</p>
<ul>
<li>指数衰减学习率（exponential decay）</li>
</ul>
<p>  指学习率在模型训练过程中按指数函数衰减，由于指数函数的高增长特性，前期往往能快速收敛，但是后期由于学习率衰减过快，导致训练停滞，收敛速度很慢。</p>
<ul>
<li>线性预热+线性衰减</li>
</ul>
<p>  指学习率在模型训练过程中先线性增加再线性较少，此方案在大语言模型的预训练被广泛使用，缺点在于是线性变化，变化不太平滑。</p>
<ul>
<li>余弦退火学习率（Cosine Annealing）</li>
</ul>
<p>  余弦退火学习率的公式如下： <span class="math display">\[
\eta_{t} = \eta_{min} + \frac{1}{2}(\eta_{max} -
\eta_{min})(1+\cos(\frac{t}{T}\pi))
\]</span>   其中，<span class="math inline">\(\eta_{t}\)</span>是第<span class="math inline">\(t\)</span>步的学习率，<span class="math inline">\(\eta_{min}\)</span>是训练过程中能接受的最小学习率，<span class="math inline">\(\eta_{max}是最大学习率\)</span>，<span class="math inline">\(t\)</span>为当前的训练步树，<span class="math inline">\(T\)</span>该epoch中的总训练步数。从公式可以看出，在训练的开始阶段，学习率解决最大值，模型的收敛速度较快，到达一定训练步数后，学些率开始减少，模型会在更小的参数空间内搜索最优值，随着训练步数的增长，学习率逐渐逼近最小值，模型在最优值附近的参数空间内搜索。</p>
<p>  同时学习率是按照余弦曲线的方式进行更新，变化平滑的同时，不会出现变化幅度不大。目前基于余弦退火学习率衍生出一些更精细的学习率调度算法，如：</p>
<ul>
<li><p>带热重启的余弦退火</p>
<p>  学习率在训练过程中周期性回到最大值，然后再次降低，公式如下：</p>
<p><span class="math display">\[
  \eta_{t} = \eta_{min} + \frac{1}{2}(\eta_{max} -
\eta_{min})(1+\cos(\frac{T_{cur}}{T_i}\pi))
  \]</span>   其中，<span class="math inline">\(T_{cur}\)</span>是自上次热重启之后的训练步数，<span class="math inline">\(T_i\)</span>是当前周期的总步数。这种学习率调度方式有助于跳出局部最优解，探索更广阔的参数空间</p></li>
<li><p>带预热的余弦退火</p>
<p>  带预热的余弦退火通过两阶段学习率调整实现：</p>
<ul>
<li><p>​预热阶段</p>
<p>训练初期学习率从较小值线性或非线性增长至预设的最大值，避免随机初始化权重下的大幅度参数更新引发振荡。</p></li>
<li><p>余弦退火阶段</p>
<p>  预热完成后，学习率按余弦函数周期性衰减，从最大值逐渐降至最小值，数学公式如下：
<span class="math display">\[
  \eta_{t}=
  \begin{cases}
  \eta_{start} + \dfrac{t}{T_{warmup}}(\eta_{max} -
\eta_{start})    \qquad 0 \leq t &lt; T_{warmup} \\
  \eta_{max} + \frac{1}{2}(\eta_{max} -
\eta_{min})(1+cos(\frac{t}{T_{cos}}\pi))  \qquad t \geq T_{warmup}
  \end{cases}
  \]</span> <span class="math inline">\(T_{warmup}\)</span>为预热步数，<span class="math inline">\(T_{cos}\)</span>为退火周期</p></li>
</ul></li>
<li><p>循环余弦退火（cyclical cosine annealing）</p>
<p>  顾名思义，就是在模型训练过程中，学习率的变化在余弦退火的基础上存在多个余弦周期，通过重启（Restart）机制恢复学习率至最大值，形成多周期探索-利用循环，可以平衡梯度在参数空间的探索与利用，避免剧烈波动。</p></li>
</ul>
<p>  余弦退火学习率策略通过平滑地调整学习率，帮助模型在训练初期快速收敛，在训练后期精细调整参数，从而找到更好的局部最优解。它的提出和发展极大地推动了深度学习模型训练的稳定性和效率，已成为现代深度学习训练的标准配置之一。</p>
<p>  虽然它不是万能的解决方案，但在大多数深度学习任务中，特别是大型语言模型的训练中，余弦退火都能提供显著的性能提升。通过理解其工作原理和适用边界，我们可以更好地利用这一强大工具，提高模型训练效果。</p>
<blockquote>
<p>编程实现</p>
</blockquote>
<ul>
<li><p>demo 1 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">from torch.optim.lr_scheduler <span class="keyword">import</span> CosineAnnealingLR</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"># 创建模型和优化器</span><br><span class="line">model = <span class="built_in">MyModel</span>()</span><br><span class="line">optimizer = torch.optim.<span class="built_in">Adam</span>(model.<span class="built_in">parameters</span>(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"># 创建余弦退火调度器</span><br><span class="line"># T_max是半个周期的长度，通常设为总训练步数</span><br><span class="line"># eta_min是最小学习率</span><br><span class="line">scheduler = <span class="built_in">CosineAnnealingLR</span>(optimizer, T_max=<span class="number">1000</span>, eta_min=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line"># 在训练循环中使用</span><br><span class="line"><span class="keyword">for</span> epoch in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch in data_loader:</span><br><span class="line">        # 训练步骤</span><br><span class="line">        optimizer.<span class="built_in">zero_grad</span>()</span><br><span class="line">        outputs = <span class="built_in">model</span>(inputs)</span><br><span class="line">        loss = <span class="built_in">criterion</span>(outputs, targets)</span><br><span class="line">        loss.<span class="built_in">backward</span>()</span><br><span class="line">        optimizer.<span class="built_in">step</span>()</span><br><span class="line">        </span><br><span class="line">        # 更新学习率</span><br><span class="line">        scheduler.<span class="built_in">step</span>()</span><br></pre></td></tr></table></figure></p></li>
<li><p>demo 2 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">import</span> math</span></span><br><span class="line"><span class="function">def <span class="title">get_cosine_lr</span><span class="params">(current_step, total_steps, max_lr, min_lr)</span>:</span></span><br><span class="line"><span class="function">    <span class="string">&quot;&quot;</span><span class="string">&quot;计算当前步骤的余弦退火学习率&quot;</span><span class="string">&quot;&quot;</span></span></span><br><span class="line"><span class="function">    return min_lr + <span class="number">0.5</span> * (max_lr - min_lr) * (<span class="number">1</span> + math.cos(math.pi * current_step / total_steps))</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># 在训练循环中使用</span></span><br><span class="line"><span class="function">max_lr =</span> <span class="number">0.001</span></span><br><span class="line">min_lr = <span class="number">0.0001</span></span><br><span class="line">total_steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step in <span class="built_in">range</span>(total_steps):</span><br><span class="line">    # 计算当前学习率</span><br><span class="line">    current_lr = <span class="built_in">get_cosine_lr</span>(step, total_steps, max_lr, min_lr)</span><br><span class="line">    </span><br><span class="line">    # 更新优化器的学习率</span><br><span class="line">    <span class="keyword">for</span> param_group in optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">&#x27;lr&#x27;</span>] = current_lr</span><br><span class="line">    </span><br><span class="line">    # 正常的训练步骤</span><br><span class="line">    optimizer.<span class="built_in">zero_grad</span>()</span><br><span class="line">    outputs = <span class="built_in">model</span>(inputs)</span><br><span class="line">    loss = <span class="built_in">criterion</span>(outputs, targets)</span><br><span class="line">    loss.<span class="built_in">backward</span>()</span><br><span class="line">    optimizer.<span class="built_in">step</span>()</span><br></pre></td></tr></table></figure></p></li>
<li><p>demo 3</p>
<p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">from torch.optim.lr_scheduler <span class="keyword">import</span> CosineAnnealingWarmRestarts</span><br><span class="line"></span><br><span class="line"># 创建带热重启的余弦退火调度器</span><br><span class="line"># T_0是第一次重启前的迭代次数</span><br><span class="line"># T_mult是控制重启周期如何变化的因子</span><br><span class="line">scheduler = <span class="built_in">CosineAnnealingWarmRestarts</span>(</span><br><span class="line">    optimizer, </span><br><span class="line">    T_0=<span class="number">1000</span>,  # 第一个周期的长度</span><br><span class="line">    T_mult=<span class="number">2</span>,  # 每次重启后周期长度翻倍</span><br><span class="line">    eta_min=<span class="number">0.0001</span>  # 最小学习率</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 在训练循环中使用</span><br><span class="line"><span class="keyword">for</span> epoch in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch in data_loader:</span><br><span class="line">        # 训练步骤</span><br><span class="line">        optimizer.<span class="built_in">zero_grad</span>()</span><br><span class="line">        outputs = <span class="built_in">model</span>(inputs)</span><br><span class="line">        loss = <span class="built_in">criterion</span>(outputs, targets)</span><br><span class="line">        loss.<span class="built_in">backward</span>()</span><br><span class="line">        optimizer.<span class="built_in">step</span>()</span><br><span class="line">        </span><br><span class="line">        # 更新学习率</span><br><span class="line">        scheduler.<span class="built_in">step</span>()</span><br></pre></td></tr></table></figure></p></li>
</ul>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>学习率</tag>
        <tag>模型训练</tag>
        <tag>余弦退火</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM之模型训练Tricks</title>
    <url>/posts/15775ef35/</url>
    <content><![CDATA[<p>  学习LLM过程中的经典策略记录。 <span id="more"></span></p>
<h2 id="causal-mask">Causal mask</h2>
<p>  Causal
mask是自注意力机制中的一种掩码。在自注意力机制中，每个位置的输出都依赖于序列中所有位置都输入，包括当前位置之后的位置。这种机制在解码器中特别常见，因为解码器在生成输出时必须按顺序依次生成。但是在预测时，未来显然是不可见的，所以需要在每个位置处将未来的位置的信息屏蔽，这种屏蔽的方式也就是Causal
mask。它保证了解码器在生成时，只是基于历史信息，而不是未来的信息。</p>
<p>  通常情况下，Causal
mask是一个二维矩阵，该矩阵对角线以下的元素全为1，表示允许当前位置之前的信息流动，而对角线以上的元素都为0，表示屏蔽了当前位置之后的信息。在使用
Transformer
架构进行序列到序列任务时，常见的做法是在解码器的自注意力层中应用这种
causal mask，以确保生成的序列是自左向右生成的。 <img src="CausalMask.png"></p>
<h2 id="为什么在llm模型预训练时计算交叉熵损失函数时需要loss-mask">为什么在LLM模型预训练时，计算交叉熵损失函数时需要loss
mask</h2>
<p>  在自回归训练中，模型需要根据前<span class="math inline">\(n-1\)</span>个Token预测第<span class="math inline">\(n\)</span>个Token。此时，输入序列为<span class="math inline">\(X =
[x_1,x_2,...,x_{n-1}]\)</span>，目标序列为<span class="math inline">\(Y=
[y_1,y_2,...,y_m] \quad m \geq
n\)</span>，掩码规则为仅对目标序列中与当前预测位置匹配的Token计算损失（如第<span class="math inline">\(i\)</span>个预测位置仅计算<span class="math inline">\(y_i\)</span>的损失,其他位置（包括填充Token）设为0，不参与损失计算.
其主要作用如下：</p>
<ul>
<li><p>忽略无效Token</p>
<p>  预训练数据通常包含填充（Padding）标记或特殊符号（如<eos>），这些位置的真实值对模型无意义。若不加掩码，模型会错误学习生成这些无效Token，导致训练效率下降。</eos></p></li>
<li><p>聚焦有效生成</p>
<p>  通过掩码选择性计算损失，模型仅关注实际需要生成的Token（如自回归任务中预测下一个Token），避免被无关信息干扰。</p></li>
<li><p>保持损失归一化</p>
<p>  掩码确保不同批次、不同长度序列的损失计算公平，避免填充Token拉低整体损失值</p></li>
</ul>
<h2 id="深度学习中常用归一化技术">深度学习中常用归一化技术</h2>
<p>  归一化技术在深度学习中常用于对输入数据进行标准化处理的方法，归一化技术本质上是将输入空间映射到一个规范化空间的过程，此映射的常见形式如下：</p>
<ul>
<li><p>Min-Max</p>
<p><span class="math display">\[\bar{x} = \dfrac{x-x_{min}}{x_{max} -
x_{min}}\]</span></p></li>
</ul>
<p>  将数据缩放到<span class="math inline">\([0,1]\)</span>范围，适用于数据分布已知且无异常值的请况。</p>
<ul>
<li><p>Z-Score</p>
<p><span class="math display">\[\bar{x} = \dfrac{x -
\mu}{\delta}\]</span></p></li>
</ul>
<p><span class="math display">\[\mu = \frac{1}{m} \sum_{i=1}^{m} x_i,
\quad \sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2\]</span>
  将数据转换为均值为0，标准差为1的分布，适用于数据服从正态分布的情况</p>
<p>  在实际使用中，会针对性的做一些调整以适应不同任务，不同场景，主要有Batch
Norm、Layer Norm、RMS Norm等:</p>
<p><img src="norm.png"></p>
<ul>
<li><p>Batch Norm</p>
<p>  对每个特征通道（如图像的RGB通道）在批量数据上计算均值和方差，通过标准化和平移/缩放参数调整分布，公式如下：</p>
<p><span class="math display">\[y = \gamma \cdot
\dfrac{x-\mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</span></p></li>
<li><p>Layer Norm</p>
<p>  对单个样本的所有特征维度（如序列模型的每个token向量）计算均值和方差，实现跨样本的归一化，公式同上，只是针对的输入不同</p></li>
<li><p>RMS Norm</p>
<p>  LayerNorm的变体，仅计算均方根（RMS）替代均值和方差，去除平移参数<span class="math inline">\(\beta\)</span>，简化计算，公式如下：</p>
<p><span class="math display">\[y = \gamma \cdot
\dfrac{x-\mu}{\sqrt{RMS(x)^2 + \epsilon}}\]</span></p>
<p>其中 <span class="math display">\[RMS(x) = \sqrt{\frac{1}{N}\sum_{i =
1}^Nx_i^2}\]</span></p></li>
</ul>
<p>在深度学习中归一化技术有如下意义：</p>
<ul>
<li><p>提高训练速度</p>
<ul>
<li>归一化可以使不同特征的数值范围相近，避免某些特征值过大或过小，影响梯度更新，导致训练过程缓慢。</li>
<li>通过减少梯度下降的震荡，使优化过程更稳定，加快收敛速度。</li>
</ul></li>
<li><p>防止梯度消失或梯度爆炸</p>
<ul>
<li>归一化可以保持数据分布的稳定性，防止某些层的输入值过大或过小，从而避免梯度消失或梯度爆炸问题，尤其是在深层网络中。</li>
</ul></li>
<li><p>提高模型的泛化能力</p>
<ul>
<li>归一化可以减少特征对不同尺度的依赖，使模型对输入数据的变化更具鲁棒性，提高泛化能力，降低过拟合风险。</li>
</ul></li>
<li><p>减少不同特征之间的影响</p>
<ul>
<li>由于不同特征的尺度不同，模型可能会过度依赖某些大数值特征，导致训练过程不稳定。归一化后，各特征的权重分布更加均衡，提高模型的稳定性。</li>
</ul></li>
</ul>
<p>归一化技术对比如下：</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 30%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>典型应用场景</th>
<th>核心优势</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch Norm</td>
<td>图像分类（CNN）、小批量训练场景</td>
<td>加速收敛、降低初始化敏感度</td>
<td>对批量大小敏感，无法处理序列数据（如NLP）</td>
</tr>
<tr>
<td>Layer Norm</td>
<td>自然语言处理（如BERT、GPT）、RNN、长序列模型</td>
<td>适应动态批量、保持序列位置不变性</td>
<td>计算开销较大，对噪声敏感</td>
</tr>
<tr>
<td>RMS Norm</td>
<td>超大规模语言模型（如LLaMA、DeepSeek）、高精度工业检测</td>
<td>计算高效、数值稳定性强、减少梯度爆炸风险</td>
<td>无法处理均值敏感任务（如某些CV任务）</td>
</tr>
</tbody>
</table>
<p>  需要注意的是，在训练和推理阶段，Batch
Norm的处理方式是截然不同的。</p>
<ul>
<li><p>训练阶段</p>
<p>  在训练阶段，BN会计算每个批次的均值和方差，同时维护全局均值和全局方差，并使用滑动平均来进行更新：</p>
<p><span class="math display">\[
\begin{cases}\mu_g = \alpha \mu_g + (1 - \alpha)\mu_{b} \\ \delta_g^2 =
\alpha \delta_g^2 + (1-\alpha)\delta_b^2
\end{cases}\]</span> 其中<span class="math inline">\(\alpha\)</span>为平滑系数。</p></li>
<li><p>推理阶段</p>
<p>  在推理阶段，会直接使用训练阶段得到的全局均值<span class="math inline">\(\mu_g\)</span>，全局方差<span class="math inline">\(\delta_g^2\)</span>进行数据归一化，确保输入数据特征分布的一致性，同时保证模型在推理阶段的稳定性。</p></li>
</ul>
<blockquote>
<p>  因此在进行模型推理时，若使用原生pytorch，在推理前，需调用模型的eval()方法，将模型设置为推理模式，以确保在使用BN的模型推理场景，能正确使用训练阶段的全局均值和方差。对于使用非BN标准化方法的场景，如LN，RN，IN，GN等在训练和推理时都是采用各自当前的均值和方差，但是也要在推理前将模型设置为推理模式，因为Dropout算法在训练和推理阶段也是完全不同的，在推理阶段，Dropout是静默的，随机冻结参数的比例为0。若使用第三方推理框架，如OnnxRuntime、TensorRT等，在使用torch到处模型参数前，也需要将模型设置为推理模式。</p>
</blockquote>
<h2 id="大模型构建流程">大模型构建流程</h2>
<p>  根据OpenAI联合创始人Andrej Karpathy在微软Build
2023大会上公开的信息，OpenAI使用的大模型构建流程如下图所示，主要包括以下四个阶段：</p>
<p><img src="LLMBuild.png"></p>
<ul>
<li><p>预训练</p>
<p>  预训练阶段使用的主要模型结构是自回归（Autoregressive）模型GPT[1]，此外还有自编码（Autoencoding）模型BERT[2]、编码器-解码器模型BART[3]，以及融合上述三种方法的自回归填空（Autoregressive
Blank Infiling）模型GLM（General Language Model）[3]。</p></li>
<li><p>有监督微调（SFT）</p>
<p>  有监督微调分为全参数微调和轻量级参数微调。而常用的轻量级微调方法有LORA[4]、AdaLoRA[5]、QLoRA[6]等。</p></li>
<li><p>Reward Model</p>
<p>  练Reward
Model的目标是构建一个文本质量对比模型，用于对两个结果之间的优劣进行对比。Reward
Model主要用于下阶段的强化学习，强化学习会使最终生成的回答获得更高的Reward。InstructGPT中给出了如何构建Reward
Model[7]，可以阅读原论文。</p></li>
<li><p>人工反馈强化学习</p>
<p>  强化学习阶段根据数十万用户给的prompt，利用前一阶段训练的Reward
Model，使用近段策略优化PPO[8]对语言模型继续进行微调，使得模型更好的学习人类偏好。</p></li>
<li><p>参考文献</p>
<p><a href="https://link.zhihu.com/?target=https%3A//www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">[1]
GPT-1技术报告 Improving Language Understanding by Generative
Pre-Training</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">[2]
GPT-2技术报告：Language Models are Unsupervised Multitask
Learners</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2005.14165.pdf">[3]
GPT-3技术报告：Language Models are Few-Shot Learners</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2203.02155.pdf">[4]
Instruct GPT技术报告：Training language models to follow instructions
with human feedback</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//cdn.openai.com/papers/gpt-4.pdf">[5]
GPT-4技术报告：GPT-4 Technical Report</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2302.13971.pdf">[6]
Llama-1技术报告：LLaMA: Open and Efficient Foundation Language
Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2307.09288.pdf">[7]
Llama-2技术报告：Llama 2: Open Foundation and Fine-Tuned Chat
Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2309.10305.pdf">[8]
Baichuan-2技术报告：Baichuan 2: Open Large-scale Language Models</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2103.10360.pdf">[9]
GLM技术报告：GLM: General Language Model Pretraining with Autoregressive
Blank</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2403.04652.pdf">[10]
InfillingYi-34B技术报告：Yi: Open Foundation Models by 01.AI</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf">[11]
Qwen-1技术报告：QWEN TECHNICAL REPORT</a></p>
<p><a href="https://link.zhihu.com/?target=https%3A//qwenlm.github.io/blog/qwen1.5/">[12]
Qwen1.5技术报告大模型综述：A Survey of Large Language
Models</a></p></li>
</ul>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>模型训练</tag>
      </tags>
  </entry>
  <entry>
    <title>傅立叶变换</title>
    <url>/posts/684f5543/</url>
    <content><![CDATA[<p>  希尔伯特空间是一个完备的内积空间，其标准正交函数系，直观来看就是向量空间中<code>基</code>的延伸。其为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。下文中我们将通过希尔伯特空间的标准正交函数系推导周期函数和有限区间上函数的傅立叶级数表示，并进一步推出傅里叶积分来表示无穷区间的非周期函数，最后引入复数形式的傅立叶积分，引出傅立叶变换。在这一系列推导中，鉴于篇幅，主动略去了一些比较关键的部分，比如<span class="math inline">\(f(x)\)</span>可积性及级数收敛性的讨论，有兴趣的读者可以在了解大致原理后，进行细致的理论推导以作补充。为了便于理解希尔伯特空间的概念，引用维基百科中的定义：
<span id="more"></span></p>
<blockquote>
<p>  在数学里，希尔伯特空间（英语：Hilbert
space）即完备的内积空间，也就是一个带有内积的完备矢量空间。内积的构造推广了欧几里得空间的距离和角的概念；完备则确保了其上所有的柯西序列会收敛到此空间里的一点，从而微积分中的许多概念都可以推广到希尔伯特空间中。希尔伯特空间为基于任意正交坐标系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一,另外希尔伯特空间也是量子力学的重要数学基础之一。</p>
</blockquote>
<h1 id="希尔伯特空间">希尔伯特空间</h1>
<p>  若无限维<code>酉空间</code><span class="math inline">\(V\)</span>中每个基本序列收敛于V中的元素，则称<span class="math inline">\(V\)</span>是<code>完备</code>的。一个完备的无限维酉空间称为希尔伯特空间又称为<span class="math inline">\(H\)</span>空间。在<span class="math inline">\(n\)</span>维空间中矢量被定义为<span class="math inline">\((f_1,f_2,\cdots,f_n)\)</span>,在无限维空间中矢量被定义为<span class="math inline">\(t\)</span>从<span class="math inline">\(a\)</span>变换到<span class="math inline">\(b\)</span>的函数<span class="math inline">\(f(t)\)</span>。在希尔伯特空间中矢量的加法和乘法定义为函数的加法与函数和数的乘法。</p>
<ul>
<li><p>内积</p>
<p>​ 对于<span class="math inline">\(f,g \in
H\)</span>,则二者的内积定义为： <span class="math display">\[
(f,g) = \int_{a}^{b}f(t)g(t)d t
\tag{1}
\]</span></p></li>
<li><p>度量</p>
<ul>
<li>对于<span class="math inline">\(f(t)\in H\)</span>,其长度定义为：
<span class="math display">\[
L_H=\sqrt{\int_{a}^{b}f^2(t)dt}\tag{2}
\]</span></li>
</ul></li>
<li><p>若<span class="math inline">\(f(t),g(t)\in
H\)</span>则二者之间的距离定义为： <span class="math display">\[
D_H=\sqrt{\int_{a}^{b}(f(t)-g(t))^2dt}\tag{3}
\]</span> 直观来看就是两个函数的均方差，就是以均方差来作为<span class="math inline">\(H\)</span>空间的距离的度量。</p></li>
<li><p>若<span class="math inline">\(f(t),g(t)\in
H\)</span>,则二者的夹角定义为： <span class="math display">\[
\Omega =
\arccos{\frac{\int_{a}^{b}f(t)g(t)dt}{\sqrt{\int_{a}^{b}f^2(t)}\sqrt{\int_{a}^{b}g^2(t)dt}}}\tag{4}
\]</span></p></li>
<li><p>正交函数系</p>
<ul>
<li><p>若非零矢量<span class="math inline">\(f,g\in
H\)</span>的内积<span class="math inline">\((f,g)=0\)</span>，则由<span class="math inline">\(H\)</span>空间的夹角定义公式可知<span class="math inline">\(\Omega=\frac{\pi}{2}\)</span>,此时称矢量<span class="math inline">\(f,g\)</span>正交。</p></li>
<li><p>若<span class="math inline">\(f_i \in
H,i=1,\cdots,n\)</span>且两两正交，设 <span class="math display">\[
f(x) = \sum\limits_{i=1}^{n}f_{i}(x)\tag{5}
\]</span> 则有 <span class="math display">\[
L_{f(x)}^2=\sum\limits_{i=1}^{n}L_{f_{i}(x)}^2\tag{6}
\]</span> 由<span class="math inline">\(H\)</span>空间长度和正交的定义可推出： <span class="math display">\[
L_{f(x)}^{2}=\int_{a}^bf^2(x)dx=\int_{a}^b[\sum\limits_{i=0}^{n}f_i(x)]^2dx=\int_{a}^b[\sum\limits_{i=0}^{n}
f_i^2(x)]dx=\sum\limits_{i=0}^{n}[\int_{a}^bf_i^2(x)dx]\tag{7}
\]</span> 上述积分都是指勒贝格积分有意义。</p></li>
<li><p>若函数系<span class="math inline">\(\phi_i(x)\in
H,i=1,\cdots,n,\cdots\)</span>中任意两个函数相互正交，即 <span class="math display">\[
\int_{a}^{b}\phi_i(x)\phi_j(x)dx=0（i\neq j）\tag{8}
\]</span> 则称这个函数系为正交函数系，若还满足 <span class="math display">\[
\int_{a}^{b}\phi_{k}^2(x)dx=1（k \in 1,\cdots,n,\cdots）\tag{9}
\]</span> 则称此函数系为标准正交系。</p></li>
</ul></li>
<li><p>依标准正交函数系的分解</p>
<p>​ 若在<span class="math inline">\(H\)</span>空间中给定一个完备的标准正交函数系<span class="math inline">\(\phi_1(x),\phi_2(x),\cdots,\phi_n(x),\cdots\)</span>（即不可能再加一个不恒为零的函数与系中的一切函数正交），则对于任意函数<span class="math inline">\(f(x)\)</span>都可根据这个标准正交函数系展开成级数（平均收敛）：</p>
<p><span class="math display">\[
f(x)=\sum\limits_{i=1}^{\infty}a_i\phi_i(x)=a_1\phi_1(x)+a_2\phi_2(x)+\cdots+a_n\phi_n(x)+\cdots\tag{10}
\]</span> <span class="math inline">\(a_n\)</span>为<span class="math inline">\(\phi_n(x)\)</span>在这个标准正交函数系上的投影：</p></li>
</ul>
<p><span class="math display">\[
  a_n = (f,\phi_n)=\int_{a}^{b}f(x)\phi_n(x)dx（n=1,2,\cdots）\tag{11}
\]</span> 很容易证明 <span class="math display">\[
  \int_{a}^{b}f^2(x)dx=\sum\limits_{i=1}^{\infty}a^2_i\tag{12}
\]</span> 代表<span class="math inline">\(H\)</span>空间中矢量的长度平方等于该矢量在完备的标准正交系中的矢量上的投影平方和。</p>
<h1 id="傅立叶级数">傅立叶级数</h1>
<p>  希尔伯特空间是有限维欧几里得空间的推广，与欧几里得空间相同，希尔伯特空间也是内积空间，也有距离和角的概念，并且不同于欧几里得空间，<span class="math inline">\(H\)</span>空间具有<code>完备性</code>：希尔伯特空间内的所有的<code>柯西列</code>会收敛到一点。因此微积分中的大部分概念可无障碍推广至希尔伯特空间中。希尔伯特空间提供了一个很强大理论工具：<strong>对于<span class="math inline">\(H\)</span>空间中的任意函数<span class="math inline">\(f(x)\)</span>都可以由<span class="math inline">\(H\)</span>空间中完备的标准正交函数系展开成级数</strong>。也就是说，可以通过一个标准正交函数系去逼近一个任意函数（这些函数都是基于希尔伯特空间的）。</p>
<h2 id="三角函数系">三角函数系</h2>
<p>  基于上述的讨论，接下来讨论如何通过一个标准正交函数系来展开任意函数。基于标准正交函数的定义，直观来看，三角函数系似乎完美的切合，对于三角函数系
<span class="math display">\[
1,cos(x),sin(x),\cdots,cos(kx),sin(kx),\cdots\tag{13}
\]</span> 由于 <span class="math display">\[
\int_{-\pi}^{\pi}1*cos(kx)\mathrm{d}x=\frac{2}{k}\int_{0}^{\pi}cos(kx)\mathrm{d}
kx =\frac{2}{k}sin(kx)|^{\pi}_{0}=0（k=1,2,3,\dots）\tag{14}
\]</span> 又由奇函数的性质直接得到（15）（16）定积分等式成立： <span class="math display">\[
\int_{-\pi}^{\pi}1*sin(kx)\mathrm{d}x=0（k=1,2,3,\dots）\tag{15}
\]</span></p>
<p><span class="math display">\[
\int_{-\pi}^{\pi}sin(kx)cos(nx)\mathrm{d}x=0（k，n=1,2,3,\dots；k\neq
n）\tag{16}
\]</span></p>
<p>又： <span class="math display">\[
\int_{-\pi}^{\pi}cos(kx)cos(nx)\mathrm{d}x\overset{积化和差}{=}\int_{-\pi}^{\pi}\frac{1}{2}[cos(k+n)x+cos(k-n)x]\mathrm{d}x=0（k，n=1,2,3,\dots；k\neq
n）\tag{17}
\]</span> 同理得到： <span class="math display">\[
\int_{-\pi}^{\pi}sin(kx)sin(nx)\mathrm{d}x\overset{积化和差}{=}\int_{-\pi}^{\pi}-\frac{1}{2}[cos(k+n)x-cos(k-n)x]\mathrm{d}x=0（k，n=1,2,3,\dots；k\neq
n）\tag{18}
\]</span> 于是得到三角函数系（13）在<span class="math inline">\([-\pi,\pi]\)</span>是一个正交函数系，但是，细心的读者可能发现，三角函数系（13）并不是一个标准正交函数系，为此基于（13）构造三角函数系
<span class="math display">\[
\frac{1}{\sqrt{2\pi}},\frac{cos(x)}{\sqrt{\pi}},\frac{sin(x)}{\sqrt{\pi}},\cdots,\frac{cos(kx)}{\sqrt{\pi}},\frac{sin(kx)}{\sqrt{\pi}},\cdots\tag{19}
\]</span>
显然，三角函数系（20）是一个正交函数系，下面进一步证明该函数系是一个标准正交函数系，则只需证明对于该函数系任意函数有（9）式成立即可：
<span class="math display">\[
\int_{-\pi}^{\pi}(\frac{1}{\sqrt{2\pi}})^2\mathrm{d}x=\frac{1}{2\pi}x|_{-\pi}^{\pi}=1\tag{20}
\]</span></p>
<p><span class="math display">\[
\int_{-\pi}^{\pi}(\frac{cos(kx)}{\sqrt{\pi}})^2\mathrm{d}x=\int_{-\pi}^{\pi}\frac{cos^2(kx)}{\pi}\mathrm{d}x\overset{三角降幂公式}{=}\int_{-\pi}^{\pi}\frac{1+cos(2kx)}{2\pi}\mathrm{d}x=(\frac{1}{2\pi}x+\frac{1}{4k\pi}sin(k\pi))｜_{-\pi}^{\pi}=1\\\tag{21}
\]</span></p>
<p><span class="math display">\[
\int_{-\pi}^{\pi}(\frac{sin(kx)}{\sqrt{\pi}})^2\mathrm{d}x=\int_{-\pi}^{\pi}\frac{sin^2(kx)}{\pi}\mathrm{d}x\overset{三角降幂公式}{=}\int_{-\pi}^{\pi}\frac{1-cos(2kx)}{2\pi}\mathrm{d}x=(\frac{1}{2\pi}x-\frac{1}{4k\pi}sin(k\pi))｜_{-\pi}^{\pi}=1\tag{22}
\]</span></p>
<p>由（20）（21）（22）三式可得，三角函数系（19）为希尔伯特空间下的标准正交函数系。则任意定义在<span class="math inline">\([-\pi,\pi]\)</span>的函数<span class="math inline">\(f(x)\)</span>有： <span class="math display">\[
f(x)=c_0\frac{1}{\sqrt{2\pi}}+a_0\frac{cos(x)}{\sqrt{\pi}}+b_0\frac{sin(x)}{\sqrt{\pi}}+\dots+a_k\frac{cos(kx)}{\sqrt{\pi}}+b_k\frac{sin(kx)}{\sqrt{\pi}}+\dots\tag{23}
\]</span> 整理得： <span class="math display">\[
f(x)=c_0\frac{1}{\sqrt{2\pi}}+\sum_{i=1}^{\infty}a_i\frac{cos(ix)}{\sqrt{\pi}}+\sum_{i=0}^{\infty}b_i\frac{sin(ix)}{\sqrt{\pi}}=c_0\frac{1}{\sqrt{2\pi}}+\sum_{i=1}^{\infty}(a_i\frac{cos(ix)}{\sqrt{\pi}}+b_i\frac{sin(ix)}{\sqrt{\pi}})\tag{24}
\]</span> 此时，只需确定系数<span class="math inline">\(c_0,a_i,b_i（i=1,2,3,\dots）\)</span>即可得到<span class="math inline">\(f(x)\)</span>在在<span class="math inline">\([-\pi,\pi]\)</span>的级数展开形式：</p>
<blockquote>
<p><span class="math inline">\(c_0\)</span>:</p>
<p>  对等式（24）两边同时求积分得到： <span class="math display">\[
\int_{-\pi}^{\pi}f(x)\mathrm{d}x =
\int_{-\pi}^{\pi}c_0\frac{1}{\sqrt{2\pi}}\mathrm{d}x+\int_{-\pi}^{\pi}\sum_{i=1}^{\infty}(a_i\frac{cos(ix)}{\sqrt{\pi}}+b_i\frac{sin(ix)}{\sqrt{\pi}})\mathrm{d}x\tag{25}
\]</span> 结合（14）（15）（16）得到： <span class="math display">\[
\int_{-\pi}^{\pi}f(x)\mathrm{d}x =
\int_{-\pi}^{\pi}c_0\frac{1}{\sqrt{2\pi}}\mathrm{d}x+0=c_0\sqrt{2\pi}\tag{26}
\]</span> 求得: <span class="math display">\[
c_0 = \frac{1}{\sqrt{2\pi}}\int_{-\pi}^{\pi}f(x)\mathrm{d}x\tag{27}
\]</span> <span class="math inline">\(a_i\)</span>:</p>
<p>  结合（21）式，对等式（24）两边同乘<span class="math inline">\(cos(jx)\)</span>,得到： <span class="math display">\[
f(x)cos(jx)=c_0\frac{1}{\sqrt{2\pi}}cos(jx)+\sum_{i=1}^{\infty}(a_i\frac{cos(ix)cos(jx)}{\sqrt{\pi}}+b_i\frac{sin(ix)cos(jx)}{\sqrt{\pi}})（j=1,2,3,\dots）\tag{28}
\]</span> 同样对等式（30）两边同时求积分得到： <span class="math display">\[
\int_{-\pi}^{\pi}f(x)cos(jx)\mathrm{d}x=\color{#F00}{\int_{-\pi}^{\pi}c_0\frac{1}{\sqrt{2\pi}}cos(jx)\mathrm{d}x}+\color{#00F}{\sum_{i=1}^{\infty}\int_{-\pi}^{\pi}a_i\frac{cos(ix)cos(jx)}{\sqrt{\pi}}\mathrm{d}x}+\color{#F00}{\sum_{i=1}^{\infty}\int_{-\pi}^{\pi}b_i\frac{sin(ix)cos(jx)}{\sqrt{\pi}}\mathrm{d}x}\tag{29}
\]</span> 由（14）（16）式可知，等式（29）中两个红色定积分都为<span class="math inline">\(0\)</span>，对于蓝色定积分，由（17）（21）式可知，当且仅当<span class="math inline">\(i= j\)</span>的项积分为<span class="math inline">\(1\)</span>，其余项的积分都为<span class="math inline">\(0\)</span>，故： <span class="math display">\[
\int_{-\pi}^{\pi}f(x)cos(jx)\mathrm{d}x=\color{#00F}{\sum_{i=1}^{\infty}\int_{-\pi}^{\pi}a_i\frac{cos(ix)cos(jx)}{\sqrt{\pi}}\mathrm{d}x}=\color{#F00}{\sum_{i=1,i\neq
j
}^{\infty}\int_{-\pi}^{\pi}a_i\frac{cos(ix)cos(jx)}{\sqrt{\pi}}\mathrm{d}x}+\color{#0F0}{\int_{-\pi}^{\pi}a_j\frac{cos(jx)cos(jx)}{\sqrt{\pi}}\mathrm{d}x}\tag{30}
\]</span>
将蓝色部分的积分拆分为红色和绿色两部分积分之和（绿色积分为<span class="math inline">\(i=j\)</span>时），显然由（17）式红色部分积分仍旧为<span class="math inline">\(0\)</span>，由（21）式绿色部分积分为<span class="math inline">\(\sqrt{\pi}a_j\)</span>，进一步得到: <span class="math display">\[
a_i\overset{i=j}{=}a_j=\frac{1}{\sqrt{\pi}}\int_{-\pi}^{\pi}f(x)cos(ix)\mathrm{d}x\tag{31}
\]</span> <span class="math inline">\(b_i\)</span>:</p>
<p>  同上可得到： <span class="math display">\[
b_i=\frac{1}{\sqrt{\pi}}\int_{-\pi}^{\pi}f(x)sin(ix)\mathrm{d}x\tag{32}
\]</span></p>
</blockquote>
<h2 id="周期与非周期下的傅里叶级数">周期与非周期下的傅里叶级数</h2>
<p>  在上文中，已经求出了系数的表达式，将这些系数代入（24）式，整理得到：
<span class="math display">\[
\begin{aligned}
f(x)&amp;=c_0\frac{1}{\sqrt{2\pi}}+\sum_{i=1}^{\infty}(a_i\frac{cos(ix)}{\sqrt{\pi}}+b_i\frac{sin(ix)}{\sqrt{\pi}})\\
&amp;=（\frac{1}{\sqrt{2\pi}}\int_{-\pi}^{\pi}f(x)\mathrm{d}x）\frac{1}{\sqrt{2\pi}}+\sum_{i=1}^{\infty}[(\frac{1}{\sqrt{\pi}}\int_{-\pi}^{\pi}f(x)cos(ix)\mathrm{d}x)\frac{cos(ix)}{\sqrt{\pi}}+(\frac{1}{\sqrt{\pi}}\int_{-\pi}^{\pi}f(x)sin(ix)\mathrm{d}x)\frac{sin(ix)}{\sqrt{\pi}}]\\
&amp;=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)\mathrm{d}x+\sum_{i=0}^{\infty}[\color{#00F}{\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)cos(ix)\mathrm{d}x}
*
\color{#F0F}{cos(ix)}+\color{#00F}{\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)sin(ix)\mathrm{d}x}
* \color{#F0F}{sin(ix)}]
\end{aligned}
\]</span> 此时，令 <span class="math display">\[
\begin{aligned}
a_0&amp;=\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)\mathrm{d}x\\
a_n&amp;=\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)cos(nx)\mathrm{d}x\\
b_n&amp;=\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)sin(nx)\mathrm{d}x
\end{aligned}\tag{34}
\]</span> 得到： <span class="math display">\[
f(x) = \frac{a_0}{2}+\sum_{n=1}^{\infty}[a_icos(nx)+b_isin(nx)]\tag{35}
\]</span>
式（35）显然就是傅立叶级数。级数（35）的收敛性证明，涉及较多泛函分析的内容，在此不做展开，有时间在另开一篇文章说明，有兴趣的读者可以尝试一下证明。细心的读者可能注意到<span class="math inline">\(f(x)\)</span>的定义域是<span class="math inline">\([-\pi,\pi]\)</span>，那么如果对于任意周期<span class="math inline">\(T\)</span>，级数还成立吗？将定义域拓展到实数域后，级数（35）还成立吗？对此，下面进一步讨论。</p>
<h3 id="周期函数的傅立叶级数">周期函数的傅立叶级数</h3>
<p>  当<span class="math inline">\(f(x)\)</span>是一个周期为<span class="math inline">\(2\pi\)</span>的周期函数时，那么在区间<span class="math inline">\([-\pi,\pi]\)</span>中，作变换： <span class="math display">\[
x = \frac{2\pi}{2T}t=\frac{\pi}{T}t\tag{36}
\]</span> 则<span class="math inline">\(F(t)=f(\frac{\pi}{T}t),t \in
[-T,T]\)</span>，为周期<span class="math inline">\(2T\)</span>的周期函数，构造标准正交三角函数系：
<span class="math display">\[
\frac{1}{\sqrt{2T}},\frac{cos(x)}{\sqrt{T}},\frac{sin(x)}{\sqrt{T}},\cdots,\frac{cos(kx)}{\sqrt{T}},\frac{sin(kx)}{\sqrt{T}},\cdots\tag{37}
\]</span> 进一步得到此时的傅立叶系数： <span class="math display">\[
\begin{aligned}
a_0&amp;=\frac{1}{T}\int_{-T}^{T}f(x)\mathrm{d}x\\
a_n&amp;=\frac{1}{T}\int_{-T}^{T}f(x)cos(\frac{n\pi}{T}x)\mathrm{d}x\\
b_n&amp;=\frac{1}{T}\int_{-T}^{T}f(x)sin(\frac{n\pi}{T}x)\mathrm{d}x
\end{aligned}\tag{38}
\]</span> 则此时的傅立叶级数为： <span class="math display">\[
\color{#F00}{F(t) =
\frac{a_0}{2}+\sum_{n=1}^{\infty}[a_icos(\frac{n\pi}{T}t)+b_isin(\frac{n\pi}{T}t)]}\tag{39}
\]</span>
因此，对于可积的任意周期函数，都能展开为对应的傅立叶级数，并且由于周期函数的特性，当定义域拓展到实数域时也是成立的。</p>
<h3 id="非周期函数的傅立叶积分">非周期函数的傅立叶积分</h3>
<p>  在更多情况下，需要处理非周期函数，对此比较直观的处理技巧是，将非周期函数视为周期为<span class="math inline">\(\infty\)</span>的周期函数，即<span class="math inline">\(2T \to +\infty\)</span>。设<span class="math inline">\(\xi(x)\)</span>是定义在<span class="math inline">\(R\)</span>上，并且在定义域绝对可积，<span class="math inline">\(\xi_T(x)\)</span>是<span class="math inline">\(\xi(x)\)</span>在有限区间<span class="math inline">\([-T,T]\)</span>上的截取，因为<span class="math inline">\(\xi_{T}(x)\)</span>可视为该有限区间上的周期函数，令<span class="math inline">\(\omega=\frac{2\pi}{2T},\)</span>则得到<span class="math inline">\(\xi_{T}(x)\)</span>的傅立叶级数为： <span class="math display">\[
\xi_{T}(x)=\frac{a_0}{2}+\sum_{n=1}^{\infty}[a_ncos(n\omega
x)+b_nsin(n\omega x)]\tag{40}
\]</span> 其中： <span class="math display">\[
\begin{aligned}
a_0&amp;=\frac{1}{T}\int_{-T}^{T}f(t)\mathrm{d}t\\
a_n&amp;=\frac{1}{T}\int_{-T}^{T}f(t)cos(n\omega t)\mathrm{d}t\\
b_n&amp;=\frac{1}{T}\int_{-T}^{T}f(t)sin(n \omega t)\mathrm{d}t
\end{aligned}\tag{41}
\]</span> 得到： <span class="math display">\[
\begin{aligned}
\xi_T(x)&amp;=\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t+\sum_{n=1}^{\infty}[\frac{1}{T}\int_{-T}^{T}f(t)cos(n\omega
t)\mathrm{d}tcos(n\omega x)+\frac{1}{T}\int_{-T}^{T}f(t)sin(n \omega
t)\mathrm{d}tsin(n\omega x)] \\
&amp;=\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t+\sum_{n=1}^{\infty}\frac{1}{T}\int_{-T}^{T}f(t)[cos(n\omega
t)cos(n\omega x)+sin(n \omega t)sin(n\omega x)]\mathrm{d}t \\
&amp;=\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t+\sum_{n=1}^{\infty}\frac{1}{T}\int_{-T}^{T}f(t)cos[n\omega(x-t)]\mathrm{d}t
\end{aligned}\tag{42}
\]</span></p>
<p>于是有： <span class="math display">\[
\xi(x)=\lim_{T \to \infty}\xi_{T}(x)=\lim_{T \to
\infty}\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t+\lim_{T \to
\infty}\sum_{n=1}^{\infty}\frac{1}{T}\int_{-T}^{T}f(t)cos[n\omega(x-t)]\mathrm{d}t\tag{43}
\]</span></p>
<p>对于（43）式的两个极限，接下来分别进行讨论：</p>
<blockquote>
<p>对于第一个极限：</p>
<p>  由于<span class="math inline">\(f(t)\)</span>在<span class="math inline">\(R\)</span>上绝对可积，则： <span class="math display">\[
\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t
\leq\frac{1}{2T}\int_{-\infty}^{\infty}|f(t)|\mathrm{d}t=\frac{\alpha}{2T}\tag{44}
\]</span> 所以，有： <span class="math display">\[
\lim_{T \to \infty}\frac{1}{2T}\int_{-T}^{T}f(t)\mathrm{d}t=0\tag{45}
\]</span> 对于第二个极限，先直接给出结论：</p>
<p>  令<span class="math inline">\(\lambda=n\omega=\frac{\pi}{T}\)</span>，有： <span class="math display">\[
\lim_{T \to
\infty}\sum_{n=1}^{\infty}\frac{1}{T}\int_{-T}^{T}f(t)cos[n\omega(x-t)]\mathrm{d}t=\frac{1}{\pi}\int_{0}^{+\infty}[\int_{-\infty}^{+\infty}f(t)cos[\lambda(x-t)]\mathrm{d}t]\mathrm{d}\lambda\tag{46}
\]</span></p>
</blockquote>
<p>于是，得到非周期函数<span class="math inline">\(f(x)\)</span>的傅立叶积分表示： <span class="math display">\[
f(x)
=\frac{1}{\pi}\int_{0}^{+\infty}[\int_{-\infty}^{+\infty}f(t)cos[\lambda(x-t)]\mathrm{d}t]\mathrm{d}\lambda\tag{47}
\]</span> 或写为： <span class="math display">\[
f(x)=\int_{0}^{+\infty}[A(\lambda)cos(\lambda x)+B(\lambda)sin(\lambda
x)]\mathrm{d}\lambda\tag{48}
\]</span> 其中： <span class="math display">\[
A(\lambda)=\frac{1}{\pi}\int_{-\infty}^{+\infty}f(t)cos(\lambda
t)\mathrm{d}t\\
B(\lambda)=\frac{1}{\pi}\int_{-\infty}^{+\infty}f(t)sin(\lambda
t)\mathrm{d}t
\tag{49}
\]</span>
综合来看，周期函数和有限区间上的函数可以用傅立叶级数来表示；而无穷区间的非周期函数，用傅里叶积分表示，对应了频率的连续分布。</p>
<h1 id="傅里叶变换">傅里叶变换</h1>
<p>  傅里叶级数的本质是函数在某个函数空间中各个基底的投影和，在上文中我们通过引入希尔伯特空间，构造标准正交三角函数系进而推导出傅立叶级数与傅立叶积分。然而，这一切都是基于实数域推导，那么这种思路在复数域是否也成立呢，答案是显而易见的。下面，我们通过欧拉公式（公式证明见文末）来得到傅立叶积分的复数形式：</p>
<p><span class="math display">\[
e^{ix}=cos(x)+isin(x)\tag{50}
\]</span>   观察（47）式，由于： <span class="math display">\[
\int_{-\infty}^{+\infty}f(t)cos[\lambda(x-t)]\mathrm{d}t=\lim_{\alpha
\to
\infty}\int_{-\alpha}^{\alpha}f(t)cos[\lambda(x-t)]\mathrm{d}t\tag{51}
\]</span> 因为 <span class="math display">\[
\begin{aligned}
\int_{-\alpha}^{\alpha}f(t)e^{i\lambda(x-t)}\mathrm{d}t&amp;=\int_{-\alpha}^{\alpha}f(t)\{cos[\lambda(x-t)]+isin[\lambda(x-t)]\}\mathrm{d}t\\
&amp;=\int_{-\alpha}^{\alpha}f(t)cos[\lambda(x-t)]\mathrm{d}t+i*0（奇函数在对称区间的积分为零）
\end{aligned}\tag{52}
\]</span> 则非周期函数<span class="math inline">\(f(x)\)</span>的傅立叶积分（47）可改写为： <span class="math display">\[
\begin{aligned}
f(x)&amp;=\frac{1}{\pi}\int_{0}^{+\infty}[\int_{-\infty}^{+\infty}f(t)cos[\lambda(x-t)]\mathrm{d}t]\mathrm{d}\lambda\\
&amp;=\frac{1}{\pi}\int_{0}^{+\infty}[\int_{-\infty}^{+\infty}f(t)e^{i\lambda(x-t)}\mathrm{d}t]\mathrm{d}\lambda\\
&amp;=\frac{1}{\pi}\int_{0}^{+\infty}e^{i\lambda
x}\int_{-\infty}^{+\infty}f(t)e^{-i\lambda
t}\mathrm{d}t\mathrm{d}\lambda\\
&amp;=\frac{1}{2\pi}\int_{-\infty}^{+\infty}e^{i\lambda
x}\color{#F00}{\int_{-\infty}^{+\infty}f(t)e^{-i\lambda
t}\mathrm{d}t}\mathrm{d}\lambda\\
\end{aligned}\tag{53}
\]</span>
上式最后一个等式即为傅立叶积分的复数形式，而红色积分部分就是大名鼎鼎的<code>傅立叶变换</code>也叫<code>像函数</code>，是一个复数表示<code>振幅</code>和<code>相位</code>：
<span class="math display">\[
F(\lambda)=\color{#F00}{\int_{-\infty}^{+\infty}f(t)e^{-i\lambda
t}\mathrm{d}t}\tag{54}
\]</span> 而<span class="math inline">\(f(x)\)</span>也称为<code>傅立叶逆变换</code>也叫<code>本函数</code>：
<span class="math display">\[
f(x)=\frac{1}{2\pi}\int_{-\infty}^{+\infty}e^{i\lambda
x}\color{#F00}{F(\lambda)}\mathrm{d}\lambda\tag{55}
\]</span>   <code>傅里叶变换</code>一词既指变换操作本身（将函数<span class="math inline">\(f(x)\)</span>
进行傅里叶变换），又指该操作所生成的复数函数（<span class="math inline">\(F(\lambda)\)</span>是<span class="math inline">\(f(x)\)</span>的傅里叶变换）,需要注意的是，一般情况下傅立叶变换是可逆的。</p>
<h2 id="傅立叶变换的性质">傅立叶变换的性质</h2>
<p>  傅立叶级数使用不同频率的三角函数和来表示周期函数和有限区上的函数，而傅立叶积分则是对频率作无穷积分来表示无穷区间上的函数。本质上其实是从不同的角度刻画相同的函数，所以你经常可以听到这样的说法，傅立叶变换是一种线性积分变换，常用于信号<code>时域</code>到<code>频域</code>之间的变换，这里说的<code>时域</code>是从时间的角度描述函数或信号，而<code>频域</code>则是从频率的角度描述函数或信号。本质上傅立叶变换就像化学分析，像分析物质的基本成分一样，确定函数或信号的基本组成。</p>
<p>  接下来讨论一下傅立叶变换的一些基本性质，为了方便描述，约定<span class="math inline">\(\mathscr{F}\)</span>为傅立叶变换的作用算子，即<span class="math inline">\(\mathscr{F}[f]=F[\lambda]\)</span>为<span class="math inline">\(f(x)\)</span>的傅立叶变换，<span class="math inline">\(\mathscr{F}^{-1}[F]=f(x)\)</span>表示<span class="math inline">\(F(\lambda)\)</span>的傅立叶逆变换，并且函数<span class="math inline">\(f(x),g(x)\)</span>都存在傅立叶变换：</p>
<ul>
<li><p>线性性质</p>
<p>两函数之和的傅里叶变换等于各自的傅立叶变换之和： <span class="math display">\[
\mathscr{F}[\alpha f+ \beta
g]=\alpha\mathscr{F}[f]+\beta\mathscr{F}[g]（\alpha,\beta \in C）
\]</span></p></li>
<li><p>频移性质 <span class="math display">\[
\mathscr{F}[f(x)e^{i\lambda_{0}x}]=\mathscr{F}[f](\lambda-\lambda_0)=F(\lambda-\lambda_0)（\lambda_0
\in R）
\]</span></p></li>
<li><p>时移特性 <span class="math display">\[
\mathscr{F}^{-1}[f(x)e^{i\lambda x_0}]=\mathscr{F}[{f}](x+x_0)
\]</span></p></li>
<li><p>帕塞瓦尔定理</p>
<p>  若<span class="math inline">\(f(x)\)</span>平方可积，则有： <span class="math display">\[
\int_{-\infty}^{+\infty}f^2(x)\mathscr{d}x=\frac{1}{2\pi}\int_{-\infty}^{+\infty}|F(\lambda)|^2\mathscr{d}\lambda
\]</span></p></li>
<li><p>卷积的傅里叶变换</p>
<p>  若<span class="math inline">\(f(x),g(x)，x \in
R\)</span>且在定义域内绝对可积，定义卷积函数： <span class="math display">\[
f*g = \int_{-\infty}^{+\infty}f(x-\xi)g(\xi)\mathscr{d}\xi
\]</span></p>
<p>则有： <span class="math display">\[
\begin{aligned}
&amp;\mathscr{F}[f*g]=\mathscr{F}[f]\cdot\mathscr{F}[g] \\
&amp;\mathscr{F}^{-1}[F(\lambda)*G(\lambda)] =
2\pi\mathscr{F}^{-1}[F(\lambda)]\cdot\mathscr{F}^{-1}[G(\lambda)]
\end{aligned}
\]</span>
  傅里叶变换在时域和频域之间搭起来一座桥梁，一些在时域很难解决甚至无法解决的问题，在频域下却可以轻松得到解决，在信号、图像处理还有偏微分方程等领域都有着广泛的应用。</p></li>
</ul>
<h2 id="离散傅里叶变换">离散傅里叶变换</h2>
<p>  <strong>离散傅里叶变换</strong>（Discrete Fourier
Transform，缩写为DFT），是傅里叶变换在时域和频域上都呈离散的形式，将信号的时域采样变换为其离散时间傅里叶变换的频域采样。</p>
<p>对于序列<span class="math inline">\({x[n]|,n=0,1,\dots,N-1}\)</span>
其离散傅里叶变换（DFT）如下：<br>
<span class="math display">\[
  \hat{x}[k]=\sum_{n=0}^{N-1}e^{-i\frac{2nk\pi}{N}}x[n]\quad
k=0,1,\dots,N-1.
\]</span> 离散傅里叶变换的逆变换（IDFT）如下： <span class="math display">\[
  x[n]=\frac{1}{N}\sum_{k=0}^{N-1}e^{i\frac{2nk\pi}{N}}\hat{x}[k] \quad
n=0,1,\dots,N-1
\]</span> 离散傅里叶变换的应用：</p>
<blockquote>
<ul>
<li>数据压缩</li>
</ul>
</blockquote>
<blockquote>
<p>  由于人类感官的分辨能力存在极限，因此很多有损压缩算法利用这一点<strong>将语音、音频、图像、视频等信号的高频部分除去</strong>。高频信号对应于信号的细节，滤除高频信号可以在人类感官可以接受的范围内获得很高的压缩
比。这一去除高频分量的处理就是通过离散傅里叶变换完成的。将时域或空域的信号转换到频域，仅储存或传输较低频率上的系数，在解压缩端采用逆变换即可重建信号。</p>
</blockquote>
<blockquote>
<ul>
<li>长整数与多项式乘法</li>
</ul>
</blockquote>
<blockquote>
<p>  目前长整数或多项式乘法最快速的算法是基于离散傅里叶变换的。由于整数（或多项式）乘法是逐位（或逐项）乘累加的形式，因此整数（或多项式）乘积的数字（或系数）可以用乘数数字（或乘式系数）的卷积表示。利用&gt;卷积定理，只要将数字（或系数）序列通过离散傅里叶变换变到频域，就可以将逐个乘累加的卷积变为对位的乘法，从而减少计算量，再以一次逆变换便可以得到乘法结果。需要注意整数乘法还有进位的问题。</p>
</blockquote>
<blockquote>
<ul>
<li>求解偏微分方程</li>
</ul>
</blockquote>
<blockquote>
<p>  离散傅里叶变换及其多维形式在偏微分方程的求解中也有应用。此时DFT被看作傅里叶级数的近似。傅里叶级数将函数在复指数<span class="math inline">\(e^{i\lambda
x}\)</span>上展开，这正是微分算子的特征方程： <span class="math display">\[
\frac{d}{dx}e^{i\lambda x}=i\lambda e^{i\lambda x}
\]</span>
  因此，通过傅里叶级数的形式，线性常微分方程被转换为代数方程，而后者是很容易求解的。此时得到的结果是偏微分方程解的级数表示，只要通过DFT逆变换即可得到其一般表示，这种方法被称作谱方法或级数解法。</p>
</blockquote>
<h1 id="快速傅里叶变换">快速傅里叶变换</h1>
<p>  离散傅里叶变换十分强大，但是计算复杂度较高，对于一个大小为<span class="math inline">\(n\)</span>的序列，其离散傅里叶级数的复杂度为<span class="math inline">\(O(n^2)\)</span>，对于一些需要实时计算的场景，不太能满足需求。因此，快速傅里叶变换（FFT）应运而生，快速傅里叶变换是快速计算序列的离散傅里叶变换]叶变换)（DFT）或其逆变换的方法。傅里叶分析将信号从原始域（通常是时间或空间）转换到频域的表示或者逆过来转换。FFT会通过把DFT矩阵分解为稀疏因子之积来快速计算此类变换，
因此，它能够将计算DFT的复杂度从<span class="math inline">\(O(n^2)\)</span>降低到<span class="math inline">\(n\log_2 n\)</span>。</p>
<p>  FFT的本质就是通过不断的把长序列的DFT分解为几个短序列的DFT，并利用单位根的周期性和对称性来减少计算量。FFT算法有很多种，不过大致可以分为两类：</p>
<blockquote>
<p><strong>按抽取方法可分为</strong>：</p>
<ul>
<li>时域抽取法（DIT）</li>
<li>频域抽取大（DIF）</li>
</ul>
<p><strong>按<code>基数</code>可分为</strong>：</p>
<ul>
<li>基2-FFT算法</li>
<li>基4-FFT算法</li>
<li>混合基FFT算法</li>
<li>分裂基FFT算法</li>
</ul>
</blockquote>
<h2 id="cooley-tukey算法">Cooley-Tukey算法</h2>
<p>  Cooley-Tukey算法是最常见的FFT算法。这一方法以分治法为策略递归地将长度为<span class="math inline">\(N=N_{1}N_{2}\)</span>的离散傅里叶变换分解为长度为<span class="math inline">\(N_{1}\)</span>的<span class="math inline">\(N_{2}\)</span>个较短序列的离散傅里叶变换，以及与<span class="math inline">\(\mathrm {O} (N)\)</span>个转因子的复数乘法。</p>
<p>  Cooley-Tukey算法最有名的应用，是将序列长为<span class="math inline">\(N\)</span> 的DFT分割为两个长为<span class="math inline">\(\frac{N}{2}\)</span>
的子序列的DFT，因此这一应用只适用于序列长度为2的幂的DFT计算，即基2-FFT。实际上，如同高斯和Cooley与Tukey都指出的那样，<strong>Cooley-Tukey算法也可以用于序列长度<em>N</em>
为任意因数分解形式的DFT，即混合基FFT，而且还可以应用于其他诸如分裂基FFT等变种</strong>。尽管Cooley-Tukey算法的基本思路是采用递归的方法进行计算，大多数传统的算法实现都将显式的递归算法改写为非递归的形式。另外，因为Cooley-Tukey算法是将DFT分解为较小长度的多个DFT，因此它可以同任一种其他的DFT算法联合使用。</p>
<h3 id="基2时间抽取法">基2时间抽取法</h3>
<p>  基2时间抽取算法是Cooley-Tukey算法的一种分支，当序列<span class="math inline">\(x(n)\)</span>的点数为<span class="math inline">\(N=2^{M}\quad M \in
\N\)</span>（若不满足，可补零），此时的Cooley-Tukey算法称之为基2时间抽取法。由于序列点数为2的整数幂，则可以将序列按序号<span class="math inline">\(n\)</span>的奇偶性分为两组：</p>
<ul>
<li><p>偶序列 <span class="math display">\[
x_1=x_{(2r)} \quad r = 0,1,\dots,\frac{N}{2}-1
\]</span></p></li>
<li><p>奇序列 <span class="math display">\[
x_2=x_{(2r+1)} \quad r = 0,1,\dots,\frac{N}{2}-1
\]</span>
即一组由偶数序号组成，另外一组由奇数序号组成（注意数据长度为<span class="math inline">\(\frac{N}{2}\)</span>）</p></li>
</ul>
<h2 id="互质因子算法">互质因子算法</h2>
<p><a href="https://www.wikiwand.com/zh/%E4%BA%92%E8%B3%AA%E5%9B%A0%E5%AD%90%E7%AE%97%E6%B3%95">互质因子算法</a></p>
<h2 id="winograd算法">Winograd算法</h2>
<p><a href="https://www.wikiwand.com/zh/%E5%A8%81%E8%AB%BE%E6%A0%BC%E6%8B%89%E5%BE%B7%E5%BF%AB%E9%80%9F%E5%82%85%E7%AB%8B%E8%91%89%E8%AE%8A%E6%8F%9B%E6%BC%94%E7%AE%97%E6%B3%95">Winograd算法</a></p>
<ul>
<li><p>拉普拉斯变换</p>
<p><a href="https://zhuanlan.zhihu.com/p/40783304">拉普拉斯变换</a></p>
<p><strong>傅里叶变换是将函数分解到频率不同、幅值恒为1的单位圆上；拉普拉斯变换是将函数分解到频率幅值都在变化的圆上。因为拉普拉斯变换的基有两个变量，因此更灵活，适用范围更广。</strong></p></li>
</ul>
<blockquote>
<p>关于快速傅里叶变换只是做了简单的介绍和梳理,详细内容等以后有时间再更新。</p>
</blockquote>
<h1 id="概念解析">概念解析</h1>
<ul>
<li><code>酉空间</code></li>
</ul>
<p>设<span class="math inline">\(V\)</span>为一个复数域<span class="math inline">\(F\)</span>上的线形空间，若在<span class="math inline">\(V\)</span>中定义了两个变量<span class="math inline">\(\alpha,\beta\)</span>的内积（数量积），记作<span class="math inline">\((\alpha,\beta)\)</span>，且满足：</p>
<p><span class="math inline">\((i)\)</span> <span class="math inline">\((\alpha,\beta)=\overline{(\beta,\alpha)}\)</span>，其中<span class="math inline">\(\overline{(\beta,\alpha)}\)</span>是<span class="math inline">\((\alpha,\beta)\)</span>的共轭</p>
<p><span class="math inline">\((ii)\)</span> <span class="math inline">\((\alpha,\alpha) \geq0\)</span>，当且仅当<span class="math inline">\(\alpha=0\)</span>时等号成立</p>
<p><span class="math inline">\((iii)\)</span> <span class="math inline">\((a_1\alpha_1+a_2\alpha_2,\beta)=a_1(\alpha_1,\beta)+a_2(\alpha_2,\beta)\)</span>,对任意<span class="math inline">\(\alpha_1,\alpha_2,\beta\in V,a_1,a_2\in
F\)</span></p>
<p>则称<span class="math inline">\(V\)</span>为酉空间(<span class="math inline">\(U\)</span>空间)，又称为内积空间，当<span class="math inline">\(F\)</span>为实数域时，此时的内积是可交换的，有限维的实酉空间也就是欧几里德空间。直观来说，酉空间就是将欧几里德空间的内积运算从实数域拓展到复数域。</p>
<ul>
<li><p><code>完备性</code></p>
<p>一个向量空间具有完备性指空间中的任何<a href="https://www.wikiwand.com/zh/柯西序列">柯西序列</a>都收敛在该空间之内。。</p></li>
<li><p><code>柯西列</code></p>
<p>  柯西列就是空间中元素构成的一个序列，并且这个序列在无穷远处两个元素之间的距离趋于零。准确的说，如果空间中有一个序列
<span class="math inline">\(\{x_n\}\)</span> ,当<span class="math inline">\(n,m \to \infty\)</span>的时候，<span class="math inline">\(||x_n-x_m|| \to 0\)</span>
（即二者的距离趋零），则 <span class="math inline">\(\{x_n\}\)</span>就是一个柯西列，也就是说完备性保证了取序列极限不会跑到空间外面去。一个<strong>不完备</strong>的例子就是有理数的集合，例如这个集合可以用柯西列的极限去逼近<span class="math inline">\(\sqrt{2}\)</span>
，而这个极限并不在有理数这个集合中，所以有理数集合是不完备的，而实数集合是完备的。</p></li>
<li><p>三角恒等式</p>
<p><a href="https://www.wikiwand.com/zh/%E4%B8%89%E8%A7%92%E6%81%92%E7%AD%89%E5%BC%8F#/%E4%BA%8C%E5%80%8D%E8%A7%92%E3%80%81%E4%B8%89%E5%80%8D%E8%A7%92%E5%92%8C%E5%8D%8A%E8%A7%92%E5%85%AC%E5%BC%8F">维基百科：三角恒等式</a></p></li>
<li><p>内积空间</p>
<p>  <strong>内积空间</strong>是<a href="https://www.wikiwand.com/zh-hans/线性代数">线性代数</a>里的基本概念，是增添了一个额外的结构的<a href="https://www.wikiwand.com/zh-hans/向量空间">向量空间</a>。这个额外的结构叫做<strong><a href="https://www.wikiwand.com/zh-hans/内积">内积</a></strong>或<a href="https://www.wikiwand.com/zh-hans/标量积">标量积</a>。内积将一对<a href="https://www.wikiwand.com/zh-hans/向量">向量</a>与一个标量连接起来，允许我们严格地谈论<a href="https://www.wikiwand.com/zh-hans/向量">向量</a>的“<a href="https://www.wikiwand.com/zh-hans/角">夹角</a>”和“<a href="https://www.wikiwand.com/zh-hans/长度">长度</a>”，并进一步谈论向量的<a href="https://www.wikiwand.com/zh-hans/正交">正交性</a>。内积空间由<a href="https://www.wikiwand.com/zh-hans/欧几里得空间">欧几里得空间</a>抽象而来（内积是点积的抽象），这是<a href="https://www.wikiwand.com/zh-hans/泛函分析">泛函分析</a>讨论的课题。</p></li>
<li><p>欧拉公式的证明</p>
<p>  欧拉公式（50）的证明方式有两种，第一种是构造函数：</p></li>
</ul>
<p><span class="math display">\[
  f(x)=\frac{e^{ix}}{cos(x)+isin(x)}
\]</span></p>
<p>利用拉格朗日中值定理证明<span class="math inline">\(f(x)=1\)</span>即可；第二种则是使用麦克劳林级数分别展开得到：
<span class="math display">\[
  \begin{aligned}
  e^{ix}&amp;=\sum_{i=0}^{\infty}\frac{(ix)^n}{n!}=1+ix-\frac{x^2}{2!}-i\frac{x^3}{3!}+\dots+i^n\frac{x^n}{n!}+\dots（\forall
x \in R）\\
  cos(x)&amp;=\sum_{i=0}^{\infty}\frac{(-1)^n}{n!}x^{2n}=1-\frac{x^2}{2!}+\frac{x^4}{4!}+\dots+\frac{(-1)^n}{(2n)!}x^{2n}+\dots（\forall
x \in R）\\
  isin(x)&amp;=i\sum_{i=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}=ix-i\frac{x^3}{3!}+i\frac{x^5}{5!}+\dots+i\frac{(-1)^n}{(2n+1)!}x^{2n+1}+\dots（\forall
x \in R）
  \end{aligned}
\]</span>
后两者的麦克劳林级数展开相加刚好等于前者的麦克劳林级数展开。在此只补充拉格朗日中值定理，具体证明过程较简单，不做详细展开。</p>
<blockquote>
<p>拉格朗日中值定理:</p>
<p>  如果函数<span class="math inline">\(f(x)\)</span>，在闭区间<span class="math inline">\([a,b]\)</span>上连续，在开区间<span class="math inline">\((a,b)\)</span>内可微。则少存在一点<span class="math inline">\(\xi \in (a,b)\)</span>，使下面的等式成立: <span class="math display">\[
f(b)-f(a)=f^\prime(\xi)(b-a)
\]</span> 推论：</p>
<p>  若函数<span class="math inline">\(f(x)\)</span>的导数恒为零，则<span class="math inline">\(f(x)\)</span>为常值函数，即<span class="math inline">\(f(x)=C,C \in R\)</span>。</p>
</blockquote>
]]></content>
      <categories>
        <category>数学理论</category>
      </categories>
      <tags>
        <tag>傅立叶</tag>
        <tag>希尔伯特空间</tag>
        <tag>正交函数系</tag>
      </tags>
  </entry>
  <entry>
    <title>卡尔曼滤波</title>
    <url>/posts/b654ab75/</url>
    <content><![CDATA[<p>  <strong>卡尔曼滤波</strong>（Kalman filter）是一种高效率的<a href="https://www.wikiwand.com/zh-hans/递归滤波器">递归滤波器</a>（<a href="https://www.wikiwand.com/zh-hans/自迴歸模型">自回归</a>滤波器），它能够从一系列的不完全及包含<a href="https://www.wikiwand.com/zh-hans/雜訊_(通訊學)">杂讯</a>的<a href="https://www.wikiwand.com/zh-hans/测量">测量</a>中，估计<a href="https://www.wikiwand.com/zh-hans/动态系统">动态系统</a>的状态。卡尔曼滤波会根据各测量量在不同时间下的值，考虑各时间下的<a href="https://www.wikiwand.com/zh-hans/联合分布">联合分布</a>，再产生对未知变数的估计，因此会比只以单一测量量为基础的估计方式要准。卡尔曼滤波得名自主要贡献者之一的<a href="https://www.wikiwand.com/zh-hans/鲁道夫·卡尔曼">鲁道夫·卡尔曼</a>，最早用于解决阿波罗计划的轨道预测问题。
<span id="more"></span>
  上面的定义摘自维基百科，是对卡尔曼滤波的专业阐述，为了便于理解通过下面两个例子来对卡尔曼滤波有个大致的了解。</p>
<p>  航天器的发动机能够在足够高的温度下燃烧燃料，为航天器提供足够的动力，航天器发动机燃烧室在燃烧时温度可以达到数千摄氏度，过高的温度可能会损坏发动机的机械部件，导致火箭发射失败，因此需要密切关注火箭燃烧室的内部温度，显然在燃烧室内部放置温度传感器会直接被融化。此时，无法直接测量燃烧室的内部温度，基于这种情况，可以在燃烧室外放置一个温度传感器测量燃烧室的外部温度，使用卡尔曼滤波器利用外部温度来估算燃烧室的内部温度。</p>
<p>  这是卡尔曼滤波器一种使用方式：</p>
<blockquote>
<p>  当系统的状态无法通过直接测量得到但是可以间接测量时，可以使用卡尔曼滤波通过间接测量值来估算系统的状态。
总所周知，汽车的导航系统使用<code>车载传感器（Onboard sensors）</code>得到汽车的当前位置并导航到目的地。常用的车载传感器有：<code>惯性测量单元（Inertial measurement unit,IMU）</code>使用加速度计和陀螺仪来测量汽车的加速度和角速度；<code>里程表(Odometer)</code>测量汽车的相对行驶距离；<code>GPS接收器(GPS receiver)</code>接收来自GPS卫星的信号，来确定汽车在地球表面的位置。</p>
</blockquote>
<blockquote>
<p>  IMU中的加速度计提供了汽车当前的加速度大小及方向，但是想要获得汽车的位置，需要对加速度进行两次积分（<span class="math inline">\(s =\iint
a(t)dt\)</span>）得到汽车的位置，数值积分算法在计算位置时会存在微小的误差并且会随着时间的累积会不断，最终产生积分漂移，完全偏离汽车的正确位置；里程表容易受到轮胎压力和道路状况的影响；而GPS位置跟新速度慢，而且会存在一定噪声，最大问题还是在车辆通过隧道或车库时，信号很差，甚至无法接收到信号。三种车载传感器各自有各自的局限和优势，只用单独一种传感器进行定位效果都不尽如人意，此时可以使用卡尔曼滤波融合三种传感器得到汽车位置的最优估计值。</p>
</blockquote>
<p>  这是卡尔曼滤波的另一种使用方式：</p>
<blockquote>
<p>组合各种可能受到噪声影响的传感器测量值，得到一个最优的估计值。</p>
</blockquote>
<p>  卡尔曼滤波主要应用于高精度传感系统中，在时下大热的机器人、无人机、自动导航都有着应用，由于其递归的特性（即只要获知上一时刻状态的估计值以及当前状态的观测值就可以计算出当前状态的估计值），因此不需要记录观测或者估计的历史信息。与大多数滤波器不同之处，卡尔曼滤波器是一种纯粹的<a href="https://www.wikiwand.com/zh-hans/時域">时域</a>滤波器，它不需要像<a href="https://www.wikiwand.com/zh-hans/低通滤波器">低通滤波器</a>等<a href="https://www.wikiwand.com/zh-hans/频域">频域</a>滤波器那样，需要在频域设计再转换到时域实现。</p>
<p>  其本质思想是采用信号与噪声的状态空间模型，利用前一时刻地估计值和现时刻的观测值来更新对状态变量的估计，求出现时刻的估计值，它适合于实时处理和计算机运算。下面我们一步步来解开卡尔曼滤波的神秘面纱。</p>
<hr>
<h3 id="隐马尔可夫">隐马尔可夫</h3>
<h4 id="随机过程">随机过程</h4>
<p>  随机过程被认为是概率论的“动力学”部分，其研究对象是随时间演变的随机现象。对于这种现象，已不能用随机变量或多维随机变量来合理表达，而需要一族（无限多个）随机变量来描述。高等教育出版社的《概率论和数理统计》第四版是这样定义随机过程：</p>
<blockquote>
<p>  设<span class="math inline">\(T\)</span>是一无限实数集，我们把依赖于参数<span class="math inline">\(t\in
T\)</span>的一族随机变量称为<code>随机过程</code>，记为<span class="math inline">\(\{X(t),t\in T\}\)</span>,其中对<span class="math inline">\(\forall t \in T,X(t)\)</span>是一随机变量，<span class="math inline">\(T\)</span>叫做<code>参数集</code>。通常把<span class="math inline">\(t\)</span>看作时间，称<span class="math inline">\(X(t)\)</span>为时刻<span class="math inline">\(t\)</span>时过程的状态，而<span class="math inline">\(X(t_1)=x,x \in R\)</span>定义为<span class="math inline">\(t=t_1\)</span>时过程处于状态<span class="math inline">\(x\)</span>，对于<span class="math inline">\(\forall t\in
T,X(t)\)</span>的所有可能取一切值的全体称为随机过程的<code>状态空间</code>，<code>泊松过程</code>和<code>维纳过程</code>都是典型的随机过程，篇幅有限这里不做进一步展开，有兴趣的读者可以自行Google。</p>
</blockquote>
<h4 id="马尔可夫过程">马尔可夫过程</h4>
<p>  一个随机过程在时刻<span class="math inline">\(t_0\)</span>所处的状态为已知的条件下，随机过程在时刻<span class="math inline">\(t（t&gt;t_0)\)</span>所处的状态与其在时刻<span class="math inline">\(t_0\)</span>之前所处的状态无关。简而言之，就是“将来”仅依赖于”现在”与“过去”无关。我们称这种特性为<code>马尔可夫性</code>或无后效性。而这种具有马尔可夫性质的随机过程称之为<code>马尔可夫过程</code>，可以推出，泊松过程是时间连续状态离散的马尔可夫过程，而维纳过程则是时间状态都连续的马尔可夫过程。对于时间和状态都是离散的马尔可夫过程就是构成卡尔曼滤波的重要基石<code>马尔可夫链</code>：</p>
<p>  对于随机过程<span class="math inline">\({X_n,n=1,2,\cdots}\)</span>,若其条件概率分布满足:
<span class="math display">\[
P\{X_{n+1}=x_{n+1}|X_n=x_n,\cdots,X_1=x_1\}=P\{X_{n+1}=x_{n+1}|X_n=x_n\}
\]</span>，则称此随机过程为马尔可夫链。</p>
<h4 id="状态转移矩阵">状态转移矩阵</h4>
<p>  在马尔可夫链中，从一个状态转移到另一个状态的概率称为状态转移概率，如果系统的可能状态是有限的，例如有<span class="math inline">\(K\)</span>个状态，则状态转移概率构成一个<span class="math inline">\(K\times K\)</span>的状态转移矩阵： <span class="math display">\[P=\left[ \begin{matrix}   p_{11} &amp; p_{12}
&amp; \cdots &amp; p_{1K} \\ p_{21} &amp; p_{22} &amp; \cdots &amp;
p_{2K}\\ \cdots &amp; \cdots &amp; \cdots &amp;\cdots \\ p_{K1} &amp;
p_{K2} &amp; \cdots &amp; p_{KK}\end{matrix}  \right]\]</span>
其中矩阵的每一行的和为1，该矩阵是一个随机矩阵，任何一个随机矩阵都可以作为状态转移矩阵
。</p>
<h4 id="隐马尔可夫模型">隐马尔可夫模型</h4>
<p>  在马尔可夫模型中，系统的状态是直接可见的，这样状态的转移概率就可以构成该系统的全部参数。但是，在实际生活中更多的情况是：系统状态并不是直接可见，我们往往只能观测到受状态影响的某些变量。显然对这些可观测到的变量使用马尔可夫模型对系统进行状态分析是不严谨的。为此，对于这种含有隐含未知参数的马尔可夫过程，数学家通过随机过程可观察的参数确定该过程的隐含参数，然后利用这些参数来作进一步分析，这也是大名鼎鼎的<code>隐马尔可夫模型（Hidden Markov model，HMM）</code>的本质，至于隐马尔可夫模型的更多详细内容，如果进行展开的话，需要花费大量篇幅，在此先不做展开。隐马尔可夫模型在语音识别和模式识别领域都有着应用，而卡尔曼滤波也是构建在隐马尔可夫模型之上的。</p>
<h3 id="线性卡尔曼滤波">线性卡尔曼滤波</h3>
<p>  <code>卡尔曼滤波是一个递归滤波器，其算法本质是通过动态系统的有限个观测值，估计动态系统的隐藏状态。</code>接下来，我们从动态系统的状态空间模型开始，一步步推导卡尔曼滤波算法。</p>
<h4 id="状态空间模型">状态空间模型</h4>
<p>  动态系统都具有一个基本特征：系统的状态，那么什么是系统的状态呢，其定义如下：</p>
<blockquote>
<p>  一个随机动态系统的状态被定义为最少量的信息，这些信息包含过去作用于该系统的输入的影响，并足以完全描述系统将来的行为。（该定义截取自机械工业出版社译制的《神经网络与机器学习》第三版
第十四章 动态系统状态估计的贝叶斯滤波）</p>
</blockquote>
<p>  通常我们使用状态空间模型来描述动态系统状态对外部世界的影响，一般而言，状态空间模型分为两个部分：</p>
<blockquote>
<ul>
<li><p>系统模型</p>
<p>  系统模型使用时域函数来描述动态系统状态的演变，其数学表示为一阶马尔可夫链：<span class="math inline">\(x_{n+1}=\zeta_{n}(x_n,\omega_n)\)</span>，其中<span class="math inline">\(n\)</span>表示离散时间，向量<span class="math inline">\(x_n\)</span>表示动态系统当前的状态，向量<span class="math inline">\(x_{n+1}\)</span>表示下一状态的值，向量<span class="math inline">\(\omega_n\)</span>表示过程噪声，<span class="math inline">\(\zeta\)</span>为<span class="math inline">\(x_n,\omega_n\)</span>的向量函数，会随时间改变。</p></li>
<li><p>测量模型</p>
<p>  测量模型描述了动态系统状态对外部世界的影响，公式如下：<span class="math inline">\(y_n = \xi(x_n,v_n)\)</span>，其中向量<span class="math inline">\(y_n\)</span>表示外部世界对动态系统的一组观测值，向量<span class="math inline">\(v_n\)</span>是外部世界噪声的测量值，<span class="math inline">\(\xi\)</span>是<span class="math inline">\(x_n,v_n\)</span>的向量函数，会随时间改变。</p></li>
</ul>
</blockquote>
<p>  对于状态空间模型，有着如下假设：</p>
<blockquote>
<ul>
<li>动态系统的任意时刻<span class="math inline">\(k\)</span>,其过程噪声<span class="math inline">\(\omega_k\)</span>与初始状态<span class="math inline">\(x_0\)</span>无关；</li>
<li>动态系统的过程噪声<span class="math inline">\(\omega_n\)</span>于测量噪声<span class="math inline">\(v_n\)</span>是统计独立，也就是说对<span class="math inline">\(\forall i,j\)</span>都有<span class="math inline">\(E[\omega_iv_j^T]=0\)</span>成立；</li>
</ul>
</blockquote>
<p>  一般而言，跟状态与状态之间的是否为线性关系及过程噪声、测量噪声二者是否服从高斯分布将状态空间模型分为四大类：</p>
<blockquote>
<ul>
<li>线性高斯模型</li>
<li>线性非高斯模型</li>
<li>非线性高斯模型</li>
<li>非线性非高斯模型</li>
</ul>
</blockquote>
<p>  显然不管是非线性还是非高斯或二者兼之的状态空间模型，其处理难度是远高于线性高斯模型的，我们先来处理最简单的状态空间模型——线性高斯模型。</p>
<h4 id="线性卡尔曼滤波的理论推导">线性卡尔曼滤波的理论推导</h4>
<p>对于线性高斯模型，其状态空间模型可设为如下形式：</p>
<p><span class="math display">\[
\begin{cases}
x_{n}=A_{n}x_{n-1}+\omega_{n} \\
y_n = H_nx_n+v_n
\end{cases}
\]</span>   其中<span class="math inline">\(\omega_n\sim N(0,Q),v_n\sim
N(0,R)\)</span>，<span class="math inline">\(A_{n+1}\)</span>是动态系统从状态<span class="math inline">\(x_n\)</span>到<span class="math inline">\(x_{n+1}\)</span>的状态转移矩阵，<span class="math inline">\(H_n\)</span>是测量矩阵，表示系统状态<span class="math inline">\(x_n\)</span>对<span class="math inline">\(y_n\)</span>的增益，将系统状态映射到外部世界。</p>
<p>  考虑到动态系统会受到系统中已知的控制器的控制信息的影响，需要在系统模型中加入这部分信息，修正后的状态空间模型如下：</p>
<p><span class="math display">\[
\begin{cases}
x_{n}=A_{n}x_{n-1}+B_{n}\mu_{n}+\omega_{n} \\ y_n = H_nx_n+v_n
\end{cases}
\]</span> 其中<span class="math inline">\(\mu_n\)</span>是系统的控制器向量，<span class="math inline">\(B_n\)</span>是系统的控制向量。</p>
<p>  为方便进行推导，定义<span class="math inline">\(\hat{x}^{-}_k \in
R^n\)</span>（<span class="math inline">\(^-\)</span>代表先验,^代表估计）为在已知第<span class="math inline">\(k\)</span>步以前状态情况下第<span class="math inline">\(k\)</span>步的先验状态估计。定义<span class="math inline">\(\hat{x}_k\in R^n\)</span>为已知观测变量<span class="math inline">\(y_k\)</span>时，第<span class="math inline">\(k\)</span>步的后验估计状态，由此定义先验估计误差和后验估计误差：</p>
<p><span class="math display">\[
\begin{cases}
  e_k^-\equiv{x_k-\hat{x}_k^-} \\ e_k\equiv{x_k-\hat{x}_k}
\end{cases}
\]</span> 先验估计误差的协方差为：</p>
<p><span class="math display">\[
P_k^-=E[e_k^-{e_k^-}^T]
\]</span> 后验误差估计的协方差为： <span class="math display">\[
P_k=E[e_ke_k^T]
\]</span>   显然<span class="math inline">\(P_k^-\)</span>是真实值和预测值之间的协方差，<span class="math inline">\(P_k\)</span>是真实值和最优估计值之间的协方差。卡尔曼滤波的核心就是如何根据<span class="math inline">\(k-1\)</span>时刻的最优状态估计<span class="math inline">\(\hat{x}_{k-1}\)</span>和第<span class="math inline">\(k\)</span>时刻的观测值<span class="math inline">\(y_k\)</span>得到<span class="math inline">\(k\)</span>时刻的最优状态估计值<span class="math inline">\(\hat{x}_k\)</span>,显然根据定义<span class="math inline">\(P_k\)</span>越小，估计值越接近于真实值，此时，只需求解当前条件下使得<span class="math inline">\(P_k\)</span>最小的状态估计值即是当前时刻状态的最优估计值。利用上文修正后的状态空间模型的系统模型得到<span class="math inline">\(k\)</span>时刻状态的预测值<span class="math inline">\(\hat{x}_k^-=A\hat{x}_{k-1}+B\mu_k\)</span>，为了得到<span class="math inline">\(k\)</span>时刻的最小协方差<span class="math inline">\(P_k\)</span>，卡尔曼滤波定义参数卡尔曼增益<span class="math inline">\(K=\frac{\hat{x}_k-\hat{x}_k^-}{y_k-H\hat{x}_k^-}\)</span>，其中，观测向量及其预测之差<span class="math inline">\(y_k-H\hat{x}_k^-\)</span>被称为测量过程的新息或残余。新息反应了预测值和实际值之间的不一致程度，为零时表明二者完全吻合，推导过程如下。</p>
<hr>
<p>由卡尔曼增益的定义式变形得到:</p>
<p><span class="math display">\[\hat{x}_k=\hat{x}_k^-+K(y_k-H\hat{x}_k^-)\]</span></p>
<p>将状态空间模型的观测模型代入得到:</p>
<p><span class="math display">\[\hat{x}_k=\hat{x}_k^-+K(Hx_k
+v_k-H\hat{x}_k^-)\]</span></p>
<p>进一步整理变换得到:</p>
<p><span class="math display">\[\hat{x}_k-x_k=\hat{x}_k^—x_k+KH(x_k-\hat{x}_k^-)+Kv_k\]</span></p>
<p>结合先验估计误差和后验估计误差的定义可知:</p>
<p><span class="math display">\[e_k=(I-KH)e_k^-+Kv_k\]</span></p>
<p>代入后验误差协方差的定义计算得到:</p>
<p><span class="math display">\[P_k=E[e_ke_k^T]=
E[[(I-KH)e_k^—Kv_k][(I-KH)e_k^—Kv_k]^T]\]</span></p>
<p>展开可知:</p>
<p><span class="math display">\[P_k=P^-_k-KHP^-_k-P^-_kH^TK^T+K(HP^-_kH^T+R)K^T=P(K)\]</span></p>
<p>要求<span class="math inline">\(P_k\)</span>的最小值，结合上式对卡尔曼增益<span class="math inline">\(K\)</span>求偏导，得到：</p>
<p><span class="math display">\[\frac{\partial{P_k}}{\partial{K}}=-2P^-_kH^T+2KHP_k^-H^T+2KR\]</span></p>
<blockquote>
<p>  需要注意的是，因为涉及到矩阵导数，与常规导数求导略有不同，有兴趣的读者，可以结合下面的矩阵求导规则进行求导：</p>
<p>若<span class="math inline">\(Y=AX\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=A^T\)</span>；</p>
<p>若<span class="math inline">\(Y=XA\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=A\)</span>；</p>
<p>若<span class="math inline">\(Y=A^TXB\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=AB^T\)</span>；</p>
<p>若<span class="math inline">\(Y=A^TX^TB\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=BA^T\)</span>；</p>
<p>若<span class="math inline">\(Y=X^TX\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=\)</span>X；</p>
<p>若<span class="math inline">\(Y=AX^T\)</span>,则<span class="math inline">\(\frac{d{Y}}{d{X}}=A\)</span>；</p>
<p>若<span class="math inline">\(Y=u(X)^Tv(X)\)</span>,则<span class="math inline">\(\frac{d{uv}}{d{X}}=\frac{du^T}{dX}v+\frac{dv^T}{X}u\)</span>；</p>
</blockquote>
<p>令<span class="math inline">\(\frac{\partial{P_k}}{\partial{K}}=0\)</span>，求解得到:</p>
<p><span class="math display">\[K=\frac{P^-_kH^T}{HP^-_KH^T+R}\]</span></p>
<blockquote>
<p>直观来看，一方面：</p>
<p><span class="math display">\[\lim\limits_{R\to0}K=\lim\limits_{R\to0}\frac{P_k^-H^T}{HP_k^-H^T+R}=H^{-1}\]</span></p>
<p>即，随着观测噪声协方差 <span class="math inline">\(R\)</span>的减小，卡尔曼增益逐渐增大，当<span class="math inline">\(R=0\)</span>时，取得最大值<span class="math inline">\(H^{-1}\)</span>;</p>
<p>另一方面：</p>
<p><span class="math display">\[\lim\limits_{P_k^-\to0}K=0\]</span></p>
<p>即，随着先验估计协方差<span class="math inline">\(P_k^-\)</span>的减小，卡尔曼增益随之减小，当<span class="math inline">\(P_k^-=0\)</span>时，取得最小值0；</p>
<p>结合卡尔曼增益的定义可知，<code>卡尔曼增益实际上表征了状态最优估计过程中模型预测误差与量测误差的比重，随着观测噪声协方差趋近于零，模型中预测误差的比重越来越大，此时模型更信任模型中观测值的信息；另一方面，随着先验估计误差协方差趋近于零，模型中预测误差的比重越来越小，此时模型更信任模型中预测值的信息。</code></p>
</blockquote>
<p>代入矩阵函数<span class="math inline">\(P(K)\)</span>得到：</p>
<p><span class="math display">\[P_k=(I-KH)P_k^-\]</span></p>
<p>至此，我们已经得到了第<span class="math inline">\(k\)</span>时刻最优状态估计值为：</p>
<p><span class="math display">\[\hat{x}_k
=\hat{x}_k^-+K(y_k-H\hat{x}_k^-)\]</span></p>
<p>  但是，到了这一步问题仍然没有得到解决，因为在计算<span class="math inline">\(k\)</span>时刻的卡尔曼增益时，仍有一个值未确定的：先验估计的协方差矩阵<span class="math inline">\(P_k^-\)</span>，在计算<span class="math inline">\(P_k^-\)</span>之前，需要先计算先验误差：</p>
<p><span class="math display">\[
e^-_k=x_k-\hat{x}_k^-=(Ax_{k-1}+B\mu_k+\omega_k)-(A\hat{x}_{k-1}+Bu_k)=Ae_{k-1}+\omega_k
\]</span> 因为</p>
<p><span class="math display">\[
P_k^-=E[e_k^-{e_k^-}^T]=E[(Ae_{k-1}+\omega_k)(Ae_{k-1}+\omega_k)^T]=E[Ae_{k-1}e_{k-1}^TA^T]+E[\omega_k\omega_k^T]
\]</span> 得到：</p>
<p><span class="math display">\[P_k^-=AP_{k-1}A^T+Q\]</span></p>
<p>到这步，卡尔曼滤波形成一个完整的理论闭环。</p>
<h3 id="离散线性卡尔曼滤波算法">离散线性卡尔曼滤波算法</h3>
<p>  卡尔曼滤波器采用反馈控制的方法来估计过程状态：滤波器估计过程某一时刻的状态，然后以（含噪声的）观测变量的方式获得反馈。因此卡尔曼滤波器可分为两个部分：</p>
<h4 id="时间更新方程"><code>时间更新方程</code></h4>
<p>  时间更新方程负责及时向前推算当前状态变量和误差协方差估计的值，以便为下一个时间状态构造先验估计，其数学表示如下：
<span class="math display">\[
\hat x_k^- = A \hat x_{k-1}+B\hat \mu_k+w_k \\
P_{k}^-=AP_{k-1}A^T+Q
\]</span></p>
<h4 id="测量更新方程"><code>测量更新方程</code></h4>
<p>  测量更新方程负责反馈——也就是说，它将先验估计
和新的测量变量结合以构造改进的后验估计，其数学表示如下：</p>
<p><span class="math display">\[K_k=P^-_kH^T(HP^-_kH^T+R)^{-1}\]</span></p>
<p><span class="math display">\[Z_k=Hx_k+v_k\]</span></p>
<p><span class="math display">\[\hat x = \hat x_k^{-}+K_k(Z_k-H\hat
x_k^{-})\]</span></p>
<p><span class="math display">\[P_k=(I-K_kH)P_k^{-}\]</span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">离散线性卡尔曼滤波器的递归计算过程如下图：</span><br></pre></td></tr></table></figure>
<p><img src="1.jpg" alt="离散卡尔曼滤波的递归过程">
  时间更新方程将当前状态变量作为先验估计及时地向前投射到测量更新方程，测量更新方程校正先验估计以获得状态的后验估计。完成时间更新方程和测量更新方程的一轮更新之后，再次重复这个过程，将上一次计算得到的后验估计作为下一次计算的先验估计。</p>
<h4 id="卡尔曼滤波器参数分析">卡尔曼滤波器参数分析</h4>
<p>  在实现卡尔曼滤波器算法时，会涉及到很多参数，如下：</p>
<blockquote>
<p><span class="math inline">\(x_0\)</span>：动态系统的初始状态变量<span class="math inline">\(x_0\)</span>一般直接取第一个测量值<span class="math inline">\(z_0\)</span>；</p>
<p><span class="math inline">\(P_0\)</span>：系统初始状态变量<span class="math inline">\(x_0\)</span>的协方差，只要初始值不为零，初始协方差矩阵的取值对滤波效果影响很小，都能很快收敛；</p>
<p><span class="math inline">\(A\)</span>：状态转移矩阵，是根据经验对下一个时间周期目标状态的
一种预测。在某些情况下，这种预测是确定的，而另一些情况下
这种预测是未知的；</p>
<p><span class="math inline">\(B\)</span>：输入控制矩阵，作用在控制向量<span class="math inline">\(\mu_k\)</span>上的<span class="math inline">\(n\times1\)</span>输入控制矩阵，由于控制信息是已知的，一般很好确定；
<span class="math inline">\(H\)</span>：<span class="math inline">\(m\times n\)</span>观测模型矩阵，
负责将系统的真实状态映射到观测空间，根据动态系统很容易确定；</p>
<p><span class="math inline">\(P ^-\)</span>：为<span class="math inline">\(n\times n\)</span>先验估计误差协方差矩阵，由<span class="math inline">\(P_0\)</span>递归即可得到；</p>
<p><span class="math inline">\(P\)</span> ：为<span class="math inline">\(n×n\)</span>后验估计误差协方差矩阵，由<span class="math inline">\(P_0\)</span>递归即可得到；</p>
<p><span class="math inline">\(Q\)</span>：<span class="math inline">\(n\times n\)</span>过程噪声<span class="math inline">\(\omega_n\)</span>是的协方差矩阵
，当动态系统的状态转换过程确定时，<span class="math inline">\(Q\)</span>是一个确定的值，此时，可通过离线测试确定对于某个过程的最优<span class="math inline">\(Q\)</span>值，一般来说，当状态转换过程为已确定时，<span class="math inline">\(Q\)</span> 的取值越小越好。 当 <span class="math inline">\(Q\)</span>
取值逐渐增大时，滤波收敛变慢，且状态变量的扰动变大。当系统的动态转换过程是不确定的，随时间变化，此时<span class="math inline">\(Q\)</span>不再是一个确定值，而是一个随时间变化的变量<span class="math inline">\(Q(k)\)</span>，此时的卡尔曼滤波为自适应卡尔曼滤波，关于自适应卡尔曼滤波的更详细内容请参考此文—<a href="http://read.pudn.com/downloads76/ebook/285147/Paper/pdf/y8576770004.pdf">自适应卡尔曼滤波技术</a>，在此不做进一步展开；</p>
<p><span class="math inline">\(R\)</span>：<span class="math inline">\(m\times m\)</span>测量噪声<span class="math inline">\(v_n\)</span>协方差矩阵，
跟系统外部环境的测量仪器相关，一般很难获得改值，根据平时的使用经历来看:</p>
<blockquote>
<p><span class="math inline">\(R\)</span>取值过小或过大都会使滤波效果变差；</p>
<p><span class="math inline">\(R\)</span>取值越小收敛越快，反之收敛越慢；</p>
<p>一般离线得到合适的<span class="math inline">\(R\)</span>值，再代入滤波器中；</p>
</blockquote>
<p><span class="math inline">\(I\)</span>：<span class="math inline">\(n\times n\)</span> 单位矩阵</p>
<p><span class="math inline">\(K\)</span>:<span class="math inline">\(n×m\)</span>阶矩阵， 卡尔曼增益</p>
</blockquote>
<p>  关于卡尔曼滤波参数的更详细内容，由于篇幅有限，只做了简单的描述，但是卡尔曼滤波器参数的确定却是重中之重，直接影响到算法的效果，要进行实际应用的读者可结合<a href="http://file.elecfans.com/web1/M00/80/05/pIYBAFwnp8uAL-vDAAZFhhuu8y8385.pdf">卡尔曼滤波器参数分析与应用方法研究</a>一文，来做进一步讨论。</p>
<h3 id="发散现象及平方根滤波">发散现象及平方根滤波</h3>
<h4 id="发散现象">发散现象</h4>
<p>  线性卡尔曼滤波基于线性高斯状态空间模型的假设推导的，但是在实际应用时动态系统不一定完全满足这些假定，这会儿导致卡尔曼滤波的不稳定，这也叫卡尔曼滤波的发散现象，另外在计算时如果数值不太精确也会产生发散现象。</p>
<h4 id="平方根滤波">平方根滤波</h4>
<p>  一个数学上优美且计算可行的，解决卡尔曼滤波发散问题的方法是使用平方根滤波，其本质思想是对卡尔曼滤波进行修正，在算法的每一次循环中使用数值稳定的正交变换。具体而言，就是使用乔里斯基分解将卡尔曼滤波的误差协方差矩阵转换为其平方根形式。理论推导部分可以参考机械工业出版社的《神经网络与机器学习》
第十四章 动态系统状态估计的贝叶斯滤波
中的平方根滤波部分，在此不做推导。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>卡尔曼滤波</tag>
        <tag>马尔可夫链</tag>
      </tags>
  </entry>
  <entry>
    <title>奇异值分解</title>
    <url>/posts/5a1d36e7/</url>
    <content><![CDATA[<p>  奇异值分解（Singular Value
Decomposition，SVD）是线性代数中一种重要的矩阵分解方法，区别于只适用于实对称矩阵的特征分解方法，奇异值分解可对任意实矩阵进行分解。
<span id="more"></span></p>
<h1 id="特征分解">特征分解</h1>
<p> 特征分解（eigendecomposition）又叫谱分解（Spectral
decomposition），是把一个矩阵根据其特征值和特征向量分解的过程，只有可以正交化的矩阵才可以进行特征分解。</p>
<blockquote>
<p><span class="math inline">\(A\)</span>为<span class="math inline">\(n\)</span>阶方阵，若存在<span class="math inline">\(n\)</span>维非零向量<span class="math inline">\(x\)</span>使得： <span class="math display">\[
Ax = \lambda x
\]</span> 则称<span class="math inline">\(\lambda\)</span>为矩阵<span class="math inline">\(A\)</span>的特征值，<span class="math inline">\(x\)</span>为<span class="math inline">\(A\)</span>属于<span class="math inline">\(\lambda\)</span>的特征向量（eigenvector）。</p>
</blockquote>
<p>  有了上述定义，接下来讨论如何计算一个矩阵的特征值和特征向量。由定义可知：</p>
<p><span class="math display">\[
Ax-\lambda x=0 \Rightarrow (A-\lambda I)x=0
\]</span></p>
<p>  其中<span class="math inline">\(I\)</span>为单位矩阵，显然上式的推导结果是一个<span class="math inline">\(n\)</span>元<span class="math inline">\(n\)</span>次的齐次线性方程组，<span class="math inline">\(x\)</span>为该方程组的一个非零解，则有$r(A-I)=r
&lt; n |A-I|=0 <span class="math inline">\(，其中\)</span>|A-I|=0<span class="math inline">\(称为\)</span>A<span class="math inline">\(的特征方程，\)</span>|A-I|<span class="math inline">\(称为\)</span>A$的特征多项式。基于此，可得到求解方阵A特征值和特征向量的步骤如下：</p>
<blockquote>
<p>1、计算方阵A的特征多项式<span class="math inline">\(|A-\lambda
I|\)</span>；</p>
<p>2、求出特征方程<span class="math inline">\(|A-\lambda
I|=0\)</span>的所有根（包括复根和重根），这些根<span class="math inline">\(\lambda_1,\lambda_2,\cdots,\lambda_n\)</span>即为<span class="math inline">\(A\)</span>的所有特征值；</p>
<p>3、对于<span class="math inline">\(A\)</span>的每一个特征值<span class="math inline">\(\lambda_i(1\leq i\leq
n)\)</span>，求解齐次线性方程组<span class="math inline">\((A-\lambda_i
I)x=0\)</span>，该方程组的每一个非零解都是<span class="math inline">\(A\)</span>属于特征值<span class="math inline">\(\lambda_i\)</span>的特征向量；</p>
</blockquote>
<p>  求出矩阵<span class="math inline">\(A\)</span>的特征值和特征向量后，若矩阵<span class="math inline">\(A\)</span>有<span class="math inline">\(n\)</span>个线性独立的特征向量，那么 <span class="math inline">\(A\)</span>是可以正交化的，此时 <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A = WDW^{-1}
\]</span></p>
<p>其中<span class="math inline">\(W\)</span>时<span class="math inline">\(n\)</span>个特征向量所组成的<span class="math inline">\(n \times n\)</span>维矩阵，<span class="math inline">\(D\)</span>为以这<span class="math inline">\(n\)</span>个特征值为主对角线元素的对角阵。</p>
<h1 id="奇异值分解">奇异值分解</h1>
<ul>
<li><p>定义</p>
<blockquote>
<p>若<span class="math inline">\(M\)</span>为一个<span class="math inline">\(m \times
n\)</span>阶的矩阵，则存在一个分解，使得： <span class="math display">\[
M = UDV^T
\]</span> 其中<span class="math inline">\(U\)</span>为<span class="math inline">\(m\)</span>阶酉矩阵、<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>阶酉矩阵、<span class="math inline">\(D\)</span>为<span class="math inline">\(m\times
n\)</span>的非负实对角矩阵。称此分解为奇异值分解，一般我们将<span class="math inline">\(V\)</span>中的每一个特征向量叫做<span class="math inline">\(M\)</span>的右奇异向量，将<span class="math inline">\(U\)</span>中的每个特征向量叫做左奇异向量，<span class="math inline">\(D\)</span>对角线上的元素称为<span class="math inline">\(M\)</span>的奇异值，当规定奇异值降序排列时，可唯一确定一个<span class="math inline">\(D\)</span>。</p>
</blockquote>
<p>  有了定义，接下来需要确定奇异值分解的三个矩阵<span class="math inline">\(U、D、V\)</span>。比较直观的想法是通过<span class="math inline">\(M\)</span>来构造一个方阵来进行特征分解，间接计算<span class="math inline">\(U、D、V\)</span>，由于<span class="math inline">\(MM^T,M^TM\)</span>分别为<span class="math inline">\(m\times m\)</span>和<span class="math inline">\(n\times n\)</span>的方阵，则有： <span class="math display">\[
\begin{equation}
\begin{split}
MM^T=UDV^T(UDV^T)^T=UDV^TVD^TU^T = UDD^TU^T=U\sum_1 U^T \\
M^TM = (UDV^T)^TUDV^T=VD^TU^TUDV^T=VDD^TV^T=V\sum_2 V^T
\end{split}
\end{equation}
\]</span> 注意到： <span class="math display">\[
M = UDV^T \Rightarrow MV = UDV^TV \Rightarrow MV= UD \Rightarrow Mv_i =
\delta_iu_i \Rightarrow \delta_i = \frac{Mv_i}{u_i}
\]</span> 其中，<span class="math inline">\(v_i,u_i\)</span>分别为<span class="math inline">\(V,U\)</span>中第<span class="math inline">\(i\)</span>个特征向量。这个式子提供了一种计算奇异值的方法，另一种思路是结合式（5）：
<span class="math display">\[
\sum = DD^T=D^2 \Rightarrow \delta_i = \sqrt{\lambda_i}
\]</span> 即，特征值矩阵为奇异值矩阵的平方，故可以通过计算<span class="math inline">\(M^TM\)</span>的特征值取平方根来计算奇异值。</p></li>
<li><p>SVD的计算步骤</p>
<blockquote>
<p>1.计算<span class="math inline">\(MM^T\)</span>和<span class="math inline">\(M^TM\)</span>；</p>
<p>2.分别计算<span class="math inline">\(MM^T\)</span>和<span class="math inline">\(M^TM\)</span>的特征向量和特征值；</p>
<p>3.<span class="math inline">\(MM^T\)</span>的特征向量组成<span class="math inline">\(U\)</span>，而<span class="math inline">\(M^TM\)</span>的特征向量组成<span class="math inline">\(V\)</span>；</p>
<p>4.对<span class="math inline">\(MM^T\)</span>和<span class="math inline">\(M^TM\)</span>的非零特征值求平方根，对应上述特征向量的位置，填入对角阵<span class="math inline">\(D\)</span>的位置；</p>
</blockquote></li>
<li><p>计算示例</p>
<p>  接下来以计算矩阵<span class="math inline">\(A=\begin{pmatrix}0&amp;1 \\ 1&amp;  1
\\1&amp;0\end{pmatrix}\)</span>的奇异值分解为例，来进一步熟悉：</p>
<p>第一步，先计算<span class="math inline">\(A\)</span>的两个转置积：
<span class="math display">\[
\begin{equation}
\begin{split}
&amp;A^TA=\begin{pmatrix} 0 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 0
\end{pmatrix}\begin{pmatrix}  0 &amp; 1 \\ 1 &amp; 1 \\ 1 &amp;
0\end{pmatrix} = \begin{pmatrix}2 &amp; 1 \\ 1&amp; 2\end{pmatrix} \\
&amp;AA^T = \begin{pmatrix}0 &amp; 1 \\ 1 &amp; 1 \\ 1 &amp;
0\end{pmatrix}\begin{pmatrix}0 &amp; 1 &amp;1 \\ 1 &amp; 1 &amp;
0\end{pmatrix}=\begin{pmatrix}1 &amp; 1 &amp; 0 \\ 1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; 1\end{pmatrix}
\end{split}
\end{equation}
\]</span> 第二步，分别计算两个转置积的特征值和特征向量： <span class="math display">\[
\begin{equation}
\begin{split}
&amp;\color{#F00}{(A^TA-\lambda I)x = 0} \Rightarrow
|A^TA-\lambda I|=0
\\
&amp;\Rightarrow
\begin{vmatrix}
2-\lambda &amp; 1 \\
1 &amp; 2-\lambda
\end{vmatrix} = 0 \Rightarrow \lambda^2-4\lambda+3=0
\end{split}
\end{equation}
\]</span> 容易得到式（9）中一元二次方程的根为<span class="math inline">\(\lambda_1 = 3,\lambda_2=1\)</span>，当<span class="math inline">\(\lambda=3\)</span>时，将特征根分别带入式（1）中，得到：
<span class="math display">\[
\begin{equation}
\begin{split}
&amp;&amp;(A^TA-3I)x = 0 \Rightarrow\begin{pmatrix}-1 &amp; 1 \\ 1 &amp;
-1\end{pmatrix}x=  0 \Rightarrow\begin{pmatrix}-1 &amp; 1 \\ 1 &amp;
-1\end{pmatrix}\begin{pmatrix}x_1 \\ x_2\end{pmatrix}=  0 \\
\end{split}
\end{equation}
\]</span> 此时的单位特征向量为： <span class="math display">\[
x_{\lambda=3} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}\end{pmatrix}
\]</span> 同理得到： <span class="math display">\[
x_{\lambda=1} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}}\end{pmatrix}
\]</span> 同理计算<span class="math inline">\(AA^T\)</span>的特征根和特征向量： <span class="math display">\[
\begin{equation}
\begin{split}
&amp;\lambda_1=3，u_1=
\begin{pmatrix}
\frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{6}} \\ \frac{1}{\sqrt{6}}
\end{pmatrix}；
&amp;\lambda_2 = 1，u_2 =
\begin{pmatrix}
\frac{1}{\sqrt{2}} \\ 0 \\ -\frac{1}{\sqrt{2}}
\end{pmatrix}；
&amp;\lambda_3 = 0，u_3 =
\begin{pmatrix}
\frac{1}{\sqrt{3}} \\ -\frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}}
\end{pmatrix}
\end{split}
\end{equation}
\]</span></p>
<p>第三步，使用两个转置积的单位特征向量构造<span class="math inline">\(U,V\)</span>矩阵： <span class="math display">\[
\begin{equation}
\begin{split}
&amp;U =(u_1,u_2,u_3)=
\begin{pmatrix}
\frac{1}{\sqrt{6}} &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{3}} \\
\frac{2}{\sqrt{6}} &amp; 0 &amp; -\frac{1}{\sqrt{3}} \\
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{3}}
\end{pmatrix} \\
&amp;V^T = (v_1,v_2)^T=
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}
\end{pmatrix}
\end{split}
\end{equation}
\]</span> 第四步，计算奇异值，直接使用<span class="math inline">\(\delta_i =
\sqrt{\lambda_i}\)</span>计算奇异值并组成对角阵<span class="math inline">\(D\)</span>： <span class="math display">\[
D =
\begin{pmatrix}
\sqrt{3} &amp; 0\\
0 &amp; 1\\
0 &amp; 0
\end{pmatrix}
\]</span> 最终得到矩阵<span class="math inline">\(A\)</span>的奇异值分解： <span class="math display">\[
A= UDV^T
=\begin{pmatrix}
\frac{1}{\sqrt{6}} &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{3}} \\
\frac{2}{\sqrt{6}} &amp; 0 &amp; -\frac{1}{\sqrt{3}} \\
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{3}}
\end{pmatrix}\begin{pmatrix}
\sqrt{3} &amp; 0\\
0 &amp; 1\\
0 &amp; 0
\end{pmatrix}
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}
\end{pmatrix}
\]</span></p></li>
</ul>
<h1 id="应用">应用</h1>
<p>  对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵：
<span class="math display">\[
A_{m \times n} = U_{m \times m}D_{m \times n}V^T_{n \times n} \approx
U_{m \times k} D_{k \times k} V^T_{k \times n}
\]</span> 这样处理的好处是，我们可以用三个较小的矩阵<span class="math inline">\(U_{m \times k},D_{k \times k},V_{k \times
n}^T\)</span>来表示一个大矩阵<span class="math inline">\(A\)</span>，如下图所示，使用三个灰色部分的小矩阵来表示大矩阵。</p>
<figure>
<img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105140822191-1774139119.png" alt="图片来源于刘建平Pinard">
<figcaption aria-hidden="true">图片来源于<a href="https://www.cnblogs.com/pinard/p/6251584.html">刘建平Pinard</a></figcaption>
</figure>
<p>  由于这个重要的性质，SVD可以用于PCA降维，来做图片数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
<blockquote>
<p>Note：</p>
<p>  需要注意的是，奇异值分解中特征值的求解是比较核心的地方，在工程应用中，往往需要进行奇异值分解都是大矩阵，对这类大矩阵，如果采用上面的方法求解特征值需要花费较多的时间和资源。对此，可以采用<a href="https://liangjiandeng.github.io/teaching/Numer_Analy/Chap24.pdf">乘幂法</a>和<a href="https://liangjiandeng.github.io/teaching/Numer_Analy/Chap24.pdf">反幂法</a>或者QR方法来近似求解矩阵的特征根，在此不做进一步展开，有兴趣的读者可以进一步了解一下。</p>
</blockquote>
<h1 id="基本概念说明">基本概念说明</h1>
<ul>
<li><p>矩阵的子式</p>
<p>  设有<span class="math inline">\(m \times n\)</span>矩阵A，在<span class="math inline">\(A\)</span>中任意取定<span class="math inline">\(k\)</span>个行和<span class="math inline">\(k\)</span>个列（<span class="math inline">\(k \leq
\min\{m,n\}\)</span>），位于这些行与列交叉处的元素按原来的相对顺序排成一个<span class="math inline">\(k\)</span>阶行列式，称它为矩阵<span class="math inline">\(A\)</span>的一个<span class="math inline">\(k\)</span>阶子式，特别地，<span class="math inline">\(A\)</span>中每一个元素就是<span class="math inline">\(A\)</span>的一阶子式。 　　对于确定的<span class="math inline">\(k\)</span>，在<span class="math inline">\(m \times
n\)</span>矩阵<span class="math inline">\(A\)</span>中，总共有<span class="math inline">\(C_m^k \times C_n^k\)</span>个<span class="math inline">\(k\)</span>阶子式，这些子式的值有的可能是零，也可能不为零，把值不为零的子式称为非零子式。</p></li>
<li><p>矩阵的秩</p>
<p>  在<span class="math inline">\(m\times n\)</span>矩阵<span class="math inline">\(A\)</span>中，非零子式的最高阶数称为矩阵<span class="math inline">\(A\)</span>的秩，记为<span class="math inline">\(r(A)\)</span>或秩规定零矩<span class="math inline">\((A)\)</span>,规定零矩阵的秩为零。</p>
<blockquote>
<p>推论1：</p>
<p><span class="math inline">\(r(A)=r
\Leftrightarrow  A\)</span>中所有<span class="math inline">\(r+1\)</span>阶子式（如果有的话）全为零，而<span class="math inline">\(A\)</span>中至少有一个<span class="math inline">\(r\)</span>阶子式非零。</p>
</blockquote></li>
<li><p>矩阵的谱半径</p>
<p>  <span class="math inline">\(A\)</span>为<span class="math inline">\(n\)</span>阶方阵，<span class="math inline">\(\lambda_i（1\leq i\leq
n）\)</span>为其特征值，则<span class="math inline">\(A\)</span>的谱半径定义如下： <span class="math display">\[
\rho(r) = max\{|\lambda_1|,|\lambda_2|,\dots,|\lambda_n|\}
\]</span> 即方阵<span class="math inline">\(A\)</span>的谱半径为<span class="math inline">\(A\)</span>特征值中绝对值最大的那个值。</p></li>
<li><p>正定矩阵</p>
<p>  如果对于所有的非零实系数向量 <span class="math inline">\(z\)</span>，都有 <span class="math inline">\(z^TAz&gt;0\)</span>，则称矩阵 <span class="math inline">\(A\)</span> 是正定的。正定矩阵的行列式必然大于0，
所有特征值也必然大于0。相对应的，半正定矩阵的行列式必然 ≥ 0。</p></li>
<li><p>正交矩阵</p>
<p>  若一个方阵其行与列皆为正交的单位向量（即二者的内积为0），则该方阵为正交矩阵。</p></li>
<li><p>酉矩阵</p>
<p>  酉矩阵（unitary matrix）是一种特殊的方阵，它满足<span class="math inline">\(UU^H=U^HU=I_n\)</span>（<span class="math inline">\(U^H\)</span>为<span class="math inline">\(U\)</span>的共轭转置，其在转置的基础上，增加了复数的共轭）。酉矩阵实际上是推广的正交矩阵（orthogonal
matrix）；当酉矩阵中的元素均为实数时，酉矩阵实际就是正交矩阵。另一方面，由于<span class="math inline">\(U^{-1}U=UU^{-1}=I_n\)</span>，所以酉矩阵 <span class="math inline">\(U\)</span>满足$
U<sup>{−1}=U</sup>H$；事实上，这是一个矩阵是酉矩阵的充分必要条件。</p></li>
<li><p>正规矩阵</p>
<p>  同酉矩阵一样，正规矩阵（normal
matrix）也是一种特殊的方阵，它要求在矩阵乘法的意义下与它的共轭转置矩阵满足交换律，即<span class="math inline">\(MM^H=M^HM\)</span>。显然，复系数的酉矩阵和实系数的正交矩阵都是正规矩阵。</p></li>
<li><p>谱定理和谱矩阵</p>
<p>  矩阵的对角化是线性代数中的一个重要命题。谱定理（spectral
theorem）给出了方阵对角化的一个结论：若矩阵<span class="math inline">\(M\)</span>是一个正规矩阵，则存在酉矩阵 <span class="math inline">\(U\)</span>，以及对角矩阵 <span class="math inline">\(\sum\)</span>，使得<span class="math inline">\(M=U\sum
U^H\)</span>。也就是说，正规矩阵，可经由酉变换，分解为对角矩阵；这种矩阵分解的方式，称为谱分解（spectral
decomposition）。</p>
<hr>
<h1 id="参考文章">参考文章</h1>
<ul>
<li><a href="https://www.cnblogs.com/pinard/p/6251584.html">奇异值分解原理与在降维中的应用</a></li>
<li><a href="https://bainingchao.github.io/2018/10/11/%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/">一步步教你轻松学奇异值分解SVD降维算法</a></li>
<li><a href="https://yanfei.site/docs/statscompbook/matrices.html#%E5%B9%82%E6%96%B9%E6%B3%95power-method">统计计算-奇异值分解</a></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>数学理论</category>
      </categories>
      <tags>
        <tag>svd</tag>
        <tag>特征分解</tag>
        <tag>特征向量</tag>
      </tags>
  </entry>
  <entry>
    <title>机器视觉之基本概念</title>
    <url>/posts/781628f/</url>
    <content><![CDATA[<p>  主要记录机器视觉领域的一些基本概念及一些实践中的tricks。
<span id="more"></span></p>
<h2 id="ccd">CCD</h2>
<p>  CCD（Charge-Coupled
Device）即电荷耦合器件,通常称为CCD图像传感器，是一种半导体器件，当入射光线照射到CCD芯片上的感光单元（微小光敏物质，即像素（Pixel）），光子会被转为电子，这些电子在像素内累计形成电荷。一块CCD芯片上包含的感光单元越多，其成像分辨率也就越高。其成像过程如下：</p>
<ul>
<li><p>光电转换</p>
<p>入射光线通过光电二极管转换为电荷</p></li>
<li><p>电荷积累</p>
<p>光敏元件根据光强的不同积累不同数量的电荷</p></li>
<li><p>电荷转移</p>
<p>通过电荷转移机制，逐行、逐列将电荷移至输出放大器</p></li>
<li><p>电压转换</p>
<p>输出放大器将电荷信号转为电压信号，并传递给模数转换器（ADC），生成数字图像数据</p></li>
</ul>
<h2 id="cmos">CMOS</h2>
<p>  CMOS（Complementary Metal-Oxide
Semiconductor，互补金属氧化物半导体）成像原理与CCD成像原理类似，但CMOS成像器件的电荷转移和电压转换过程在同一块硅片上完成，因此CMOS成像器件的结构更加简单，成本更低，功耗更低，适用于高分辨率、低功耗的成像应用。</p>
<p>CCD与CMOS成像器件的对比如下：</p>
<table>
<thead>
<tr>
<th style="text-align: center;">特性</th>
<th style="text-align: center;">CCD</th>
<th style="text-align: center;">CMOS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">图像质量</td>
<td style="text-align: center;">高、噪声低，均匀性好</td>
<td style="text-align: center;">略低，噪声高</td>
</tr>
<tr>
<td style="text-align: center;">制造成本</td>
<td style="text-align: center;">高</td>
<td style="text-align: center;">低</td>
</tr>
<tr>
<td style="text-align: center;">功耗</td>
<td style="text-align: center;">高</td>
<td style="text-align: center;">低</td>
</tr>
<tr>
<td style="text-align: center;">读取速度</td>
<td style="text-align: center;">较慢</td>
<td style="text-align: center;">快</td>
</tr>
<tr>
<td style="text-align: center;">动态范围</td>
<td style="text-align: center;">宽</td>
<td style="text-align: center;">较窄</td>
</tr>
<tr>
<td style="text-align: center;">弱光性能</td>
<td style="text-align: center;">优</td>
<td style="text-align: center;">稍弱</td>
</tr>
</tbody>
</table>
<p>通常来说，CCD通常用于高端相机、科学研究和天文观测等对图像质量要求极高的领域，而CMOS则广泛应用于消费类设备（如智能手机摄像头）、监控设备和工业检测等场景，其成本低、功耗低和速度快更具优势。需要注意的是由于CCD采用全局曝光，而CMOS相机使用卷帘曝光，所以在拍摄运动目标时，优先考虑CCD传感器。</p>
<blockquote>
<p>Note 1.
全局曝光是传感器上所有像素在同一时刻开启曝光并在同一时刻曝光结束，将物体某时刻的状态成像，对运动物体而言，类似于将物体冻结了，所以适合拍摄高速运动的物体。</p>
</blockquote>
<blockquote>
<p>Note 2.
卷帘曝光是逐行（hang）顺序开启曝光，不同行间曝光的开启时刻有个很小的延迟，所以不适合运动物体的拍摄。</p>
</blockquote>
<h2 id="像素">像素</h2>
<p>  像素是构成数字图像的基本单位，相机靶面上的一个感光单元为一个像素，每个像素点都拥有色调和嫉恶调等色彩信息，规则排列的像素集合可以形成一张完整的图像。</p>
<h2 id="像素直径">像素直径</h2>
<p>  像素直径，也称像元尺寸指每个CCD元件的大小，通常使用<span class="math inline">\(\mu
m\)</span>作为单位，严谨的说，此处的大小中包含了感光单元和信号传输通路，其与像素间距（即某个像素的中心到邻近一个像素的中心的距离）相同。如果像素直径较小，则图像将通过较小的像素进行描绘，因此可以获得更加精细的图像，显然通过像素间距和有效像素数可以直接求出CCD元件中感光区域的大小（即CCD元件的总面积）</p>
<h2 id="相机靶面尺寸">相机靶面尺寸</h2>
<p>  相机靶面尺寸指用于接收光信号的传感器的尺寸，指的是传感器（工作区域为矩形）对角线长度。一般分为采用英寸单位表示和采用APS-C大小等规格表示这2种方式。采用英寸表示时，该尺寸并不是拍摄的实际尺寸，而是相当于摄像管的对角长度。比如1/2英寸的CCD表示<strong>拥有相当于1/2英寸的摄像管的拍摄范围</strong>。z之所以会如此计算，是由于当初制造CCD的目的就是用于代替电视机、录像机的摄像管。而想要继续使用镜头等光学器件的需求比较强烈，由此诞生了这种奇怪的规格，主要的英寸规格如下表所示。</p>
<table>
<thead>
<tr>
<th style="text-align: center;">尺寸(英寸)</th>
<th style="text-align: center;">对角长度(mm)</th>
<th style="text-align: center;">拍摄区域(wxh,mm)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2/3</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">8.8x6.6</td>
</tr>
<tr>
<td style="text-align: center;">1/2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6.4x4.8</td>
</tr>
<tr>
<td style="text-align: center;">1/3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4.8x3.6</td>
</tr>
<tr>
<td style="text-align: center;">1/4</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">3.6x2.0</td>
</tr>
</tbody>
</table>
<h2 id="快门速度">快门速度</h2>
<p>  快门速度在工业视觉中也称为曝光时间，表示CCD或CMOS感光元件中积累电荷的时间，单位为秒。快门速度越快，感光元件中积累电荷的时间越短，图像的亮度越低，图像的细节越少，图像的噪点越多。快门速度越慢，感光元件中积累电荷的时间越长，图像的亮度越高，图像的细节越多，图像的噪点越少。直观来看，快门速度可以用于调整光量。</p>
<h2 id="曝光值">曝光值</h2>
<p>  曝光值（EV，Exposure
Value）是用于衡量图像整体亮度的一个综合指标，由一下三个主要因素决定：</p>
<ul>
<li>快门速度</li>
</ul>
<p>  控制光线进入的时间，决定了运动模糊的程度，曝光时间过长，如果相机的感光元件在曝光期间，被拍摄的物体发生了运动，导致在单帧中记录下了物体的移动轨迹。</p>
<ul>
<li>光圈（F值）</li>
</ul>
<p>  控制进入光线的量，决定了相机的景深</p>
<ul>
<li>感光度(ISO)</li>
</ul>
<p>  控制感光元件对光线的灵敏度</p>
<p>其定义公式如下： <span class="math display">\[
E_v = \log_2 \frac{N^2}{t}
\]</span></p>
<h2 id="增益">增益</h2>
<p>  增益，是指将图像信号进行电子增幅的过程，增益越大，图像的亮度越高，但噪声也会随之增加。在暗区拍摄图像时，往往通过增加增益来提高图像的亮度，但也会导致图像噪声的增加。因此，在拍摄图像时，需要根据实际情况合理选择增益值，以达到最佳的图像质量。</p>
<h2 id="面阵相机">面阵相机</h2>
<p>  面阵相机（Area Scan
Camera）指感光单元（像素）按矩阵形式排列的相机，其一次拍摄即可捕获整个二维图像，常见视觉检测场景基本都是使用面阵相机，也常用于结构光三维重建。</p>
<h2 id="线阵相机">线阵相机</h2>
<p>  线阵相机（Line Scan
Camera）指感光单元（像素）按线性形式排列的相机，其一次拍摄只能捕获一行图像，成像时，需要通过移动相机或被检测物体来获取整个二维图像。相较于面阵相机，其拥有更高的分辨率，成像质量更高，同时需要配合编码器来配合触发拍照，并使用线型光源，常用于纸张、纺织品、金属板等表面缺陷检测场景。</p>
<h2 id="通讯方式">通讯方式</h2>
<h3 id="usb接口">USB接口</h3>
<p>  支持热插拔、标准统一，可连接多个设备，同时相机也可以通过USB线缆供电</p>
<h3 id="gige千兆以太网接口">Gige千兆以太网接口</h3>
<p>  可以简单方便的进行多相机设置，支持100米线材输出，是一种基于千兆以太网通信协议开发的相机接口标准</p>
<h3 id="cameralink接口">Cameralink接口</h3>
<p>  专门针对高速图像数据传输设计的通讯接口，是一种串行通讯协议，采用LVDS接口标准，传输速度快、抗干扰能力强、功耗低。</p>
<h2 id="工作距离wd">工作距离（WD）</h2>
<p>  指的是镜头的最下端到景物之间的距离。一般的镜头是可以看到无限远的，也就是说是没有上限的。</p>
<figure>
<img src="1.png" alt="相机成像结构">
<figcaption aria-hidden="true">相机成像结构</figcaption>
</figure>
<h2 id="视场角fov">视场角（FOV）</h2>
<p>  指镜头所能覆盖的范围。(相机实际拍摄到的区域尺寸)一个摄像机镜头能涵盖多大范围的景物，通常以角度来表示，这个角度就叫镜头的视角FOV。焦距越长，FOV越小，焦距越短，FOV越大</p>
<h2 id="光圈">光圈</h2>
<p>  光圈是一个用来控制光线透过镜头进入机身内感光面光量的装置。当光线不足时，我们把光圈调大，自然可以让更多光线进入相机，反之亦然。一般通过调整通光孔径大小来调节光圈，完整的光圈数值系列如下：F1，F1.4，F2，F2.8，F4，F5.6，F8，F11，F16，F22，F32，F44，F64。用F表示，以【镜头焦距f和通光孔径D的比值】(即，相对孔径的倒数，也就是“焦距÷有效孔径”)来衡量，每个镜头上都标有最大F值，例如：8mm/F1.4代表最大孔径D为5.7mm.
&gt; F值越小、光圈越大、进光量越大；F值越大、光圈越小、进光量越小。</p>
<h2 id="焦距">焦距</h2>
<p>  焦距就是镜头到成像面的距离，比如：50mm镜头，8mm镜头还是75mm镜头等。这些就是镜头到成像面的距离，也就是焦距，单位是毫米。
&gt;
焦距的大小决定着视角大小，焦距越大，视角越小，观察范围越小；焦距越小，视角越大，观察范围也越大</p>
<figure>
<img src="2.png" alt="相机成像结构">
<figcaption aria-hidden="true">相机成像结构</figcaption>
</figure>
<h2 id="景深">景深</h2>
<p>  景深与视野相似，不同的是景深指的是纵深的范围，视野指的是横向的范围。在最小工作距离到最大工作距离之间的范围称为景深，景深内的物体都可以清晰成像。景深一般可以通过光圈调节，光圈越小，景深越大。景深与镜头使用光圈、镜头焦距、拍摄距离以及对像质的要求(表现为对容许弥散圆的大小)有关。在假定其它条件不变的情况下：</p>
<ul>
<li>光圈越大，景深越小；光圈越小，景深越大</li>
<li>镜头焦距越长，景深越小；焦距越短，景深越大</li>
<li>拍摄距离越远，景深越大；距离越近，景深越小</li>
</ul>
<p>  总而言之，对于对景深有要求的项目，尽可能使用小的光圈；在选择放大倍率的镜头时，在项目许可下尽可能选用低倍率镜头。如果项目要求比较苛刻时，倾向选择高景深的尖端镜头。</p>
<h2 id="同轴光源">同轴光源</h2>
<p>  同轴光照明,<code>是指经被测物反射的光线与镜头光轴平行的照明方式</code>,同轴光源是机器视觉光源里唯一一种能直接在镜头正下方使用的光源，能提供来自镜头方向的照明角度。</p>
<div class="img-container">
<img src="同轴光源.png" alt="同轴光源" style="display:block; margin:0 auto; width:30%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>同轴光源</code></pre>
</div>
</div>
<hr>
<div class="img-container">
<img src="同轴光源成像.png" alt="同轴光源成像" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>同轴光源成像</code></pre>
</div>
</div>
<p>  同轴光源的发光区在侧面，出射的光线经过有特殊涂层的分光镜或分光棱镜，一半光线透过后被吸光材料吸收，而另一半光线被反射后照射在被测物表面，经被测物表面反射后经镜头到相机芯片成像。根据实际需要，可设计为不同的发光尺寸。由于光学结构特殊，当扩大发光区宽度时，光源的高度也需要随之改变。
  同轴光源可以消除物体表面不平整引起的阴影，从而减少干扰；部分采用分光镜设计，减少光损失，提高成像清晰度，均匀照射物体表面。应用领域:系列光源最适宜用于反射度极高的物体，如金属、玻璃、胶片、晶片等表面的划伤检测，芯片和硅晶片的破损检测，Mark点定位，包装条码识别。</p>
<h2 id="背光源">背光源</h2>
<p>  用高密度LED阵列面提供高强度背光照明，能突出物体的外形轮廓特征，尤其适合作为显微镜的载物台。红白两用背光源、红蓝多用背光源，能调配出不同颜色，满足不同被测物多色要求。应用领域:机械零件尺寸的测量，电子元件、IC的外型检测，胶片污点检测，透明物体划痕检测等。</p>
<h2 id="条形光源">条形光源</h2>
<p>  条形光源是较大方形结构被测物的首选光源；颜色可根据需求搭配，自由组合；照射角度与安装随意可调。应用领域:金属表面检查，图像扫描，表面裂缝检测，LCD面板检测等。</p>
<h2 id="环形光源">环形光源</h2>
<p>  环形光源提供不同照射角度、不同颜色组合，更能突出物体的三维信息；高密度LED阵列，高亮度；多种紧凑设计，节省安装空间；解决对角照射阴影问题；可选配漫射板导光，光线均匀扩散。应用领域:PCB基板检测，IC元件检测，显微镜照明，液晶校正，塑胶容器检测，集成电路印字检查</p>
<h2 id="aoi专用光源">AOI专用光源</h2>
<p>  不同角度的三色光照明，照射凸显焊锡三维信息；外加漫射板导光，减少反光；不同角度组合；应用领域:用于电路板焊锡检测。</p>
<h2 id="球积分光源">球积分光源</h2>
<p>  具有积分效果的半球面内壁，均匀反射从底部360度发射出的光线，使整个图像的照度十分均匀。应用领域:合于曲面，表面凹凸，弧形表面检测，或金属、玻璃表面反光较强的物体表面检测。</p>
<h2 id="线形光源">线形光源</h2>
<p>  超高亮度，采用柱面透镜聚光，适用于各种流水线连续检测场合。应用领域:阵相机照明专用，AOI专用。</p>
<h2 id="点光源">点光源</h2>
<p>  大功率LED，体积小，发光强度高；光纤卤素灯的替代品，尤其适合作为镜头的同轴光源等；高效散热装置，大大提高光源的使用寿命。应用领域:适合远心镜头使用，用于芯片检测，Mark点定位，晶片及液晶玻璃底基校正。</p>
<h2 id="组合条形光源">组合条形光源</h2>
<p>  
四边配置条形光，每边照明独立可控；可根据被测物要求调整所需照明角度，适用性广。应用案例:CB基板检测，IC元件检测，焊锡检查，Mark点定位，显微镜照明，包装条码照明，球形物体照明等。</p>
<h2 id="对位光源">对位光源</h2>
<p>  对位速度快；视场大；精度高；体积小，便于检测集成；亮度高，可选配辅助环形光源。应用领域:VA系列光源是全自动电路板印刷机对位的专用光源。</p>
]]></content>
      <categories>
        <category>机器视觉</category>
      </categories>
      <tags>
        <tag>相机</tag>
        <tag>视觉</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title>理解自注意力机制</title>
    <url>/posts/15775ef77/</url>
    <content><![CDATA[<p>  自Transformer架构的开山之作《<a href="https://arxiv.org/abs/1706.03762">Attention Is All You
Need</a>》发表以来，自注意力机制已成为深度学习模型的基石技术，尤其在自然语言处理领域展现革命性突破。鉴于该机制已渗透至各类先进模型架构，深入理解其运作原理变得至关重要。</p>
<span id="more"></span>
<p>  注意力机制的概念源起于神经机器翻译中对长序列处理的探索<a href="https://arxiv.org/abs/1409.0473"></a>。传统逐字翻译的局限在于：不同语言特有的语法结构与语义逻辑（如中文"吃食堂"的动宾搭配）在直译中易被破坏，导致语义失真。</p>
<hr>
<div class="img-container">
<img src="sample-01.png" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>逐字翻译（上）与注意力机制翻译（下）效果对比：后者能捕捉&quot;making&quot;与&quot;more&quot;&quot;difficult&quot;的语义关联</code></pre>
</div>
</div>
<hr>
<p>  Transformer架构的革命性突破在于完全摒弃了循环神经网络的时序依赖。其提出的自注意力机制，通过动态计算序列元素间的关联权重，使模型能够：
1. <strong>全局感知</strong>：每个位置都可以直接访问全部序列信息 2.
<strong>动态聚焦</strong>：根据上下文自动强化关键特征 3.
<strong>并行计算</strong>：突破RNN的序列计算瓶颈，支持并行计算</p>
<hr>
<div class="img-container">
<img src="sample-02.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>论文中的注意力热力图直观呈现了单词&quot;making&quot;与上下文的多维关联（颜色深度表示注意力权重强度）</code></pre>
</div>
</div>
<hr>
<p>  从数学视角看，自注意力机制实质是构建动态特征增强网络：通过<span class="math inline">\((q,k,v)\)</span>之间的矩阵运算，将原始词嵌入向量转换为包含上下文信息的增强表征向量。这种特性使其天然适配语言任务，例如"bank"在"river
bank"与"bank loan"中通过注意力权重获得截然不同的语义编码。</p>
<p>  尽管后续研究涌现出稀疏注意力、局部注意力等改进变体，但原始缩放点积注意力仍是工业界首选，其优势在于：</p>
<ul>
<li>计算复杂度O(n²)在大规模分布式训练中可通过并行化缓解<br>
</li>
<li>相比近似方法保留完整语义关联<br>
</li>
<li>与硬件优化技术（如FlashAttention）高度适配</li>
</ul>
<p>本文将以经典论文公式为框架展开解析： <span class="math display">\[
Attention(Q,K,V)=softmax(\dfrac{QK^T}{\sqrt{d} + \epsilon})V
\]</span> 对该公式的逐项解构将揭示自注意力机制的本质特性。</p>
<hr>
<p>扩展阅读推荐：<br>
- 演进脉络：《<a href="https://arxiv.org/abs/2009.06732">高效Transformers综述</a>》<br>
- 工程实践：《<a href="https://arxiv.org/abs/2302.01107">高效训练技术解析</a>》<br>
- 最新突破：<a href="https://arxiv.org/abs/2205.14135">FlashAttention</a>系列优化技术</p>
<hr>
<h2 id="embedding-an-input-sentence">Embedding an Input Sentence</h2>
<p>  接下来以句子<code>Life is short, eat dessert first</code>为例，解释自注意力机制的原理。在处理文本时，由于计算机无法直接处理字符，一般会将字符映射到数域<span class="math inline">\(\phi\)</span>中来处理。为了简单起见，在这个例子中数域<span class="math inline">\(\phi\)</span>仅基于当前输入句子中的单词构建，在实际应用中，一般使用训练数据集中的所有单词来构建数域。</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sentence = <span class="string">&#x27;Life is short, eat dessert first&#x27;</span></span><br><span class="line">dc = &#123;s:i <span class="keyword">for</span> i,s <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">sorted</span>(sentence.replace(<span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;&#x27;</span>).split()))&#125;</span><br><span class="line"><span class="built_in">print</span>(dc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;Life&#x27;</span>: 0, <span class="string">&#x27;dessert&#x27;</span>: 1, <span class="string">&#x27;eat&#x27;</span>: 2, <span class="string">&#x27;first&#x27;</span>: 3, <span class="string">&#x27;is&#x27;</span>: 4, <span class="string">&#x27;short&#x27;</span>: 5&#125;</span><br></pre></td></tr></table></figure>
<p>接下来，我们使用这个字典为每个单词分配一个整数索引：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">sentence_int = torch.tensor(</span><br><span class="line">    [dc[s] <span class="keyword">for</span> s <span class="keyword">in</span> sentence.replace(<span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;&#x27;</span>).split()]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(sentence_int)</span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([0, 4, 5, 2, 1, 3])</span><br></pre></td></tr></table></figure>
<p>  现在，得到输入句子向量形式，接下来使用嵌入层将输入编码为实向量嵌入。在这里，我们将使用一个简单的三维嵌入层，使得每个输入词都由一个三维向量表示。请注意，嵌入层的大小通常在几百到几千之间。例如，Llama2使用的嵌入大小为4096。为了方便说明在这里使用三维嵌入，这样我们就可以快速查看单个向量。例句由6个单词组成，输入通过嵌入层映射为<span class="math inline">\(6 \times 3\)</span>的张量：</p>
<p><strong>Code</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab_size = <span class="number">6</span></span><br><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">embed = torch.nn.Embedding(vocab_size, <span class="number">3</span>)</span><br><span class="line">embedded_sentence = embed(sentence_int).detach()</span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(embedded_sentence)</span><br><span class="line"><span class="built_in">print</span>(embedded_sentence.shape)</span><br></pre></td></tr></table></figure></p>
<p><strong>输出</strong> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.3374, -0.1778, -0.3035],</span><br><span class="line">        [ 0.1794,  1.8951,  0.4954],</span><br><span class="line">        [ 0.2692, -0.0770, -1.0205],</span><br><span class="line">        [-0.2196, -0.3792,  0.7671],</span><br><span class="line">        [-0.5880,  0.3486,  0.6603],</span><br><span class="line">        [-1.1925,  0.6984, -1.4097]])</span><br><span class="line">torch.Size([6, 3])</span><br></pre></td></tr></table></figure> ## Defining the Weight Matrices
  完成了输入数据的高维映射后，我们将深入探讨Transformer架构的核心组件之一：缩放点积注意力（Scaled
Dot-Product Attention）。该机制由三个可训练权重矩阵<span class="math inline">\(W_q、W_k、W_v\)</span>构成，这些矩阵作为模型的参数在训练过程中进行优化。通过这三个矩阵，输入序列被分别投影到对应的查询（query）、键（key）和值（value）向量空间。</p>
<p>  具体而言，查询、键和值序列的生成可通过以下方式实现：将输入嵌入向量x分别与权重矩阵<span class="math inline">\(W_q、W_k、W_v\)</span>进行矩阵乘法运算，公式表达为：</p>
<ul>
<li>query向量</li>
</ul>
<p><span class="math display">\[q^{(i)} = x^{(i)}\cdot W_q \quad
i\in[1,T]\]</span></p>
<ul>
<li>key向量</li>
</ul>
<p><span class="math display">\[k^{(i)} = x^{(i)}\cdot W_k \quad
i\in[1,T]\]</span></p>
<ul>
<li>value向量</li>
</ul>
<p><span class="math display">\[v^{(i)} = x^{(i)}\cdot W_v \quad
i\in[1,T]\]</span></p>
<p>索引<span class="math inline">\(i\)</span>指输入序列中的标记索引位置，其长度为<span class="math inline">\(T\)</span> 。</p>
<div class="img-container">
<img src="sample-03.png" alt="sample" style="display:block; margin:0 auto; width:30%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>计算query、key和value</code></pre>
</div>
</div>
<p>  其中，<span class="math inline">\(q^{(i)}\)</span>和<span class="math inline">\(k^{(i)}\)</span>维度为<span class="math inline">\(d_k\)</span>。投影矩阵<span class="math inline">\(W_q\)</span>和<span class="math inline">\(W_k\)</span>的维度为<span class="math inline">\(d
\times d_k\)</span> ，而<span class="math inline">\(W_v\)</span>维度为<span class="math inline">\(d
\times d_v\)</span>。需要注意的是，<span class="math inline">\(d\)</span>代表每个词向量<span class="math inline">\(x\)</span>的大小。由于我们计算的是查询向量和键向量之间的点积，因此这两个向量必须包含相同数量的元素
( <span class="math inline">\(d_q=d_k\)</span>)。在LLM中，我们对值向量使用相同的大小，例如<span class="math inline">\(d_q=d_k=d_v\)</span>然而，值向量<span class="math inline">\(v(i)\)</span>中的元素数量（它决定了最终上下文向量的大小）可以是任意的。</p>
<p>因此，对于以下代码演练，我们将设置<span class="math inline">\(d_q=d_k=2\)</span>并使用<span class="math inline">\(d_v = 4\)</span>，初始化投影矩阵如下：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">​</span><br><span class="line">d = embedded_sentence.shape[<span class="number">1</span>]</span><br><span class="line">​<span class="comment"># 与之前的词嵌入向量类似，$d_q、d_k、d_v$通常会比较大，为了便于说明，我们在这里使用较小的数字。</span></span><br><span class="line">d_q, d_k, d_v = <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">​</span><br><span class="line">W_query = torch.nn.Parameter(torch.rand(d, d_q))</span><br><span class="line">W_key = torch.nn.Parameter(torch.rand(d, d_k))</span><br><span class="line">W_value = torch.nn.Parameter(torch.rand(d, d_v))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="computing-the-unnormalized-attention-weights">Computing the
Unnormalized Attention Weights</h2>
<p>以<span class="math inline">\(x^{(2)}\)</span>的<span class="math inline">\(qkv\)</span>计算为例：</p>
<div class="img-container">
<img src="sample-04.jpg" alt="sample" style="display:block; margin:0 auto; width:30%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>注意力计算</code></pre>
</div>
</div>
<p>代码实现如下：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_2 = embedded_sentence[<span class="number">1</span>]</span><br><span class="line">query_2 = x_2 @ W_query</span><br><span class="line">key_2 = x_2 @ W_key</span><br><span class="line">value_2 = x_2 @ W_value</span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(query_2.shape)</span><br><span class="line"><span class="built_in">print</span>(key_2.shape)</span><br><span class="line"><span class="built_in">print</span>(value_2.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">torch.Size([2])</span><br><span class="line">torch.Size([2])</span><br><span class="line">torch.Size([4])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  为了下一步计算非标准化注意力权重，计算所有输入的<span class="math inline">\(qkv\)</span></p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keys = embedded_sentence @ W_key</span><br><span class="line">values = embedded_sentence @ W_value</span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;keys.shape:&quot;</span>, keys.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;values.shape:&quot;</span>, values.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">keys.shape: torch.Size([6, 2])</span><br><span class="line">values.shape: torch.Size([6, 4])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>得到了所有输入的<span class="math inline">\(qkv\)</span>，接下来开始计算非标准化注意力权重<span class="math inline">\(\omega\)</span>，如下图所示：</p>
<div class="img-container">
<img src="sample-05.jpg" alt="sample" style="display:block; margin:0 auto; width:40%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>计算非标准化注意力权重ω</code></pre>
</div>
</div>
<p>如上图所示，我们计算<span class="math inline">\(\omega_{i,j}\)</span>作为查询和键序列之间的点积:</p>
<p><span class="math display">\[\omega_{i,j}
=q^{(i)}k^{(j)}\]</span></p>
<p>例如，我们可以按如下方式计算查询和第5个输入元素（对应于索引位置4）的非规范化注意力权重：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">omega_24 = query_2.dot(keys[<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(omega_24)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor(1.2903)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由于我们稍后需要这些未标准化的注意力权重<span class="math inline">\(\omega\)</span>来计算实际的注意力权重，因此让我们计算所有输入标记的<span class="math inline">\(\omega\)</span>值，如上图所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">omega_2 = query_2 @ keys.T</span><br><span class="line"><span class="built_in">print</span>(omega_2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="computing-the-attention-weights">Computing the Attention
Weights</h2>
<p>  自注意力机制的后续步骤是应用softmax函数对未归一化的注意力权重<span class="math inline">\(\omega\)</span>进行归一化从而获得归一化的注意力权重<span class="math inline">\(\alpha\)</span>。此外，在通过softmax函数进行归一化之前，先使用<span class="math inline">\(\dfrac{1}{\sqrt{d_k}}\)</span>对<span class="math inline">\(\omega\)</span>进行缩放，如下所示：</p>
<div class="img-container">
<img src="sample-06.png" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>计算标准化注意力权重α</code></pre>
</div>
</div>
<p>通过<span class="math inline">\(d_k\)</span>进行缩放可确保权重向量的欧氏长度大致相同。这有助于防止注意力权重过小或过大，从而避免数值不稳定或影响模型在训练期间的收敛能力。</p>
<p>在代码中，我们可以按如下方式实现注意力权重的计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">​</span><br><span class="line">attention_weights_2 = F.softmax(omega_2 / d_k**<span class="number">0.5</span>, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(attention_weights_2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229])</span><br></pre></td></tr></table></figure>
<p>最后一步是计算上下文向量<span class="math inline">\(z^{(2)}\)</span>
，它是原始查询输入<span class="math inline">\(x_{(2)}\)</span>的注意力加权版本，通过注意力权重将所有其他输入元素作为其上下文：</p>
<div class="img-container">
<img src="sample-07.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>注意力权重特定于某个输入元素。这里，我们选择了输入元素x(2)</code></pre>
</div>
</div>
<p>在代码中，它看起来如下所示：</p>
<p><strong>代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">context_vector_2 = attention_weights_2 @ values</span><br><span class="line">​</span><br><span class="line">print(context_vector_2.shape)</span><br><span class="line">print(context_vector_2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([4])</span><br><span class="line">tensor([0.5313, 1.3607, 0.7891, 1.3110])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>请注意，由于我们之前指定了<span class="math inline">\(d_v &gt;
d\)</span> ，因此此输出向量具有比原始输入向量（<span class="math inline">\(d=3\)</span>）更多的维度(<span class="math inline">\(d_v=4\)</span>),但是，嵌入大小选择<span class="math inline">\(d_v\)</span>是任意的。</p>
<h2 id="self-attention">Self-Attention</h2>
<p>现在，为了总结上面章节中自注意力机制的代码实现，我们可以将前面的代码总结在一个紧凑的<code>SelfAttention</code>类中：</p>
<p><strong>代码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">​</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>(nn.Module):</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out_kq, d_out_v</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_out_kq = d_out_kq</span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))</span><br><span class="line">        <span class="variable language_">self</span>.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Parameter(torch.rand(d_in, d_out_v))</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        keys = x @ <span class="variable language_">self</span>.W_key</span><br><span class="line">        queries = x @ <span class="variable language_">self</span>.W_query</span><br><span class="line">        values = x @ <span class="variable language_">self</span>.W_value</span><br><span class="line">        </span><br><span class="line">        attn_scores = queries @ keys.T  <span class="comment"># unnormalized attention weights    </span></span><br><span class="line">        attn_weights = torch.softmax(</span><br><span class="line">            attn_scores / <span class="variable language_">self</span>.d_out_kq**<span class="number">0.5</span>, dim=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        context_vec = attn_weights @ values</span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<p>按照 PyTorch
的约定，<code>SelfAttention</code>上述类在方法中初始化自注意力参数<code>__init__</code>，并通过该方法计算所有输入的注意力权重和上下文向量<code>forward</code>。我们可以按如下方式使用此类：</p>
<p><strong>代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(123)</span><br><span class="line">​</span><br><span class="line"># reduce d_out_v from 4 to 1, because we have 4 heads</span><br><span class="line">d_in, d_out_kq, d_out_v = 3, 2, 4</span><br><span class="line">​</span><br><span class="line">sa = SelfAttention(d_in, d_out_kq, d_out_v)</span><br><span class="line">print(sa(embedded_sentence))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[-0.1564,  0.1028, -0.0763, -0.0764],</span><br><span class="line">        [ 0.5313,  1.3607,  0.7891,  1.3110],</span><br><span class="line">        [-0.3542, -0.1234, -0.2627, -0.3706],</span><br><span class="line">        [ 0.0071,  0.3345,  0.0969,  0.1998],</span><br><span class="line">        [ 0.1008,  0.4780,  0.2021,  0.3674],</span><br><span class="line">        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=&lt;MmBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果您查看第二行，您会发现它与<code>context_vector_2</code>上一节中的值完全匹配：<code>tensor([0.5313, 1.3607, 0.7891, 1.3110])</code>。</p>
<h2 id="multi-head-attention">Multi-Head Attention</h2>
<p>  Transformer使用了多头注意力的模块。这个“多头”注意力模块与我们上面讨论过的自注意力机制（尺度点积注意力）有何关系？在自注意力机制中，输入序列使用三个矩阵进行变换，分别代表查询、键和值。在多头注意力机制中，这三个矩阵可以被视为一个注意力头。下图总结了我们之前介绍和实现的这个注意力头：</p>
<div class="img-container">
<img src="sample-08.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>总结之前实现的自注意力机制</code></pre>
</div>
</div>
<p>顾名思义，多头注意力机制包含多个这样的头，每个头由查询、键和值矩阵组成。这一概念类似于在卷积神经网络中使用多个核，从而生成具有多个输出通道的特征图。</p>
<div class="img-container">
<img src="sample-09.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<pre><code>多头注意力</code></pre>
</div>
</div>
<p>为了在代码中说明这一点，我们可以<code>MultiHeadAttentionWrapper</code>为之前的<code>SelfAttention</code>类编写一个类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionWrapper</span>(nn.Module):</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out_kq, d_out_v, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.heads = nn.ModuleList(</span><br><span class="line">            [SelfAttention(d_in, d_out_kq, d_out_v) </span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]</span><br><span class="line">        )</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat([head(x) <span class="keyword">for</span> head <span class="keyword">in</span> <span class="variable language_">self</span>.heads], dim=-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这些<code>d_*</code>参数与之前的类中相同<code>SelfAttention</code>——这里唯一的新输入参数是注意力头的数量：</p>
<ul>
<li><p><code>d_in</code>：输入特征向量的维数。</p></li>
<li><p><code>d_out_kq</code>：query和key输出的维度。</p></li>
<li><p><code>d_out_v</code>：value输出的维度。</p></li>
<li><p><code>num_heads</code>：注意力头的数量。</p></li>
</ul>
<p>  我们使用这些输入参数初始化类<code>SelfAttention</code>时间<code>num_heads</code>。并使用
PyTorch<code>nn.ModuleList</code>来存储这些多个<code>SelfAttention</code>实例。然后，<code>forward</code>过程涉及将每个<code>SelfAttention</code>头（存储在
中<code>self.heads</code>）独立地应用于输入<code>x</code>。然后，每个头的结果沿着最后一个维度（<code>dim=-1</code>）连接起来。让我们在下面看看它的实际效果！</p>
<p>首先，为了便于说明，假设我们有一个输出维度为 1 的自注意力头：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">​</span><br><span class="line">d_in, d_out_kq, d_out_v = <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span></span><br><span class="line">​</span><br><span class="line">sa = SelfAttention(d_in, d_out_kq, d_out_v)</span><br><span class="line"><span class="built_in">print</span>(sa(embedded_sentence))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[-0.0185],</span><br><span class="line">        [ 0.4003],</span><br><span class="line">        [-0.1103],</span><br><span class="line">        [ 0.0668],</span><br><span class="line">        [ 0.1180],</span><br><span class="line">        [-0.1827]], grad_fn=&lt;MmBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>现在，让我们将其扩展到 4 个注意力头：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">​</span><br><span class="line">block_size = embedded_sentence.shape[<span class="number">1</span>]</span><br><span class="line">mha = MultiHeadAttentionWrapper(</span><br><span class="line">    d_in, d_out_kq, d_out_v, num_heads=<span class="number">4</span></span><br><span class="line">)</span><br><span class="line">​</span><br><span class="line">context_vecs = mha(embedded_sentence)</span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(context_vecs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;context_vecs.shape:&quot;</span>, context_vecs.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[-0.0185,  0.0170,  0.1999, -0.0860],</span><br><span class="line">        [ 0.4003,  1.7137,  1.3981,  1.0497],</span><br><span class="line">        [-0.1103, -0.1609,  0.0079, -0.2416],</span><br><span class="line">        [ 0.0668,  0.3534,  0.2322,  0.1008],</span><br><span class="line">        [ 0.1180,  0.6949,  0.3157,  0.2807],</span><br><span class="line">        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=&lt;CatBackward0&gt;)</span><br><span class="line">context_vecs.shape: torch.Size([6, 4])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  从上面的输出可以看出，之前创建的单头注意力机制的输出张量现在对应于多头注意力输出张量的第一列。</p>
<p>  需要注意的是，多头注意力的输出是一个<span class="math inline">\(6\times4\)</span>的张量：因为有6个输入token和4个自注意力头，其中每个自注意力头返回一个一维输出。在前面的自注意力计算中，同样生成了一个<span class="math inline">\(6 \times
4\)</span>的张量，那是因为在该实例中输出维度被设置为4而不是1。在实践中，如果可以在<code>SelfAttention</code>类中也可以调节输出维度的大小，那么使用多头注意力的意义是什么呢？</p>
<p>  单自注意力头增加输出维度和使用多头注意力之间的区别在于模型处理和学习数据的方式。虽然，二者都能增加模型表示数据不同特征的能力，但它们本质上是不同的方法。</p>
<p>  多头注意力中的每个注意力都有可能学会关注输入序列的不同分布，捕捉数据中各个维度的关系，这种表现的多样性是多头注意力机制成功的关键。</p>
<p>  多头注意力在并行计算方面也更加高效。每个头都可以单独处理，这使得它非常适合使用GPU或TPU等硬件加速器来加速计算。</p>
<p>  简而言之，多头注意力的使用不只是为了让模型的能力更强，更显著增强了其学习数据中各种特征和关系的能力。在7B的Llama2模型中，使用32个自注意力头。</p>
<h3 id="技术原理的深层解读">技术原理的深层解读</h3>
<p>​+ 特征解耦</p>
<p>  每个头的查询、键、值矩阵（Q/K/V）通过独立线性变换生成，使不同头能学习到输入序列的不同投影空间。例如，一个头可能聚焦局部词序依赖，另一个头可能捕捉长距离语义关联。</p>
<ul>
<li>​容错机制</li>
</ul>
<p>  实验表明，即使某些头的注意力权重失效，其他头仍能提供有效特征，增强了模型鲁棒性。
+ ​计算效率</p>
<p>  虽然总参数量与单头扩展维度相当，但多头拆分维度（如512维拆分为32头×16维）可降低单个矩阵乘法的计算复杂度。</p>
<ul>
<li>工程实践启示</li>
</ul>
<p>  在Transformer架构中，头的数量需平衡模型容量与计算资源。例如，ViT模型通常采用12-16头，而百亿参数大模型可能使用128头。值得注意的是，头数超过输入嵌入维度时会出现维度碎片化问题，因此现代模型常采用分组查询注意力（GQA）进行优化。</p>
<h2 id="causal-self-attention">Causal Self-Attention</h2>
<p>  为了在gpt（解码器风格）的文本生成大模型中应用注意力机制，在本节中，我们将引入<code>Causal Self-Attention</code>，本质上是对前文讨论的自注意机制进行适应性改造，以贴合使用场景。在原始的Transformer结构中，它对应于“屏蔽多头注意”模块，为了简单起见，我们将在本节中查看单个注意头，但是相同的概念可以推广到多个注意头。</p>
<div class="img-container">
<img src="sample-10.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
The causal self-attention module in the original transformer
architecture (via “Attention Is All You Need”,
https://arxiv.org/abs/1706.03762)
</div>
</div>
<p>  <code>Causal Self-Attention</code>确保序列中某个位置的预测输出仅依赖于先前位置的已知输出，跟未来位置的输出无关。简单来说，它确保对下一个单词的预测只依赖于前面的单词。为了在gpt这类大语言模型中实现这一点，在处理每个token时，屏蔽输入文本中位于当前token之后的token，我们将这种这种屏蔽策略称为<code>causal mask</code>。</p>
  下面的图示说明了<code>causal mask</code>如何作用于注意力权重中，以隐藏输出中的未来输入token。
<div class="img-container">
<img src="sample-11.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  为了方便说明和实现<code>Causal Self-Attention</code>力，我们使用前一节中的未加权注意力分数和注意力权重，首先，我们快速回顾一下前面Self-Attention部分的注意力分数的计算：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">123</span>)</span><br><span class="line">​</span><br><span class="line">d_in, d_out_kq, d_out_v = <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">​</span><br><span class="line">W_query = nn.Parameter(torch.rand(d_in, d_out_kq))</span><br><span class="line">W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))</span><br><span class="line">W_value = nn.Parameter(torch.rand(d_in, d_out_v))</span><br><span class="line">​</span><br><span class="line">x = embedded_sentence</span><br><span class="line">​</span><br><span class="line">keys = x @ W_key</span><br><span class="line">queries = x @ W_query</span><br><span class="line">values = x @ W_value</span><br><span class="line">​</span><br><span class="line"><span class="comment"># attn_scores are the &quot;omegas&quot;, </span></span><br><span class="line"><span class="comment"># the unnormalized attention weights</span></span><br><span class="line">attn_scores = queries @ keys.T </span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(attn_scores)</span><br><span class="line"><span class="built_in">print</span>(attn_scores.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0613, -0.3491,  0.1443, -0.0437, -0.1303,  0.1076],</span><br><span class="line">        [-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],</span><br><span class="line">        [ 0.2432, -1.3934,  0.5869, -0.1851, -0.5191,  0.4730],</span><br><span class="line">        [-0.0794,  0.4487, -0.1807,  0.0518,  0.1677, -0.1197],</span><br><span class="line">        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216, -0.2787],</span><br><span class="line">        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],</span><br><span class="line">       grad_fn=&lt;MmBackward0&gt;)</span><br><span class="line">torch.Size([6, 6])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  与之前的Self-Attention部分类似，上面的输出是一个6×6张量，其中包含6个输入令牌的成对非标准化注意权重（也称为注意分数）。之前，我们通过softmax函数计算缩放后的点积注意力，如下所示：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attn_weights = torch.softmax(attn_scores / d_out_kq**<span class="number">0.5</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831],</span><br><span class="line">        [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],</span><br><span class="line">        [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],</span><br><span class="line">        [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463],</span><br><span class="line">        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231],</span><br><span class="line">        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],</span><br><span class="line">       grad_fn=&lt;SoftmaxBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  上面的6×6输出表示注意权重。</p>
<p>  现在，在gpt这类的LLM模型中中，我们训练模型从左到右一次读取和生成一个令牌（或单词）。如果我们有一个训练文本样本，比如"Life
is short eat desert
first"，我们有以下设置，其中箭头右侧单词的上下文向量应该只包含它自己和前面的单词：</p>
<ul>
<li><p>"Life" → "is"</p></li>
<li><p>"Life is" → "short"</p></li>
<li><p>"Life is short" → "eat"</p></li>
<li><p>"Life is short eat" → "desert"</p></li>
<li><p>"Life is short eat desert" → "first"</p></li>
</ul>
<p>  实现上述设置的最简单方法是通过对对角线上方的注意力权重矩阵应用掩码来屏蔽所有未来的标记，如下图所示。这样，在创建上下文向量时就不会包含“未来”词，上下文向量是作为输入的注意力加权和而创建的。</p>
<div class="img-container">
<img src="sample-12.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
Attention weights above the diagonal should be masked out
</div>
</div>
<p>  在下面的代码中，我们可以通过PyTorch的<a href="https://pytorch.org/docs/stable/generated/torch.tril.html#">tril</a>函数来实现这一点，我们首先使用它来创建一个1和0的掩码：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">block_size = attn_scores.shape[<span class="number">0</span>]</span><br><span class="line">mask_simple = torch.tril(torch.ones(block_size, block_size))</span><br><span class="line"><span class="built_in">print</span>(mask_simple)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[1., 0., 0., 0., 0., 0.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0.],</span><br><span class="line">        [1., 1., 1., 0., 0., 0.],</span><br><span class="line">        [1., 1., 1., 1., 0., 0.],</span><br><span class="line">        [1., 1., 1., 1., 1., 0.],</span><br><span class="line">        [1., 1., 1., 1., 1., 1.]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接下来，我们将注意力权重与这个蒙版相乘，将对角线上方的所有注意力权重归零：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">masked_simple = attn_weights*mask_simple</span><br><span class="line"><span class="built_in">print</span>(masked_simple)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000],</span><br><span class="line">        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000],</span><br><span class="line">        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],</span><br><span class="line">       grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  虽然以上是一种屏蔽将来单词的方法，但请注意，每一行的注意力权重之和不再为1。为此，进行行归一化操作，使它们的权重总和为1，这是注意力权重的标准约定：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">row_sums = masked_simple.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">masked_simple_norm = masked_simple / row_sums</span><br><span class="line"><span class="built_in">print</span>(masked_simple_norm)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],</span><br><span class="line">        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],</span><br><span class="line">        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],</span><br><span class="line">       grad_fn=&lt;DivBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  可以看到，现在每一行的注意力权重之和为1。</p>
<p>  在Transformer模型中，规范化的注意力权重比非规范化的权重更有优势，主要有两个原因。首先，归一化的注意力权重之和为1，类似于概率分布。这样就可以更容易地根据比例来解释模型对输入的各个部分的关注。其次，通过约束注意权值求和为1，这种归一化有助于控制权值和梯度的尺度，从而提高训练的动态性。</p>
<p><strong>More efficient masking without renormalization</strong></p>
<p>  在上面编码的<code>Causal Self-Attention</code>过程中，我们首先计算注意得分，然后计算注意权重，掩盖对角线上方的注意权重，最后重新规范化注意权重。如下图所示：</p>
<div class="img-container">
<img src="sample-13.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
The previously implemented causal self-attention procedure
</div>
</div>
<p>  在实际实现时，还有一种更有效的方法可以达到同样的效果。在这种方法中，我们对角线上的值替换为负无穷，然后将这些值输入softmax函数以计算注意力权重。如下图所示：</p>
<div class="img-container">
<img src="sample-14.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">
<p>An alternative, more efficient approach to implementing causal
self-attention</p>
</div>
</div>
<p>我们可以在PyTorch中按照如下方式编写这个过程，首先屏蔽对角线上方的注意力得分：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mask = torch.triu(torch.ones(block_size, block_size), diagonal=<span class="number">1</span>)</span><br><span class="line">masked = attn_scores.masked_fill(mask.<span class="built_in">bool</span>(), -torch.inf)</span><br><span class="line"><span class="built_in">print</span>(masked)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  在上面的代码中，首先创建了一个对角线以下都是0，对角线以上都是1的<code>mask</code>。这里，<code>torch.triu</code>（上三角）保留了矩阵主对角线上的元素，并将下面的元素置零，从而保留了上三角部分。相比之下，<code>torch.tril</code>（下三角）则保留了主对角线上的元素以及下面的元素。然后，使用<code>masked_fill</code>方法通过正的mask值（1s）将上三角的所有元素替换为<code>-torch.inf</code>，结果如下所示。</p>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0613,    -inf,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [-0.6004,  3.4707,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [ 0.2432, -1.3934,  0.5869,    -inf,    -inf,    -inf],</span><br><span class="line">        [-0.0794,  0.4487, -0.1807,  0.0518,    -inf,    -inf],</span><br><span class="line">        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216,    -inf],</span><br><span class="line">        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],</span><br><span class="line">       grad_fn=&lt;MaskedFillBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>然后，我们所要做的就是像往常一样应用softmax函数来获得归一化和屏蔽的注意力权重：</p>
<p><strong>Code</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attn_weights = torch.softmax(masked / d_out_kq**<span class="number">0.5</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>输出</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],</span><br><span class="line">        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],</span><br><span class="line">        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],</span><br><span class="line">        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],</span><br><span class="line">       grad_fn=&lt;SoftmaxBackward0&gt;)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  为什么会这样呢？在最后一步中应用的softmax函数将输入值转换为概率分布。当输入中存在<code>-inf</code>时，因为e^（-inf）接近于0，因此这些位置对输出概率没有贡献。</p>
<h2 id="conclusion">Conclusion</h2>
<p>  在这篇文章里，我们一步步通过编程的方式，探索了自注意力机制的内部运作。基于此，我们又研究了多头注意力，这是大型语言转换器的一个基本组成部分。</p>
<p>  然后我们还编了交叉注意力，这是自注意力的一种变体，当应用于两个不同的序列之间时特别有效。最后，我们还编了<code>Causal Self-Attention</code>力，这个概念对于在GPT和Llama这样的解码器风格LLM中生成连贯且上下文适当的序列至关重要。</p>
<p>  通过从头开始编写这些复杂的机制，你或许对转换器和LLM中使用的自注意力机制的内部运作有了很好的理解。</p>
<p>(请注意，本文中提供的代码仅用于说明目的。如果您计划在培训llm时实现自我关注，我建议考虑像[Flash
Attention]（https://arxiv.org/abs/2307.08691）这样的优化实现，它可以减少内存占用和计算负载。</p>
<h2 id="bonus-topic-cross-attention">Bonus Topic: Cross-Attention</h2>
<p>  在上面的代码讲解里，咱们设置了Self-Attention和Causal-Attention这两部分，把_d_q和_d_k都设成了2，_d_v设成了4。也就是说，咱们让查询序列和键序列用了同样的维度。虽然通常情况下，值矩阵W_v都会选择和查询矩阵、键矩阵一样的维度（比如PyTorch里的MultiHeadAttention类就是这样），但其实咱们也可以给值维度选个任意的数字大小……</p>
<p>本文翻译自<a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">Understanding
and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and
Cross-Attention in LLMs</a></p>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>自注意力机制</tag>
        <tag>大语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Attention从起源到MHA、MQA和GQA</title>
    <url>/posts/2f67dcd1/</url>
    <content><![CDATA[<p>  Attention
模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head
Attention）、MQA（Multi-Query Attention）和 GQA（Grouped-Query
Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV
Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。</p>
<span id="more"></span>
<p>  当然针对 Attention
优化，也有很多其他优秀的方案和思路，如线性注意力、FlashAttention和Sliding
Window Attention等，这些在后续再开篇梳理。</p>
<hr>
<p>了解一个概念的诞生和演进，有助于我们更深入去理解它。我们先简单回顾下
attention 从起源到最初的实现。</p>
<h2 id="从-rnn-说起">1.1. 从 RNN 说起</h2>
<p>  注意力机制最初起源是为了解决序列问题。回想在还没有 Transformer
的上一世代，使用 RNN 的 <a href="https://zhida.zhihu.com/search?content_id=240607756&amp;content_type=Article&amp;match_order=1&amp;q=Seq2Seq&amp;zhida_source=entity">Seq2Seq</a>
是这样的</p>
<div class="img-container">
<img src="https://pica.zhimg.com/v2-0ff0717e8656bd02ee2bdfb89fe48a4e_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<div class="img-container">
<img src="https://pic2.zhimg.com/v2-ae99b12d4b9231acd9184122c54ee381_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<div class="img-container">
<img src="https://pic3.zhimg.com/v2-69d79bd4696d4b8fec4343edcdf4524e_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>（图来自 <a href="https://link.zhihu.com/?target=https%3A//theaisummer.com/attention/">AI
Summer</a>）</p>
<p>  每个RNN cell接收两个输入，输出一个hidden
state。比如在翻译任务中，RNN
encoder把所有输入迭代地编码成context向量<span class="math inline">\(z\)</span> ，然后由RNN decoder基于<span class="math inline">\(z\)</span>
迭代地解码。一般来说，这里decoder的第一个输入是一个特殊token，如[start]，表示解码开始。</p>
<p>  这样会有一个问题，<span class="math inline">\(z\)</span>能编码的长度显然有限，而且由于模型结构问题，会更加关注靠近尾部的输入。这样如果关键信息出现在开头，就容易被忽略。并且时间步骤上的传播由于有多次迭代相乘，梯度很容易就过小，导致梯度消失问题。</p>
<p>  当然我们有LSMT和GRU等变体来增强长距离记忆的能力，也缓解了梯度问题，但这些方案还是没有产生质变的能力。</p>
<p>  回到问题的核心，我们想要<span class="math inline">\(z\)</span>能够编码所有前面的内容，但是显然，<span class="math inline">\(z\)</span>的生成方式天然会让它更容易注意到靠后的内容，而容易忽略靠前的输入。</p>
<p>  一个直觉的想法就是，我们需要想个办法跳过<span class="math inline">\(z\)</span>，和前面的每个输入建立直接的联系。我们希望模型能够有机会学习到去“注意”关键的输入，不管这个输入是在前面还是后面。</p>
<p>  实际上神经网络天生就具有“注意力”的天赋。</p>
<p>  比如在CNN分类中，如果我们画出分类层前的heatmap，会是如下图这个样子</p>
<div class="img-container">
<img src="https://picx.zhimg.com/v2-5b5689f81577acbbfc439f99fe7c53ab_r.jpg" alt="sample" style="display:block; margin:0 auto; width:40%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  可以看到，值比较高的地方是在猫的鼻子胡子嘴巴区域，次之是身上和头上的花纹。直观来说，就是模型主要通过脸部的特征和身上的花纹，来识别出这是一只猫。这就是CNN学习到的注意力，这样的特征是神经网络implicitly学到的。</p>
<p>  回归到Seq2Seq，我们怎么来实现注意力，并且让这种implicit的机制变得explicit：单独抽离出来并具备一定可控制性？</p>
<p>  回想翻译场景，在RNN中，每一个时间步骤<span class="math inline">\(i\)</span>都会产生一个隐向量，<span class="math inline">\(h_i\)</span>向量，我们把这些<span class="math inline">\(h_i\)</span>保存起来，在最后要生成新的输出的时候，我们让模型回头看一下之前的这每一个
<span class="math inline">\(h_i\)</span>，再决定要生成什么内容。相比原来只利用最后一个hidden
state，现在我们可以访问之前所有的中间状态，如果发现前面有关键信息，就可以直接用上了，而不用担心输入太长而被覆盖了。</p>
<p>  那么问题又来了，我们怎么知道前面某一个中间状态对于当前的生成来说是否重要？如果我们不知道怎么定义是否重要，那我们就把这个问题交给模型自己解决好了--通过网络参数来学习识别某个输入状态是否重要，学习是否要“注意”到它，要给予多少的“注意力”。</p>
<p>  具体来说，我们定义在解码第<span class="math inline">\(i\)</span>个输出是，decoder当前隐状态<span class="math inline">\(y_{i-1}\)</span>和 encoder的所有隐状态<span class="math inline">\(\mathbf{h}\)</span>之间的一个score计算:</p>
<p><span class="math display">\[\mathbf{e}_i=\text{attention}_{\mathrm{net}}\left(y_{i-1},\mathbf{h}\right)\in
R^n \\\]</span></p>
<p>其中</p>
<p><span class="math display">\[e_{ij}=\text{attentiom}_{\text{net
}(\mathbf{y}_{i-1},h_j)} \\\]</span></p>
<p>  注意力网络通过<span class="math inline">\(\mathbf{y}_{i-1}\)</span>和<span class="math inline">\(h_j\)</span>来计算一个值<span class="math inline">\(e_{ij}\)</span>，这里的注意力网络可以设计各种操作，比如对输入进行拼接再通过fc层进行计算等。</p>
<p>  这里<span class="math inline">\(e_{ij}\)</span>是一个标量，但它还不是一个可用的权重值，还需要通过一个函数，把
attention net对各个encoder hidden
state的输出值转成一个分布：softmax。</p>
<p><span class="math display">\[\alpha_{ij}=\frac{\exp\left(e_{ij}\right)}{\sum_{k=1}^{T_x}\exp\left(e_{ik}\right)}
\\\]</span></p>
<p>  最后通过加权计算，获得最终输入给 decoder 的隐变量。</p>
<p><span class="math display">\[z_i=\sum_{j=1}^T\alpha_{ij}\mathbf{h}_j
\\\]</span></p>
<div class="img-container">
<img src="https://picx.zhimg.com/v2-3bacabfaa3e408c030f976a6ec916c0d_r.jpg" alt="sample" style="display:block; margin:0 auto; width:30%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  可以看到，这里的attention net的任务就是找到decoder上一个hidden
state 和encoder hidden state之间的 “相关”
关系，使得模型能够将更多的注意力放在对应的输入信息上。</p>
<p>  实际上，上面这种attention的计算方式并不是唯一的，attention的计算方式有许多种</p>
<div class="img-container">
<img src="https://pic2.zhimg.com/v2-bf5a6e816b80bf5a00945d8af9887d75_r.jpg" alt="sample" style="display:block; margin:0 auto; width:70%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  这些attention的一般形式可以写作<span class="math inline">\(\mathrm{Attention}(s, h)=\mathrm{Score}(s,h)\cdot
h\)</span> 。这里的<span class="math inline">\(s\)</span>就是
decoder的hidden state（也就是前文的 <span class="math inline">\(y\)</span>），<span class="math inline">\(h\)</span>就是encoder的hidden
state。当然从结果上看，是 scaled dot-product
attention经受住了历史的考验，成为了主流。</p>
<h2 id="transformer-的-attention">1.2.Transformer 的 attention</h2>
<p>  从RNN attention到transformer
attention，所做的事情就如论文题目所说：《Attention Is All You
Need》，彻底抛弃RNN的在time step上的迭代计算，完全拥抱
attention机制，只用最简单粗暴的方式同步计算出每个输入的hidden
state，其他的就交给 attention来解决。</p>
<div class="img-container">
<img src="https://pic2.zhimg.com/v2-ac4d5a38ff67d7e6b2ab04715b077533_r.jpg" alt="sample" style="display:block; margin:0 auto; width:40%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  这里还是保留有encoder和decoder的结构，encoder中的attention都是
self-attention，decoder则除了self-attention还有cross-attention。</p>
<p>  transformer结构下，attention的一般形式可以写作<span class="math inline">\(\mathrm{Attention}(Q,K,V)=\mathrm{Score}(Q,K)V\)</span>，这里有<span class="math inline">\(Q=W_{Q}Y，K=W_{K}X，V=W_{V}X\)</span>。对于
cross-attention，<span class="math inline">\(X\)</span>是encoder的hidden
states，<span class="math inline">\(Y\)</span>是decoder的hidden
states，而对于self-attention，则有<span class="math inline">\(X=Y\)</span>。</p>
<p>  具体到我们熟悉的scaled dot-product
attention，使用softmax计算，有</p>
<p><span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{QK^T}{\sqrt{d}})V
\\\]</span></p>
<p>  到这里，终于见到我们熟悉的attention计算。</p>
<p>  用一张很直观的图来展示整个计算</p>
<div class="img-container">
<img src="https://pic3.zhimg.com/v2-1300387875bb218c40c9c56a517cacb2_r.jpg" alt="sample" style="display:block; margin:0 auto; width:50%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  这里的「query」，「key」和「value」的名称也暗示了整个attention计算的思路。类比到一个数据库查询+预测的例子。假设我们现在有一个“文章-阅读量”数据库，记录了每篇文章在发布30天内的阅读量。每篇文章就是一个key，对应的阅读量就是value。</p>
<p>  我们现在有一篇将要发布的文章，想要预测这篇文章在30天内的阅读量，那我们就把这篇新的文章，作为query，去和数据库里的文章（key）做一个相关性计算，取最相关的5篇文章。</p>
<p>  假设top5篇文章的相关性分别是<span class="math inline">\([8,4,4,2,2]\)</span>，对应阅读量是<span class="math inline">\([5\text{w},2\text{w},8\text{w},3\text{w},6\text{w}]\)</span>。</p>
<p>  那我们把相关性得分归一化成和为1的概率值<span class="math inline">\([0.4,0.2,0.2,0.1,0.1]\)</span>，那我们就可以预测新文章30天内的阅读量是<span class="math inline">\(0.4\times5+0.2\times2+0.2\times8+0.1\times3+0.1\times6=4.9\text{w}\)</span>。</p>
<p>  这个例子中，我们计算相关性就相当于transformer attention中的<span class="math inline">\(QK^T\)</span>
，归一化就是softmax，然后通过加权求和取得最后的阅读量/特征向量。</p>
<p>  对于self-attention，<span class="math inline">\(Q、K、V\)</span>都来自输入<span class="math inline">\(X\)</span>，sequence自己计算自己每个token的之间的相关性。而对于cross-attention，decoder中的输出sequence就是上面这个例子中的“将要发布的文章”，通过把这篇新的文章和数据库中的文章做相关计算，我们得到了新的预测结果。</p>
<p>  对于self-attention，由于<span class="math inline">\(Q、K、V\)</span>都来自输入<span class="math inline">\(X\)</span>，在计算<span class="math inline">\(QK^T\)</span>，模型很容易关注到自身的位置上，也就是<span class="math inline">\(QK^T\)</span>对角线上的激活值会明显比较大。这样的情况其实不是很好，因为这会削弱模型关注其他高价值位置的能力，也就限制模型的理解和表达能力。后面讲的MHA对这个问题会有一些缓解作用。</p>
<p>  顺着这样的思路梳理下来，会发现attention的大思路还是很好理解的。而计算上，怎么去获得更好的效果，就是接下来要分析的几个内容，MHA，MQA和GQA所关注的。代码上，实现也很容易，直接看
pytorch forcasting]的代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout: <span class="built_in">float</span> = <span class="literal">None</span>, scale: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="variable language_">self</span>.softmax = nn.Softmax(dim=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.scale = scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        attn = torch.bmm(q, k.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))  <span class="comment"># query-key overlap</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.scale:</span><br><span class="line">            dimension = torch.as_tensor(k.size(-<span class="number">1</span>), dtype=attn.dtype, device=attn.device).sqrt()</span><br><span class="line">            attn = attn / dimension</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask, -<span class="number">1e9</span>)</span><br><span class="line">        attn = <span class="variable language_">self</span>.softmax(attn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = <span class="variable language_">self</span>.dropout(attn)</span><br><span class="line">        output = torch.bmm(attn, v)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="关于-scaling">1.3. 关于 scaling</h2>
<p>  BTW，为什么计算中 <span class="math inline">\(QK^T\)</span>之后还要除以 <span class="math inline">\(\sqrt{d}\)</span> ？</p>
<p>  简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。</p>
<div class="img-container">
<img src="https://pic3.zhimg.com/v2-7a69115d5c4337e7ec26434bc0d96098_r.jpg" alt="sample" style="display:block; margin:0 auto; width:30%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  苏剑林的<a href="https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/8620">博客</a>中也有详细分析，并提到如果不对attention值进行scaling，也可以通过在参数初始化是将方差除以一个<span class="math inline">\(\sqrt{d}\)</span>
，同样可以起到预防softmax饱和的效果。类似地，通过
normalization也可以做到类似的效果。不过实现上在attention里做scaling还是比较稳定高效的。</p>
<h2 id="mha">2.MHA</h2>
<p>  只要理解了attention计算的细节，MHA（multi-head
attention）其实就很好明白。</p>
<p>  MHA在2017年就随着《Attention Is All You
Need》一起提出，主要干的就是一个事：把原来一个attention计算，拆成多个小份的attention，并行计算，分别得出结果，最后再合回原来的维度。</p>
<p><span class="math display">\[\mathrm{MultiHeadAttention}(Q,K,V)=\mathrm{Concat}(head_1,\ldots,head_h)
\\\]</span></p>
<p><span class="math display">\[head_i=\text{Attention}(W_i^QQ,W_i^KK,W_i^VV)
\\\]</span></p>
<p>  假设原来模型的hidden size是<span class="math inline">\(d\)</span>
，在MHA中，会把投影后的<span class="math inline">\(Q、K、V\)</span>在
hidden state的维度上切成<span class="math inline">\(head_{num}\)</span>份，每个头的维度是<span class="math inline">\(d_{head}\)</span> 。这<span class="math inline">\(head_{num}\)</span>组小 <span class="math inline">\(Q、K、V\)</span>分别独立地进行attention计算，之后把得到的<span class="math inline">\(head_{num}\)</span>份维度<span class="math inline">\(d_{head}\)</span>的输出concat起来。</p>
<p>  直接看这个amazing的图，很直观</p>
<div class="img-container">
<img src="https://picx.zhimg.com/v2-15580348885d70fd6a11c683eac151af_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  操作是这么个操作，多头注意力相比单头有什么好处呢？《Attention Is
All You Need》文章中给出的说法是</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to
information from different representation subspaces at different
positions.</p>
</blockquote>
<p>  我们希望多个头能够在训练中学会注意到不同的内容。例如在翻译任务里，一些
attention head可以关注语法特征，另一些 attention
head可以关注单词特性。这样模型就可以从不同角度来分析和理解输入信息，获得更好的效果了。</p>
<p>  这有点类似CNN中，不同的卷积核来学习不同的信息。比如一个 <span class="math inline">\(3\times3\times128\)</span>的卷积，有128个<span class="math inline">\(3\times3\)</span>参数组，假设我们的输入是一个灰度图，其中一组<span class="math inline">\(3\times3\)</span>的参数是这样的</p>
<p><span class="math display">\[\left.\left[\begin{matrix}1&amp;0&amp;-1\\1&amp;0&amp;-1\\1&amp;0&amp;-1\end{matrix}\right.\right]
\\\]</span></p>
<p>  那么这是一个检测纵向边界的卷积，而另外一组参数长这样</p>
<p><span class="math display">\[\left.\left[\begin{matrix}1&amp;1&amp;1\\0&amp;0&amp;0\\-1&amp;-1&amp;-1\end{matrix}\right.\right]
\\\]</span></p>
<p>  这是一个检测横向边界的卷积。</p>
<p>  这128组<span class="math inline">\(3\times3\)</span>就是128个不同特征的检测器，就同MHA中多个头一样，从不同的子空间学到不同的内容，最后再放到一起融合使用。</p>
<p>  但是这是我们expect模型能做到的事情，实际情况是否真的是这样？</p>
<p>  知乎上这篇<a href="https://zhuanlan.zhihu.com/p/626820422">文章</a>里对此做了一些实验和分析。简单来说就是（1）每个头确实学到东西有所不同，但大部分头之间的差异没有我们想的那么大（比如一个学句法，一个学词义这样明显的区分）（2）多个头的情况下，确实有少部分头可以比较好地捕捉到各种文本信息，而不会过分关注自身位置，一定程度缓解了上文提到的计算
<span class="math inline">\(QK^T\)</span>之后对角线元素过大的问题。</p>
<p>  我们可以把MHA的多个attention计算视为多个独立的小模型，那么最终整体的attention计算相当于把来自多个小模型的结果进行了融合，这样效果比较好也是比较符合直觉的。</p>
<p>  另外还有一个问题是，使用几个头比较好呢？</p>
<p>  实际上这个问题比较难有确定性的答案，首先可以确定的是头的数量不是越多约好（毕竟头的数量多了，各个子空间小了，子空间能表达的内容就少了），具体多少要视模型规模，任务而定。另外<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.10650.pdf">《Are
Sixteen Heads Really Better than One?》</a>中也指出 MHA
并不总是优于单头的情况。</p>
<p>  目前可以看到的趋势是，模型越大（也就是 hidden size
越大），头数的增多越能带来平均效果上的收益（或者说允许注意力头增大而不影响子空间的学习能力）。目前LLM主流的头数视乎模型结构和规模，大致有12、16、24、48、96这样一些主流设置。这里面又有比较多的方向和工作，在此暂时不展开，挖个坑，以后专门开一篇讲。</p>
<p>  最后看一下<a href="https://link.zhihu.com/?target=https%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The
Annotated Transformer</a> 中的 MHA 代码实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        h: head number</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d</span></span><br><span class="line">        <span class="variable language_">self</span>.d = d_model // h</span><br><span class="line">        <span class="variable language_">self</span>.h = h</span><br><span class="line">        <span class="variable language_">self</span>.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attn = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, <span class="variable language_">self</span>.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=<span class="variable language_">self</span>.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h * <span class="variable language_">self</span>.d)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linears[-<span class="number">1</span>](x)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  <a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers">transformers</a>
中的写法就更为成熟一点，不过里面兼容了比较多的功能，代码太长就不放上来了。</p>
<ol start="3" type="1">
<li>解码中的 KV Cache</li>
</ol>
<hr>
<p>  在讲MQA和GQA之前，还需要了解一点背景，那就是解码的计算问题，以及KV
Cache
的方案。无论是encoder-decoder结构，还是现在我们最接近AGI的decoder-only的LLM，解码生成时都是自回归auto-regressive的方式。</p>
<p>  也就是，解码的时候，先根据当前输入<span class="math inline">\(\text{input}_{i-1}\)</span>，生成下一个 <span class="math inline">\(\text{token}_{i}\)</span>，然后把新生成的<span class="math inline">\(\text{token}_{i}\)</span>拼接在<span class="math inline">\(\text{input}_{i-1}\)</span>
后面，获得新的输入<span class="math inline">\(\text{input}_{i}\)</span>
，再用<span class="math inline">\(\text{input}_{i}\)</span>生成<span class="math inline">\(\text{token}_{i+1}\)</span>，依此迭代，直到生成结束。</p>
<p>  比如我们输入“窗前明月光下一句是”，那么模型每次生成一个
token，输入输出会是这样（方便起见，默认每个token都是一个字符）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">step0: 输入=[BOS]窗前明月光下一句是；输出=疑</span><br><span class="line">step1: 输入=[BOS]窗前明月光下一句是疑；输出=是</span><br><span class="line">step2: 输入=[BOS]窗前明月光下一句是疑是；输出=地</span><br><span class="line">step3: 输入=[BOS]窗前明月光下一句是疑是地；输出=上</span><br><span class="line">step4: 输入=[BOS]窗前明月光下一句是疑是地上；输出=霜</span><br><span class="line">step5: 输入=[BOS]窗前明月光下一句是疑是地上霜；输出=[EOS]</span><br></pre></td></tr></table></figure>
<p>  [BOS]和[EOS]分别是起始符号和终止符号</p>
<p>  仔细想一下，我们在生成 “疑” 字的时候，用的是输入序列中 “是”
字的最后一层 hidden
state，通过最后的分类头预测出来的。以此类推，后面每生成一个字，使用的都是输入序列中最后一个字的输出。</p>
<p>  我们可以注意到，下一个step的输入其实包含了上一个step的内容，而且只在最后面多了一点点（一个
token）。那么下一个step的计算应该也包含了上一个step的计算。</p>
<p>  从公式上来看是这样的，回想一下我们attention的计算：</p>
<p><span class="math display">\[\alpha_{i,j}=\text{softmax}(q_{i}k_{j}^\top)\\
o_{i}=\sum_{j=0}^{i}{\alpha_{i,j}v_{j}} \\\]</span></p>
<p>  注意对于decoder的时候，由于mask
attention的存在，每个输入只能看到自己和前面的内容，而看不到后面的内容假设我们当前输入的长度是3，预测第4个字，那每层attention所做的计算有：</p>
<p><span class="math display">\[\begin{aligned}
o_{0}&amp;=\alpha_{0,0}v_{0}\\
o_{1}&amp;=\alpha_{1,0}v_{0}+\alpha_{1,1}v_{1}\\
o_{2}&amp;=\alpha_{2,0}v_{0}+\alpha_{2,1}v_{1}+\alpha_{2,2}v_{2}\\
\end{aligned} \\\]</span></p>
<p>  预测完第4个字，放到输入里，继续预测第5个字，每层attention所做的计算有：</p>
<p><span class="math display">\[\begin{aligned}
o_{0}&amp;=\alpha_{0,0}v_{0}\\
o_{1}&amp;=\alpha_{1,0}v_{0}+\alpha_{1,1}v_{1}\\
o_{2}&amp;=\alpha_{2,0}v_{0}+\alpha_{2,1}v_{1}+\alpha_{2,2}v_{2}\\
o_{3}&amp;=\alpha_{3,0}v_{0}+\alpha_{3,1}v_{1}+\alpha_{3,2}v_{2}+\alpha_{3,3}v_{3}\\
\end{aligned} \\\]</span></p>
<p>  可以看到，在预测第5个字时，只有最后一步引入了新的计算，而<span class="math inline">\(o_{0}\)</span>到<span class="math inline">\(o_{2}\)</span>的计算和前面是完全重复的。</p>
<p>  但是模型在推理的时候可不管这些，无论你是不是只要最后一个字的输出，它都把所有输入计算一遍，给出所有输出结果。</p>
<p>  也就是说中间有很多我们用不到的计算，这样就造成了浪费。</p>
<p>  而且随着生成的结果越来越多，输入的长度也越来越长，上面这个例子里，输入长度就从step0的10个，每步增长1，直到step5的15个。如果输入的instruction是让模型写作文，那可能就有800个step。这个情况下，step0被算了800次，step1被算了799次...这样浪费的计算资源确实不容忽视。</p>
<p>  有没有什么办法可以重复利用上一个step里已经计算过的结果，减少浪费呢？</p>
<p>  答案就是KV
Cache，利用一个缓存，把需要重复利用的中间计算结果存下来，减少重复计算。</p>
<p>  而<span class="math inline">\(k\)</span>和<span class="math inline">\(v\)</span>就是我要缓存的对象。
  想象一下，在上面的例子中，假设我们一开始的输入就是3个字，我们第一次预测就是预测第4个字，那么由于一开始没有任何缓存，所有我们每一层还是要老实地计算一遍。然后把<span class="math inline">\(k\)</span>、<span class="math inline">\(v\)</span>值缓存起来。</p>
<p>  则有</p>
<p><span class="math display">\[\text{cache}_l=\text{None}
\\\]</span></p>
<p><span class="math display">\[\text{cache}_l=[(k_{0}, v_{0}),(k_{1},
v_{1}),(k_{2}, v_{2})] \\\]</span></p>
<p>  kv_cache的下标<span class="math inline">\(l\)</span>表示模型层数。</p>
<p>  在进行第二次预测，也就是预测第5个字的时候，在第<span class="math inline">\(l\)</span>层的时候，由于前面我们缓存了<strong>每层</strong>的<span class="math inline">\(k\)</span>、<span class="math inline">\(v\)</span>值，那本层就只需要算新的<span class="math inline">\(o_{3}\)</span> ，而不用算<span class="math inline">\(o_{0}、o_{1}、o_{2}\)</span>。</p>
<p>  因为第<span class="math inline">\(l\)</span>层的<span class="math inline">\(o_{0}、o_{1}、o_{2}\)</span>本来会经过FNN层之后进到<span class="math inline">\(l+1\)</span>层，再经过新的投影变换，成为<span class="math inline">\(l+1\)</span>层的<span class="math inline">\(k\)</span>、 <span class="math inline">\(v\)</span>值，但是<span class="math inline">\(l+1\)</span>层的<span class="math inline">\(k\)</span>、 <span class="math inline">\(v\)</span>值我们已经缓存过了！</p>
<p>  然后我们把本次新增算出来的<span class="math inline">\(k\)</span>、
<span class="math inline">\(v\)</span>值也存入缓存。</p>
<p><span class="math display">\[\text{cache}_l=[(k_{0}, v_{0}),(k_{1},
v_{1}),(k_{2}, v_{2})] \\\]</span></p>
<p><span class="math display">\[\text{cache}_l=[(k_{0}, v_{0}),(k_{1},
v_{1}),(k_{2}, v_{2}),(k_{3}, v_{3})] \\\]</span></p>
<p>  这样就节省了attention和FFN的很多重复计算。</p>
<p>  transformers中，生成的时候传入use_cache=True就会开启KV Cache。</p>
<p>  也可以简单看下GPT2中的实现，中文注释的部分就是使用缓存结果和更新缓存结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Class GPT2Attention(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.FloatTensor]],</span></span><br><span class="line"><span class="params">        layer_past: <span class="type">Optional</span>[<span class="type">Tuple</span>[torch.Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        use_cache: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[<span class="type">Union</span>[torch.Tensor, <span class="type">Tuple</span>[torch.Tensor]], ...]:</span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&quot;q_attn&quot;</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span></span><br><span class="line">                    <span class="string">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            query = <span class="variable language_">self</span>.q_attn(hidden_states)</span><br><span class="line">            key, value = <span class="variable language_">self</span>.c_attn(encoder_hidden_states).split(<span class="variable language_">self</span>.split_size, dim=<span class="number">2</span>)</span><br><span class="line">            attention_mask = encoder_attention_mask</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query, key, value = <span class="variable language_">self</span>.c_attn(hidden_states).split(<span class="variable language_">self</span>.split_size, dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        query = <span class="variable language_">self</span>._split_heads(query, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        key = <span class="variable language_">self</span>._split_heads(key, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        value = <span class="variable language_">self</span>._split_heads(value, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 过去所存的值</span></span><br><span class="line">        <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            past_key, past_value = layer_past</span><br><span class="line">            key = torch.cat((past_key, key), dim=-<span class="number">2</span>)  <span class="comment"># 把当前新的key加入</span></span><br><span class="line">            value = torch.cat((past_value, value), dim=-<span class="number">2</span>)  <span class="comment"># 把当前新的value加入</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cache <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            present = (key, value)  <span class="comment"># 输出用于保存</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            present = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.reorder_and_upcast_attn:</span><br><span class="line">            attn_output, attn_weights = <span class="variable language_">self</span>._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_weights = <span class="variable language_">self</span>._attn(query, key, value, attention_mask, head_mask)</span><br><span class="line"></span><br><span class="line">        attn_output = <span class="variable language_">self</span>._merge_heads(attn_output, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        attn_output = <span class="variable language_">self</span>.c_proj(attn_output)</span><br><span class="line">        attn_output = <span class="variable language_">self</span>.resid_dropout(attn_output)</span><br><span class="line"></span><br><span class="line">        outputs = (attn_output, present)</span><br><span class="line">        <span class="keyword">if</span> output_attentions:</span><br><span class="line">            outputs += (attn_weights,)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># a, present, (attentions)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  总的来说，KV
Cache是以空间换时间的做法，通过使用快速的缓存存取，减少了重复计算。（注意，只有decoder结构的模型可用，因为有mask
attention的存在，使得前面的token可以不用关注后面的token）</p>
<p>  但是，用了KV Cache之后也不是立刻万事大吉。</p>
<p>  我们简单算一下，对于输入长度为<span class="math inline">\(s\)</span>，层数为<span class="math inline">\(L\)</span>，hidden size为<span class="math inline">\(d\)</span>的模型，需要缓存的参数量为</p>
<p><span class="math display">\[2\times L\times s\times d
\\\]</span></p>
<p>  如果使用的是半精度浮点数，那么总共所需的空间就是</p>
<p><span class="math display">\[2\times 2\times L\times s\times d
\\\]</span></p>
<p>  以Llama2 7B为例，有<span class="math inline">\(L=32\)</span>
，<span class="math inline">\(L=4096\)</span>，那么每个token所需的缓存空间就是
524,288bytes，约52K，当<span class="math inline">\(s=1024\)</span>时，则需要536,870,912
bytes，超过500M的空间。</p>
<p>  这里考虑的还只是batch size=1 的情况，如果batch size
增大，这个值更是很容易就超过1G。（MHA相比单头的情况，相当于只是把<span class="math inline">\(q、k、v\)</span>切成多份并行计算了，对于实际需要缓存的大小没有影响）</p>
<p>  看下现在主流的科学计算卡配置</p>
<div class="img-container">
<img src="https://picx.zhimg.com/v2-0f831b35b55cb838f966891f95a25eef_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  强如H100也只有50M的L2 Cache（L1
Cache的大小更是可以忽略不计），大概只能支持Llama2
7B总共100个token左右的输入。</p>
<p>  想想我们现在用的LLM动辄34B/70B的规模，长度更是以千为基础单位，这样明显是不够用的。</p>
<p>  那么超出L2 Cache的部分只能走到显存中去了，但是HBM速度比L2
Cache慢多了。</p>
<div class="img-container">
<img src="https://pic1.zhimg.com/v2-98a3525b9ce728be66903fe35f3a143c_r.jpg" alt="sample" style="display:block; margin:0 auto; width:50%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  看来还需要进一步优化。</p>
<p>  要保证模型的推理加速，要么增大Cache的大小，而且是需要一到两个数量级的增强，那这个只能靠黄老板了。</p>
<p>  要么就是减少需要缓存的量。</p>
<h2 id="mqa">4.MQA</h2>
<p>  MQA就是来减少缓存所需要的量的。</p>
<p>  Google在2019年就在《Fast Transformer Decoding: One Write-Head is
All You
Need》提出了MQA，不过那时候主要到的人不多，那是大家主要还是关注在用Bert把榜刷出新高上。</p>
<p>  MQA的做法其实很简单。在MHA中，输入分别经过<span class="math inline">\(W_{Q}、W_{K}、W_{V}\)</span>变换之后，都切成了n份（n=头数），维度也从<span class="math inline">\(d_{model}\)</span>降到了<span class="math inline">\(d_{head}\)</span>，分别进行attention计算再拼接。而MQA这里，在线性变换之后，只对<span class="math inline">\(Q\)</span>进行切分（和 MHA 一样），而<span class="math inline">\(K、V\)</span>则直接在线性变换的时候把维度降到了<span class="math inline">\(d_{head}\)</span>（而不是切分变小），然后这n个Query头分别和同一份<span class="math inline">\(K、V\)</span>进行
attention计算，之后把结果拼接起来。</p>
<p>  简单来说，就是MHA中，每个注意力头的<span class="math inline">\(K、V\)</span>是不一样的，而MQA这里，每个注意力头的<span class="math inline">\(K、V\)</span>是一样的，值是共享的。而其他步骤都和MHA一样。</p>
<div class="img-container">
<img src="https://pic1.zhimg.com/v2-90c8caf647b040b1987f9bf3024a5828_r.jpg" alt="sample" style="display:block; margin:0 auto; width:50%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  这样一来，需要缓存的<span class="math inline">\(K、V\)</span>值一下就从所有头变成一个头的量。</p>
<p>  比如在Llama2
7B中用的是32个头，那用MQA后，1024个token需要缓存的量就变成
1/32，536,870,912 bytes / 32 = 16,777,216
bytes，差不多是16M，这就能全塞进缓存中了。(实现上，就是改一下线性变换矩阵，然后把
<span class="math inline">\(K、V\)</span>的处理从切分变成复制，就不再赘述。）</p>
<p>  当然，由于共享了多个头的参数，限制了模型的表达能力，MQA虽然能好地支持推理加速，但是在效果上略略比MHA差一点，但是并不多，且相比其他修改hidden
size或者head num的做法效果都好。</p>
<div class="img-container">
<img src="https://pic3.zhimg.com/v2-d42ddafcf14e3f865fe4fd9765d21bd8_r.jpg" alt="sample" style="display:block; margin:0 auto; width:50%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<div class="img-container">
<img src="https://pic1.zhimg.com/v2-ca4682e17c6914cb5c0c9c9dadec7212_r.jpg" alt="sample" style="display:block; margin:0 auto; width:50%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<h2 id="gqa">5.GQA</h2>
<p>  既然MQA对效果有点影响，MHA缓存又存不下，那GQA（Grouped-Query
Attention）就提出了一个折中的办法，既能减少MQA效果的损失，又相比MHA需要更少的缓存。
（文章：《GQA: Training Generalized Multi-Query Transformer Models from
Multi-Head Checkpoints》，2023 年）</p>
<p>  GQA里，<span class="math inline">\(Q\)</span>还是按原来MHA/MQA的做法不变。只使用一套共享的<span class="math inline">\(K、V\)</span>不是效果不好吗，那就还是多弄几套。但是不要太多，数量还是比<span class="math inline">\(Q\)</span>的头数少一些。这样相当于把<span class="math inline">\(Q\)</span>的多个头给分了group，同一个group内的<span class="math inline">\(Q\)</span>共享同一套<span class="math inline">\(K、V\)</span> ，不同group的<span class="math inline">\(Q\)</span>所用的<span class="math inline">\(K、V\)</span> 不同。</p>
<p>  MHA可以认为是<span class="math inline">\(K、V\)</span>头数最大时的GQA，而MQA可以任务是<span class="math inline">\(K、V\)</span>头数最少时的 GQA。</p>
<p>  看论文里的图就很直观</p>
<div class="img-container">
<img src="https://pic2.zhimg.com/v2-152555107b3ad3ad0b4f97b0972eb123_r.jpg" alt="sample" style="display:block; margin:0 auto; width:80%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  效果怎么样呢？</p>
<div class="img-container">
<img src="https://picx.zhimg.com/v2-79080c0b8afa595f2ed639b1171bc819_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<p>  看表中2/3/4行对比，GQA的速度相比MHA有明显提升，而效果上比MQA也好一些，能做到和MHA基本没差距。文中提到，这里的MQA和GQA都是通过average
pooling从MHA初始化而来，然后进行了少量的训练得到的。如果我们想要把之前用MHA训练的模型改造成GQA，也可以通过这样的方法，增加少量训练来实现。当然如果从一开始就加上，从零开始训练，也是没有问题的。</p>
<p>  Llama2用的就是GQA，在tech
report中也做了MHA、MQA、GQA的效果对比，可以看到效果确实很不错。</p>
<div class="img-container">
<img src="https://pic2.zhimg.com/v2-aa6302478f6dab8cf4b4cc400a406f79_r.jpg" alt="sample" style="display:block; margin:0 auto; width:60%">
<div class="caption" style="text-align: center; font-family: sans-serif">

</div>
</div>
<ol start="6" type="1">
<li>小结</li>
</ol>
<hr>
<p>  MHA、MQA、GQA的实现其实并不复杂，效果也很好，理解上并没有太多困难。但是想要真正理解它们的出发点，还是需要深入每一个细节，去了解当时要解决的事什么问题。</p>
<p>  目前来看GQA是LLM比较好的方案，但未来肯定还会有针对不同方向的进一步优化方案，计算效率、推理速度、显存消耗这些方向都值得我们继续去探索优化。</p>
<p>文章搬运自<a href="https://zhuanlan.zhihu.com/p/686149289">理解Attention:从起源到MHA,MQA和GQA</a></p>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>-- Attention -- MHA -- GQA -- MQA -- LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>FlashAttention从原理到cuda实现</title>
    <url>/posts/6721533d/</url>
    <content><![CDATA[<p>  <code>Flash Attention</code>是一种基于硬件设计的注意力加速策略，原始论文
<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135">FlashAttention:
Fast and Memory-Efficient Exact Attention with IO-Awareness</a>。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>-- Flash Attenttion -- Attention -- 自注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>点云内存布局</title>
    <url>/posts/900a74c0/</url>
    <content><![CDATA[<p>  点云数据可采用两种基本布局：<strong>结构体数组</strong> (AoS, Array
of Structures) 和 <strong>数组结构</strong> (SoA, Structure of
Arrays)。</p>
<span id="more"></span>
<p>  AoS形式将每个点的所有字段（如 x、y、z
坐标）连成一个结构体再按点排列，易于编程直观，每个点的字段存放在一起
。但当批量访问同一字段（如所有点的 x 坐标）时，AoS
需要不连续的“Gather”操作，降低 SIMD 和 GPU 内存带宽利用率 。相反，SoA
将每个字段分离成独立数组（如X[N], Y[N],
Z[N]），使得同一字段在内存中连续，这样一个 SIMD
寄存器可以一次性加载多个同类坐标 ，且 CUDA
线程按编号依次读取连续元素时能够形成<strong>合并访问</strong>（coalesced
access） 。所以，在向量化和大规模并行场景下一般<strong>推荐使用 SoA
布局</strong>。</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 45%">
<col style="width: 45%">
</colgroup>
<thead>
<tr>
<th><strong>布局</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>AoS</td>
<td>易于编程理解：每个点字段相邻，遍历单点时缓存友好</td>
<td>矢量化不友好：跨点访问同一字段时需 Gather/Scatter ；GPU
并行访问时各线程访问内存间隔大</td>
</tr>
<tr>
<td>SoA</td>
<td>SIMD 友好：同字段数据连续，可连续装载 ；GPU
多线程访问时可形成内存合并</td>
<td>难以同时处理一个点的所有字段，访问多字段时缓存局部性下降
；编程稍复杂</td>
</tr>
<tr>
<td>AoSoA (折中)</td>
<td>局部字段成块对齐到向量宽度，兼顾 SIMD 单元化和缓存局部</td>
<td>编程复杂：需要按向量长度分块组织数据</td>
</tr>
</tbody>
</table>
<p><strong>选择建议</strong>：若主要操作是批量处理（如对所有点坐标做相同计算），则
SoA 布局往往更高效 。如需兼顾单点操作和向量化，可考虑
AoSoA（每个结构体内含定长坐标数组）技术 。</p>
<h2 id="内存对齐与simd"><strong>内存对齐与SIMD</strong></h2>
<p>  为发挥SSE/AVX等SIMD指令性能，应保证数据按向量宽度对齐。Intel建议对齐规则为：<strong>SSE(128-bit)</strong>
使用 16 字节对齐，<strong>AVX(256-bit)</strong> 使用 32
字节对齐，<strong>AVX-512(512-bit)</strong> 使用 64 字节对齐 。对齐可让
<code>_mm_load_ps/_mm256_load_ps</code>
等指令在对齐模式下加载，避免未对齐访问开销。实现方法包括：</p>
<ul>
<li>在 C++ 中对结构体或数组使用 <code>alignas(16)、alignas(32)</code>
等声明。例如：<code>alignas(32) float X[N]</code>;。</li>
<li>动态分配对齐内存：使用C11
<code>aligned_alloc、POSIX posix_memalign</code>（Linux/macOS）或
Windows 的 <code>_aligned_malloc</code>
接口，确保返回指针满足指定字节对齐。</li>
<li>结构体定义时统一对齐：如
<code>struct alignas(32) MyStruct &#123; ... &#125;;</code>，或根据编译器使用
<code>__attribute__((aligned(32)))/__declspec(align(32))</code>。建议使用
C++11 标准的 alignas 以提高可移植性。</li>
</ul>
<p>  对齐策略可以参考
：<code>针对 SSE2 平台应使用 16 字节对齐，对 AVX 平台尝试 32 字节对齐，对 AVX-512 则 64 字节对齐</code>。同时，高对齐也减少了L1/L2缓存冲突，提高数据并行载入效率。</p>
<h2 id="cpu端simd优化策略"><strong>CPU端SIMD优化策略</strong></h2>
<p>  在x86CPU 上，可利用 SSE/AVX
指令集批量处理点云数据，关键策略包括：</p>
<ul>
<li><p><strong>矢量化加载/存储</strong></p>
<p>使用 <code>_mm_load_ps/_mm256_load_ps</code>（对齐）或
<code>_mm_loadu_ps/_mm256_loadu_ps</code>（未对齐）批量加载若干连续浮点数。例如对
SoA 布局中的坐标数组，每次加载 4 或 8 个浮点进行运算。Intel
指南指出应“使用单元步长访问(unit stride)和 SoA 布局，以助力向量化”
。</p></li>
<li><p><strong>无别名提示</strong>：</p>
<p>使用 <code>__restrict__</code> 或 <code>restrict</code>
修饰指针，告知编译器数据无重叠，以消除因可能别名引起的矢量化障碍。</p></li>
<li><p><strong>循环展开与指令级并行</strong>
对循环固定步长进行展开，或使用
<code>#pragma omp simd、#pragma ivdep</code> 等编译指示，协助编译器生成
SIMD 代码。对已对齐数据，编译器更容易自动生成 _mm256 类指令。</p></li>
</ul>
<p><strong>示例</strong>：下面以4点做简单变换为例（假设 X[],Y[],Z[]
是对齐的坐标数组）：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;immintrin.h&gt;</span></span></span><br><span class="line"><span class="comment">// 例如，对 X 坐标批量加常数偏移</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">transformX</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *X, <span class="type">float</span> *X_out, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    __m128 offset = _mm_set1_ps(<span class="number">1.0f</span>); <span class="comment">// 每个元素加 1.0</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="comment">// 对齐加载 4 个连续浮点</span></span><br><span class="line">        __m128 vx = _mm_load_ps(&amp;X[i]);</span><br><span class="line">        <span class="comment">// 矢量相加</span></span><br><span class="line">        __m128 res = _mm_add_ps(vx, offset);</span><br><span class="line">        <span class="comment">// 存回结果（假设 X_out 已对齐）</span></span><br><span class="line">        _mm_store_ps(&amp;X_out[i], res);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  此示例演示以 SSE（128 位寄存器）一次处理 4 个浮点的思路；类似地可用
AVX （256 位）一次处理 8 个浮点。关键在于保证 X、X_out
数组地址对齐（如使用 <code>alignas(16/32)）</code>，使
<code>_mm_load_ps</code> 调用更高效。更多优化可以参考第三方库或 PCL 的
SIMD 实现 。</p>
<h2 id="gpu-端内存访问设计"><strong>GPU 端内存访问设计</strong></h2>
<p>  在CUDA GPU上进行批量处理时，<strong>连续性访问（Coalesced
Access）</strong> 是性能关键。CUDA 对于全局内存访问要求：同一个 warp（32
线程）中的连续线程访问相邻内存地址段，才能合并为少量内存事务。采用 SoA
布局时，如果线程索引 i 访问 X[i],Y[i],Z[i]，则第 <em>i</em> 线程读取第
<em>i</em> 个点各坐标，各坐标数组内存连续，典型会落在同一个 128
字节块内，从而实现合并加载 。例如参考[25]所示：若一组线程依次请求 x[0],
x[1], x[2]，硬件可一次性读取连续浮点0,1,2，极大提升带宽利用 。</p>
<p>应注意避免未对齐大小：CUDA 编程手册建议使用 float4 等 16
字节对齐类型进行存储和传输；与之相比，float3
结构由于未对齐边界而可能导致效率下降 。因此可以将三维坐标存为
float4（第4个元素作填充），或采用 SoA
方式本质上也是将每个坐标数组对齐存储。</p>
<p>CUDA 示例：假设已在设备上分配了对齐的 X[],Y[],Z[]
数组，下面示例按线程对每个坐标加偏移：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// CUDA kernel：每个线程处理一个点</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addOffsetKernel</span><span class="params">(<span class="type">float</span> *X, <span class="type">float</span> *Y, <span class="type">float</span> *Z, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; n) &#123;</span><br><span class="line">        <span class="comment">// 访问各坐标数组中的同一位置</span></span><br><span class="line">        <span class="type">float</span> x = X[idx], y = Y[idx], z = Z[idx];</span><br><span class="line">        <span class="comment">// 简单偏移操作</span></span><br><span class="line">        X[idx] = x + <span class="number">1.0f</span>;</span><br><span class="line">        Y[idx] = y + <span class="number">2.0f</span>;</span><br><span class="line">        Z[idx] = z + <span class="number">3.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  在这个SoA布局下，线程idx顺序访问X[idx]、Y[idx]、Z[idx]，使得连续线程访问连续的内存地址，能获得最佳的内存合并性能。</p>
<h2 id="批量处理示例"><strong>批量处理示例</strong></h2>
<ul>
<li><strong>CPU 端批量变换示例（SSE/AVX）</strong>：将上述 transformX
扩展到对 x,y,z 坐标同时变换，可以在循环内同时加载 X[i],Y[i],Z[i]
到向量寄存器，并应用仿射变换矩阵（4×4）实现点云的旋转平移等运算。可参考
Eigen 或手写内联汇编优化。</li>
<li><strong>CUDA 批量计算示例</strong>：在 CUDA kernel
中，为了避免线程分支，可让每线程按元素编号连续处理任务，如上例所示。复杂计算时，可把矩阵常量存入常量内存，用单线程分别计算
x,y,z
坐标的输出，并写回全局内存。关键是保证对齐访问和共存存储局部性。</li>
</ul>
<p>（根据实际应用，可使用线程块划分、共享内存缓存等进一步优化，但核心原则是保证内存访问的对齐和并行性。）</p>
<h2 id="跨平台兼容性"><strong>跨平台兼容性</strong></h2>
<p>  为了保证代码在不同平台（x86/Linux/Windows、CUDA/非CUDA）上一致高效，需要注意：</p>
<ul>
<li><strong>对齐修饰符移植</strong>：优先使用 C++11 的 alignas
关键字指定对齐，替代编译器特定语法。这在 GCC/Clang/MSVC
上均有支持。若需兼容老旧编译器，可定义跨平台宏：例如 #ifdef _MSC_VER
#define ALIGN(N) __declspec(align(N)) #else #define ALIGN(N)
<strong>attribute</strong>((aligned(N)))。</li>
<li><strong>固定宽度类型</strong>：使用 <cstdint> 中的
uint32_t、uint64_t 等代替原生 int/long，以及确保 float/double
精度统一。注意 CUDA float3、float4 在 Host
端无内建类型，应自行定义结构体兼容。</cstdint></li>
<li><strong>内存分配</strong>：避免使用 malloc/new
默认分配（未必满足高对齐要求）。对齐分配可使用 C11 的
aligned_alloc、POSIX posix_memalign、Windows
_aligned_malloc，并在释放时配对对应函数。或利用 SIMD 接口如
_mm_malloc/_mm_free。C++17 可用 std::aligned_alloc 或 std::vector
结合对齐 allocator。</li>
<li><strong>编译环境</strong>：在需要跨编译 CUDA 代码时，可使用
<strong>host</strong> <strong>device</strong>
关键字标记通用函数/结构体；避免在共享代码中使用只在一种环境下可用的扩展。比如，在
CPU 代码避免直接包含 CUDA 头文件，在 GPU 代码避免使用 x86 SIMD
intrinsic。</li>
</ul>
<p>  通过上述方法，可避免不同平台间的对齐和内存布局陷阱，保证同一套数据结构在
CPU 和 GPU 端表现一致。</p>
<h2 id="推荐代码实现"><strong>推荐代码实现</strong></h2>
<p>  综合上述考虑，推荐采用 SoA 布局并进行对齐。示例代码模板如下（C++11
及以上）：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 示例：按 32 字节对齐的点云结构（每个坐标数组独立存储）</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">PointCloud</span> &#123;</span><br><span class="line">    <span class="type">size_t</span> N;              <span class="comment">// 点数</span></span><br><span class="line">    <span class="type">float</span> *X, *Y, *Z;      <span class="comment">// 指向对齐的坐标数组</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">PointCloud</span>(<span class="type">size_t</span> n) : <span class="built_in">N</span>(n) &#123;</span><br><span class="line">        <span class="comment">// 32 字节对齐分配，跨平台示例（实际使用时根据平台选择接口）</span></span><br><span class="line">        X = (<span class="type">float</span>*)_mm_malloc(N * <span class="built_in">sizeof</span>(<span class="type">float</span>), <span class="number">32</span>);</span><br><span class="line">        Y = (<span class="type">float</span>*)_mm_malloc(N * <span class="built_in">sizeof</span>(<span class="type">float</span>), <span class="number">32</span>);</span><br><span class="line">        Z = (<span class="type">float</span>*)_mm_malloc(N * <span class="built_in">sizeof</span>(<span class="type">float</span>), <span class="number">32</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">PointCloud</span>() &#123;</span><br><span class="line">        _mm_free(X);</span><br><span class="line">        _mm_free(Y);</span><br><span class="line">        _mm_free(Z);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若需要单点结构，可定义对齐的 AoS 结构体（结合向量指令使用）</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">alignas</span>(<span class="number">16</span>) PointXYZ &#123;</span><br><span class="line">    <span class="type">float</span> x, y, z, w;  <span class="comment">// w 作为填充，保证 16 字节对齐</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>  以上模板中，PointCloud 用 SoA 方式存储各坐标数组，构造函数使用
_mm_malloc（或 posix_memalign、aligned_alloc）指定 32 字节对齐；PointXYZ
演示对齐的AoS数据结构，在需要使用如 SSE 载入一个 float4
时非常方便。实际使用时可根据目标 SIMD 宽度调整对齐大小（例如 AVX 可设 32
或 64 字节对齐）。如需在 CUDA 中使用，可将 float* 指针对应改为 CUDA
统一地址空间或 <strong>device</strong> 全局指针，并在分配时使用
cudaMalloc（CUDA 11 起已保证至少 256 字节对齐）。通过统一的对齐策略与
SoA 设计，可实现高效且跨平台的点云批量处理结构。</p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>-- AOS -- SOA -- 点云</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux命令指南</title>
    <url>/posts/a3123ebc/</url>
    <content><![CDATA[<h2 id="du"><strong>du</strong></h2>
<p>du
用于计算文件或目录的磁盘使用情况。它帮助我们确定需要压缩的文件夹大小，用于为进度条工具（如
pv）提供总大小信息。</p>
<p><strong>常用选项：</strong></p>
<ul>
<li>-s：显示文件夹的总大小，而不是列出子目录的大小。</li>
<li>-k：以 KiB（1024 字节为一单位）显示大小。</li>
<li>-b：以byte为单位显示大小（macOS 无此选项）</li>
</ul>
<hr>
<h2 id="tar"><strong>tar</strong></h2>
<p>tar
是一个打包工具，用于将多个文件/目录打包为单一的归档文件（不压缩），然后可以通过管道传递给其他压缩工具。</p>
<p><strong>常用选项：</strong></p>
<ul>
<li>-c：创建新的归档。</li>
<li>-v：显示详细信息（如每个被打包的文件名）。</li>
<li>-f：指定输出文件名，或用 - 表示将数据传递到标准输出（如管道）。</li>
<li>-：表示数据直接输出到标准输出（通常用于与其他工具结合，如 pv 和
pigz）。</li>
</ul>
<hr>
<h2 id="pv"><strong>pv</strong></h2>
<p>pv
是一个工具，用于监视通过管道传输的数据流，显示实时的进度、速度、已处理数据量和预计完成时间。</p>
<p><strong>常用选项：</strong></p>
<ul>
<li>-s <size>：指定总数据量（通常是以字节为单位）。若未指定，pv
会根据数据流猜测总量，但不准确。</size></li>
<li>$(...)：在命令中动态执行 du 并将输出传递给 pv。</li>
<li>awk '{print $1 * 1024}'：将 du 的输出（以 KiB
为单位）转换为字节。</li>
</ul>
<hr>
<h2 id="pigz"><strong>pigz</strong></h2>
<p>pigz 是 gzip 的多线程版本，支持更高的压缩效率和速度。</p>
<p><strong>常用选项：</strong></p>
<ul>
<li>-9：指定最高压缩级别。</li>
<li>-p <number>：指定线程数（根据你的 CPU 核心数设置）。</number></li>
</ul>
<hr>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">多线程压缩</span></span><br><span class="line">tar -cvf - ./QwQ-32B | pv -s $(du -sk ./QwQ-32B | awk &#x27;&#123;print $1 * 1024&#125;&#x27;) | pigz -9 -p 16 &gt; QwQ-32B.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">% </span><span class="language-bash">线程解压</span> </span><br><span class="line">pv QwQ-32B.tar.gz | pigz -dc -p 16 | tar -xvf -</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>-- Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>NEON指令集</title>
    <url>/posts/3d830de3/</url>
    <content><![CDATA[<p>  NEON（Advanced
SIMD）是ARM架构中用于向量化计算的指令集，广泛应用于手机、嵌入式设备和
Apple
Silicon等基于ARM的平台上。NEON提供了一种高效的方式同时处理多个数据元素，在多媒体处理、信号处理和机器学习中非常重要。</p>
<span id="more"></span>
<p>  NEON 是一种 SIMD（Single Instruction Multiple
Data）技术，可以使用单条指令同时对多个数据元素进行操作，其特点如下：</p>
<ul>
<li>数据并行：一次处理 4 个 32 位浮点数或 8 个 16 位整数。</li>
<li>支持整数和浮点运算。</li>
<li>提供了专用的 128
位向量寄存器（每个寄存器可以存储多个数据元素）。</li>
</ul>
<ol start="2" type="1">
<li>为什么使用 NEON 指令集？</li>
</ol>
<ul>
<li>性能提升：通过并行处理数据，大幅提升计算密集型任务的性能。</li>
<li>节省能耗：减少指令数，降低处理时间。</li>
<li>应用场景广泛：适合图像处理（滤波器、卷积）、信号处理（FFT、卷积编码）、矩阵计算等。</li>
</ul>
<ol start="3" type="1">
<li>NEON 向量化指令的基本结构</li>
</ol>
<p>NEON 指令是对多个数据元素同时操作的指令，分为以下几部分：</p>
<p>命名规则</p>
<p>NEON 指令的命名一般由 操作类型、数据类型 和 后缀 构成。</p>
<blockquote>
<p><操作符> <数据类型><后缀></后缀></数据类型></操作符></p>
</blockquote>
<pre><code>•   操作符：描述具体操作，如 vadd（向量加法）、vmul（向量乘法）。
•   数据类型：
•   q：表示 128 位宽度的寄存器（如 float32x4_t）。
•   d：表示 64 位宽度的寄存器（如 float32x2_t）。
•   后缀：指明数据类型或操作的特殊要求。
•   s：标量（Scalar）。
•   u：无符号整数（Unsigned）。
•   i：有符号整数（Integer）。
•   f：浮点数（Floating point）。</code></pre>
<p>示例 • vaddq_f32：加法，作用于 128 位（q）浮点型（f32）向量。 •
vmulq_u16：乘法，作用于 128 位无符号 16 位整数。 • vld1q_f32：加载数据到
128 位浮点向量寄存器。 • vst1q_f32：存储 128
位浮点向量寄存器的数据到内存。</p>
<p>⸻</p>
<ol start="4" type="1">
<li>NEON 数据类型</li>
</ol>
<p>在 ARM 的 NEON 指令集中，ARM 的 NEON 向量寄存器是 128
位宽，可以存储以下数据类型：</p>
<p>数据类型 元素宽度 元素个数（128 位寄存器） 8 位整数（有/无符号） 8 位
16 16 位整数（有/无符号） 16 位 8 32 位整数（有/无符号） 32 位 4 64
位整数（有/无符号） 64 位 2 32 位浮点数 32 位 4 64 位浮点数 64 位 2</p>
<p>⸻</p>
<ol start="5" type="1">
<li>常用指令</li>
</ol>
<p>以下是一些常用的 NEON 指令，分为四类：</p>
<ol type="1">
<li><p>加载与存储 • vld1q_f32(float32_t* ptr)：从内存加载 4 个 32
位浮点数到 128 位寄存器。 • vst1q_f32(float32_t* ptr, float32x4_t
val)：将 128 位寄存器的值存储到内存。</p></li>
<li><p>算术操作 • vaddq_f32(a, b)：加法，两个向量相加。 • vsubq_f32(a,
b)：减法，两个向量相减。 • vmulq_f32(a, b)：乘法，两个向量相乘。 •
vdivq_f32(a, b)：除法（部分平台支持）。</p></li>
<li><p>逻辑与比较 • vandq_u8(a, b)：按位与操作。 • vorrq_u8(a,
b)：按位或操作。 • veorq_u8(a, b)：按位异或操作。 • vcgeq_f32(a,
b)：比较是否大于等于。</p></li>
<li><p>数据处理 • vdupq_n_f32(value)：将标量复制到每个向量元素。 •
vcombine_f32(a, b)：将两个 64 位向量合并为一个 128 位向量。 •
vextq_f32(a, b, n)：提取 a 和 b 的元素形成新的向量。</p></li>
</ol>
<p>⸻</p>
<ol start="6" type="1">
<li>NEON 实例代码</li>
</ol>
<p>以下是一个简单的例子，演示如何使用 NEON 实现两个浮点数组的加法：
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;arm_neon.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">neon_vector_add</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a, <span class="type">const</span> <span class="type">float</span>* b, <span class="type">float</span>* result, <span class="type">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (; i + <span class="number">4</span> &lt;= n; i += <span class="number">4</span>) &#123; <span class="comment">// 每次处理 4 个元素</span></span><br><span class="line">        <span class="type">float32x4_t</span> va = <span class="built_in">vld1q_f32</span>(&amp;a[i]);    <span class="comment">// 加载数组 a 的数据</span></span><br><span class="line">        <span class="type">float32x4_t</span> vb = <span class="built_in">vld1q_f32</span>(&amp;b[i]);    <span class="comment">// 加载数组 b 的数据</span></span><br><span class="line">        <span class="type">float32x4_t</span> vc = <span class="built_in">vaddq_f32</span>(va, vb);   <span class="comment">// 执行向量加法</span></span><br><span class="line">        <span class="built_in">vst1q_f32</span>(&amp;result[i], vc);            <span class="comment">// 存储结果到 result</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理剩余的标量元素</span></span><br><span class="line">    <span class="keyword">for</span> (; i &lt; n; ++i) &#123;</span><br><span class="line">        result[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> n = <span class="number">8</span>;</span><br><span class="line">    <span class="type">float</span> a[n] = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>&#125;;</span><br><span class="line">    <span class="type">float</span> b[n] = &#123;<span class="number">8.0</span>, <span class="number">7.0</span>, <span class="number">6.0</span>, <span class="number">5.0</span>, <span class="number">4.0</span>, <span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>&#125;;</span><br><span class="line">    <span class="type">float</span> result[n] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">neon_vector_add</span>(a, b, result, n);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Result: &quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; result[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>⸻</p>
<ol start="7" type="1">
<li>性能测试（Benchmark）</li>
</ol>
<p>将上述代码与普通的标量实现进行对比，可以测量 NEON 的性能提升：
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">scalar_vector_add</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a, <span class="type">const</span> <span class="type">float</span>* b, <span class="type">float</span>* result, <span class="type">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        result[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">benchmark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> n = <span class="number">1000000</span>;</span><br><span class="line">    <span class="type">float</span>* a = <span class="keyword">new</span> <span class="type">float</span>[n];</span><br><span class="line">    <span class="type">float</span>* b = <span class="keyword">new</span> <span class="type">float</span>[n];</span><br><span class="line">    <span class="type">float</span>* result = <span class="keyword">new</span> <span class="type">float</span>[n];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        a[i] = i * <span class="number">1.0f</span>;</span><br><span class="line">        b[i] = i * <span class="number">0.5f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 标量实现</span></span><br><span class="line">    <span class="keyword">auto</span> start = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    <span class="built_in">scalar_vector_add</span>(a, b, result, n);</span><br><span class="line">    <span class="keyword">auto</span> end = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Scalar Time: &quot;</span></span><br><span class="line">              &lt;&lt; std::chrono::<span class="built_in">duration_cast</span>&lt;std::chrono::milliseconds&gt;(end - start).<span class="built_in">count</span>()</span><br><span class="line">              &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// NEON 实现</span></span><br><span class="line">    start = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    <span class="built_in">neon_vector_add</span>(a, b, result, n);</span><br><span class="line">    end = std::chrono::high_resolution_clock::<span class="built_in">now</span>();</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;NEON Time: &quot;</span></span><br><span class="line">              &lt;&lt; std::chrono::<span class="built_in">duration_cast</span>&lt;std::chrono::milliseconds&gt;(end - start).<span class="built_in">count</span>()</span><br><span class="line">              &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span>[] a;</span><br><span class="line">    <span class="keyword">delete</span>[] b;</span><br><span class="line">    <span class="keyword">delete</span>[] result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">benchmark</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>总结 + NEON 指令集的核心：高效向量化数据处理。</p>
<ul>
<li><p>重要基础：理解寄存器布局、指令命名规则以及典型数据操作。</p></li>
<li><p>实践技巧：逐步优化代码并衡量性能收益。</p></li>
</ul>
]]></content>
  </entry>
</search>
