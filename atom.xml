<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老学庵</title>
  <icon>https://wcjb.github.io/icon.png</icon>
  <subtitle>天行健，君子以自强不息；地势坤，君子以厚德载物！</subtitle>
  <link href="https://wcjb.github.io/atom.xml" rel="self"/>
  
  <link href="https://wcjb.github.io/"/>
  <updated>2025-04-24T07:53:24.527Z</updated>
  <id>https://wcjb.github.io/</id>
  
  <author>
    <name>殉道者</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FlashAttention从原理到cuda实现</title>
    <link href="https://wcjb.github.io/posts/6721533d/"/>
    <id>https://wcjb.github.io/posts/6721533d/</id>
    <published>2025-04-24T07:45:33.000Z</published>
    <updated>2025-04-24T07:53:24.527Z</updated>
    
    
    <summary type="html">&lt;p&gt;  &lt;code&gt;Flash Attention&lt;/code&gt;是一种基于硬件设计的注意力加速策略，原始论文
&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.14135&quot;&gt;FlashAttention:
Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="-- Flash Attenttion -- Attention -- 自注意力" scheme="https://wcjb.github.io/tags/Flash-Attenttion-Attention-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title>理解Attention从起源到MHA、MQA和GQA</title>
    <link href="https://wcjb.github.io/posts/2f67dcd1/"/>
    <id>https://wcjb.github.io/posts/2f67dcd1/</id>
    <published>2025-04-20T05:55:43.000Z</published>
    <updated>2025-04-23T07:47:24.567Z</updated>
    
    
    <summary type="html">&lt;p&gt;  Attention
模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head
Attention）、MQA（Multi-Query Attention）和 GQA（Grouped-Query
Attention）这一路线的思路和做法被很多主流模型所采用，因此简单地梳理一些这几个变体的思路和做法，以及会涉及到的KV
Cache相关内容。思路比较直白，但也有一些细节和原理值得思考。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="-- Attention -- MHA -- GQA -- MQA -- LLM" scheme="https://wcjb.github.io/tags/Attention-MHA-GQA-MQA-LLM/"/>
    
  </entry>
  
  <entry>
    <title>理解自注意力机制</title>
    <link href="https://wcjb.github.io/posts/15775ef77/"/>
    <id>https://wcjb.github.io/posts/15775ef77/</id>
    <published>2025-04-10T09:23:36.000Z</published>
    <updated>2025-04-18T06:01:13.416Z</updated>
    
    
    <summary type="html">&lt;p&gt;  自Transformer架构的开山之作《&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You
Need&lt;/a&gt;》发表以来，自注意力机制已成为深度学习模型的基石技术，尤其在自然语言处理领域展现革命性突破。鉴于该机制已渗透至各类先进模型架构，深入理解其运作原理变得至关重要。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="自注意力机制" scheme="https://wcjb.github.io/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
    <category term="大语言模型" scheme="https://wcjb.github.io/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>LLM之KVCache</title>
    <link href="https://wcjb.github.io/posts/e16fb9d/"/>
    <id>https://wcjb.github.io/posts/e16fb9d/</id>
    <published>2025-04-08T06:25:49.000Z</published>
    <updated>2025-04-23T06:24:07.860Z</updated>
    
    
    <summary type="html">&lt;p&gt;  在Transformer架构的生成式模型（如GPT系列）中，推理过程需要逐个生成token。传统方式每次生成都需重新计算所有历史token的注意力信息，导致计算复杂度达到O(n²)。&lt;strong&gt;KV
Cache技术通过缓存历史token的Key和Value矩阵&lt;/strong&gt;，将后续生成的计算复杂度降至O(n)，实现推理加速，本质上是一种用空间来换取时间的加速策略。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="-- Trasformer -- 大模型推理 -- 性能优化" scheme="https://wcjb.github.io/tags/Trasformer-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>LLM之位置编码</title>
    <link href="https://wcjb.github.io/posts/15775ef36/"/>
    <id>https://wcjb.github.io/posts/15775ef36/</id>
    <published>2025-03-31T09:11:09.000Z</published>
    <updated>2025-04-18T06:01:35.439Z</updated>
    
    
    <summary type="html">&lt;p&gt;  在自然语言处理的任务中，位置编码是帮助模型理解序列中每个单词或词片（token）在序列中的位置的一种机制。这是因为像
Transformer
这样的架构本质上是无序的，它们通过注意力机制处理整个序列中的所有元素，但并不能直接感知这些元素在序列中的顺序。因此，我们需要将序列的位置信息编码进模型的输入，编码的方式有绝对位置编码和相对位置编码。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="— 位置编码 - LLM - 模型训练" scheme="https://wcjb.github.io/tags/%E2%80%94-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-LLM-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>LLM之模型训练Tricks</title>
    <link href="https://wcjb.github.io/posts/15775ef35/"/>
    <id>https://wcjb.github.io/posts/15775ef35/</id>
    <published>2025-03-31T09:11:09.000Z</published>
    <updated>2025-04-18T06:01:25.456Z</updated>
    
    
    <summary type="html">&lt;p&gt;  学习LLM过程中的经典策略记录。&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="LLM" scheme="https://wcjb.github.io/tags/LLM/"/>
    
    <category term="模型训练" scheme="https://wcjb.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>机器视觉之基本概念</title>
    <link href="https://wcjb.github.io/posts/781628f/"/>
    <id>https://wcjb.github.io/posts/781628f/</id>
    <published>2025-02-18T07:21:39.000Z</published>
    <updated>2025-04-18T06:02:55.472Z</updated>
    
    
    <summary type="html">&lt;p&gt;  主要记录机器视觉领域的一些基本概念及一些实践中的tricks。&lt;/p&gt;</summary>
    
    
    
    <category term="机器视觉" scheme="https://wcjb.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="相机" scheme="https://wcjb.github.io/tags/%E7%9B%B8%E6%9C%BA/"/>
    
    <category term="视觉" scheme="https://wcjb.github.io/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像" scheme="https://wcjb.github.io/tags/%E5%9B%BE%E5%83%8F/"/>
    
  </entry>
  
  <entry>
    <title>点云内存布局</title>
    <link href="https://wcjb.github.io/posts/900a74c0/"/>
    <id>https://wcjb.github.io/posts/900a74c0/</id>
    <published>2023-03-29T01:56:51.000Z</published>
    <updated>2025-04-29T02:04:32.155Z</updated>
    
    
    <summary type="html">&lt;p&gt;  点云数据可采用两种基本布局：&lt;strong&gt;结构体数组&lt;/strong&gt; (AoS, Array
of Structures) 和 &lt;strong&gt;数组结构&lt;/strong&gt; (SoA, Structure of
Arrays)。&lt;/p&gt;</summary>
    
    
    
    
    <category term="-- AOS -- SOA -- 点云" scheme="https://wcjb.github.io/tags/AOS-SOA-%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>LLM之余弦退火学习率</title>
    <link href="https://wcjb.github.io/posts/19c3723a/"/>
    <id>https://wcjb.github.io/posts/19c3723a/</id>
    <published>2023-02-15T06:06:09.000Z</published>
    <updated>2025-04-18T06:01:39.437Z</updated>
    
    
    <summary type="html">&lt;p&gt;  学习率这个概念在非线性优化中经常出现，在深度学习中模型在反向传播阶段严重依赖于损失函数梯度的链式传播，为了更好的控制参数更新的步长，引入了学习率的概念：
&lt;span class=&quot;math display&quot;&gt;&#92;[
w_{new} = w_{old} - &#92;eta &#92; &#92;Delta J(w)
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/categories/LLM/"/>
    
    
    <category term="LLM" scheme="https://wcjb.github.io/tags/LLM/"/>
    
    <category term="学习率" scheme="https://wcjb.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
    <category term="模型训练" scheme="https://wcjb.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
    <category term="余弦退火" scheme="https://wcjb.github.io/tags/%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB/"/>
    
  </entry>
  
  <entry>
    <title>傅立叶变换</title>
    <link href="https://wcjb.github.io/posts/684f5543/"/>
    <id>https://wcjb.github.io/posts/684f5543/</id>
    <published>2022-04-14T09:26:09.000Z</published>
    <updated>2025-04-18T06:03:25.014Z</updated>
    
    
    <summary type="html">&lt;p&gt;  希尔伯特空间是一个完备的内积空间，其标准正交函数系，直观来看就是向量空间中&lt;code&gt;基&lt;/code&gt;的延伸。其为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。下文中我们将通过希尔伯特空间的标准正交函数系推导周期函数和有限区间上函数的傅立叶级数表示，并进一步推出傅里叶积分来表示无穷区间的非周期函数，最后引入复数形式的傅立叶积分，引出傅立叶变换。在这一系列推导中，鉴于篇幅，主动略去了一些比较关键的部分，比如&lt;span class=&quot;math inline&quot;&gt;&#92;(f(x)&#92;)&lt;/span&gt;可积性及级数收敛性的讨论，有兴趣的读者可以在了解大致原理后，进行细致的理论推导以作补充。为了便于理解希尔伯特空间的概念，引用维基百科中的定义：&lt;/p&gt;</summary>
    
    
    
    <category term="数学理论" scheme="https://wcjb.github.io/categories/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"/>
    
    
    <category term="傅立叶" scheme="https://wcjb.github.io/tags/%E5%82%85%E7%AB%8B%E5%8F%B6/"/>
    
    <category term="希尔伯特空间" scheme="https://wcjb.github.io/tags/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
    
    <category term="正交函数系" scheme="https://wcjb.github.io/tags/%E6%AD%A3%E4%BA%A4%E5%87%BD%E6%95%B0%E7%B3%BB/"/>
    
  </entry>
  
  <entry>
    <title>奇异值分解</title>
    <link href="https://wcjb.github.io/posts/5a1d36e7/"/>
    <id>https://wcjb.github.io/posts/5a1d36e7/</id>
    <published>2021-03-12T15:21:19.000Z</published>
    <updated>2025-04-18T06:03:18.409Z</updated>
    
    
    <summary type="html">&lt;p&gt;  奇异值分解（Singular Value
Decomposition，SVD）是线性代数中一种重要的矩阵分解方法，区别于只适用于实对称矩阵的特征分解方法，奇异值分解可对任意实矩阵进行分解。&lt;/p&gt;</summary>
    
    
    
    <category term="数学理论" scheme="https://wcjb.github.io/categories/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"/>
    
    
    <category term="svd" scheme="https://wcjb.github.io/tags/svd/"/>
    
    <category term="特征分解" scheme="https://wcjb.github.io/tags/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/"/>
    
    <category term="特征向量" scheme="https://wcjb.github.io/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>卡尔曼滤波</title>
    <link href="https://wcjb.github.io/posts/b654ab75/"/>
    <id>https://wcjb.github.io/posts/b654ab75/</id>
    <published>2021-03-12T14:21:19.000Z</published>
    <updated>2025-04-18T06:03:09.927Z</updated>
    
    
    <summary type="html">&lt;p&gt;  &lt;strong&gt;卡尔曼滤波&lt;/strong&gt;（Kalman filter）是一种高效率的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/递归滤波器&quot;&gt;递归滤波器&lt;/a&gt;（&lt;a href=&quot;https://www.wikiwand.com/zh-hans/自迴歸模型&quot;&gt;自回归&lt;/a&gt;滤波器），它能够从一系列的不完全及包含&lt;a href=&quot;https://www.wikiwand.com/zh-hans/雜訊_(通訊學)&quot;&gt;杂讯&lt;/a&gt;的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/测量&quot;&gt;测量&lt;/a&gt;中，估计&lt;a href=&quot;https://www.wikiwand.com/zh-hans/动态系统&quot;&gt;动态系统&lt;/a&gt;的状态。卡尔曼滤波会根据各测量量在不同时间下的值，考虑各时间下的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/联合分布&quot;&gt;联合分布&lt;/a&gt;，再产生对未知变数的估计，因此会比只以单一测量量为基础的估计方式要准。卡尔曼滤波得名自主要贡献者之一的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/鲁道夫·卡尔曼&quot;&gt;鲁道夫·卡尔曼&lt;/a&gt;，最早用于解决阿波罗计划的轨道预测问题。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://wcjb.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="卡尔曼滤波" scheme="https://wcjb.github.io/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/"/>
    
    <category term="马尔可夫链" scheme="https://wcjb.github.io/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/"/>
    
  </entry>
  
</feed>
