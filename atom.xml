<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老学庵</title>
  <icon>https://wcjb.github.io/icon.png</icon>
  <subtitle>天行健，君子以自强不息；地势坤，君子以厚德载物！</subtitle>
  <link href="https://wcjb.github.io/atom.xml" rel="self"/>
  
  <link href="https://wcjb.github.io/"/>
  <updated>2025-04-08T06:06:17.590Z</updated>
  <id>https://wcjb.github.io/</id>
  
  <author>
    <name>殉道者</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM之位置编码</title>
    <link href="https://wcjb.github.io/posts/15775ef36/"/>
    <id>https://wcjb.github.io/posts/15775ef36/</id>
    <published>2025-03-31T09:11:09.000Z</published>
    <updated>2025-04-08T06:06:17.590Z</updated>
    
    
    <summary type="html">&lt;p&gt;  在自然语言处理的任务中，位置编码是帮助模型理解序列中每个单词或词片（token）在序列中的位置的一种机制。这是因为像
Transformer
这样的架构本质上是无序的，它们通过注意力机制处理整个序列中的所有元素，但并不能直接感知这些元素在序列中的顺序。因此，我们需要将序列的位置信息编码进模型的输入，编码的方式有绝对位置编码和相对位置编码。&lt;/p&gt;</summary>
    
    
    
    
    <category term="— 位置编码 - LLM - 模型训练" scheme="https://wcjb.github.io/tags/%E2%80%94-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-LLM-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>LLM之模型训练Tricks</title>
    <link href="https://wcjb.github.io/posts/15775ef35/"/>
    <id>https://wcjb.github.io/posts/15775ef35/</id>
    <published>2025-03-31T09:11:09.000Z</published>
    <updated>2025-04-08T06:00:55.520Z</updated>
    
    
    <summary type="html">&lt;p&gt;  学习LLM过程中的经典策略记录。&lt;/p&gt;</summary>
    
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/tags/LLM/"/>
    
    <category term="模型训练" scheme="https://wcjb.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>机器视觉之基本概念</title>
    <link href="https://wcjb.github.io/posts/781628f/"/>
    <id>https://wcjb.github.io/posts/781628f/</id>
    <published>2025-02-18T07:21:39.000Z</published>
    <updated>2025-02-19T02:03:52.610Z</updated>
    
    
    <summary type="html">&lt;p&gt;  主要记录机器视觉领域的一些基本概念及一些实践中的tricks。&lt;/p&gt;</summary>
    
    
    
    
    <category term="相机" scheme="https://wcjb.github.io/tags/%E7%9B%B8%E6%9C%BA/"/>
    
    <category term="视觉" scheme="https://wcjb.github.io/tags/%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像" scheme="https://wcjb.github.io/tags/%E5%9B%BE%E5%83%8F/"/>
    
  </entry>
  
  <entry>
    <title>初探Transformer架构</title>
    <link href="https://wcjb.github.io/posts/a65e2a56/"/>
    <id>https://wcjb.github.io/posts/a65e2a56/</id>
    <published>2025-02-17T01:29:32.000Z</published>
    <updated>2025-02-17T01:44:19.482Z</updated>
    
    
      
      
        
        
    <summary type="html">
</summary>
        
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LLM之余弦退火学习率</title>
    <link href="https://wcjb.github.io/posts/19c3723a/"/>
    <id>https://wcjb.github.io/posts/19c3723a/</id>
    <published>2023-02-15T06:06:09.000Z</published>
    <updated>2025-04-01T01:39:12.811Z</updated>
    
    
    <summary type="html">&lt;p&gt;  学习率这个概念在非线性优化中经常出现，在深度学习中模型在反向传播阶段严重依赖于损失函数梯度的链式传播，为了更好的控制参数更新的步长，引入了学习率的概念：
&lt;span class=&quot;math display&quot;&gt;&#92;[
w_{new} = w_{old} - &#92;eta &#92; &#92;Delta J(w)
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="LLM" scheme="https://wcjb.github.io/tags/LLM/"/>
    
    <category term="学习率" scheme="https://wcjb.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
    <category term="模型训练" scheme="https://wcjb.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    
    <category term="余弦退火" scheme="https://wcjb.github.io/tags/%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB/"/>
    
  </entry>
  
  <entry>
    <title>傅立叶变换</title>
    <link href="https://wcjb.github.io/posts/684f5543/"/>
    <id>https://wcjb.github.io/posts/684f5543/</id>
    <published>2022-04-14T09:26:09.000Z</published>
    <updated>2025-02-17T07:00:47.324Z</updated>
    
    
    <summary type="html">&lt;p&gt;  希尔伯特空间是一个完备的内积空间，其标准正交函数系，直观来看就是向量空间中&lt;code&gt;基&lt;/code&gt;的延伸。其为基于任意正交系上的多项式表示的傅立叶级数和傅立叶变换提供了一种有效的表述方式，而这也是泛函分析的核心概念之一。下文中我们将通过希尔伯特空间的标准正交函数系推导周期函数和有限区间上函数的傅立叶级数表示，并进一步推出傅里叶积分来表示无穷区间的非周期函数，最后引入复数形式的傅立叶积分，引出傅立叶变换。在这一系列推导中，鉴于篇幅，主动略去了一些比较关键的部分，比如&lt;span class=&quot;math inline&quot;&gt;&#92;(f(x)&#92;)&lt;/span&gt;可积性及级数收敛性的讨论，有兴趣的读者可以在了解大致原理后，进行细致的理论推导以作补充。为了便于理解希尔伯特空间的概念，引用维基百科中的定义：&lt;/p&gt;</summary>
    
    
    
    
    <category term="傅立叶" scheme="https://wcjb.github.io/tags/%E5%82%85%E7%AB%8B%E5%8F%B6/"/>
    
    <category term="希尔伯特空间" scheme="https://wcjb.github.io/tags/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
    
    <category term="正交函数系" scheme="https://wcjb.github.io/tags/%E6%AD%A3%E4%BA%A4%E5%87%BD%E6%95%B0%E7%B3%BB/"/>
    
  </entry>
  
  <entry>
    <title>奇异值分解</title>
    <link href="https://wcjb.github.io/posts/5a1d36e7/"/>
    <id>https://wcjb.github.io/posts/5a1d36e7/</id>
    <published>2021-03-12T15:21:19.000Z</published>
    <updated>2025-02-14T09:00:22.174Z</updated>
    
    
    <summary type="html">&lt;p&gt;  奇异值分解（Singular Value
Decomposition，SVD）是线性代数中一种重要的矩阵分解方法，区别于只适用于实对称矩阵的特征分解方法，奇异值分解可对任意实矩阵进行分解。&lt;/p&gt;</summary>
    
    
    
    
    <category term="svd" scheme="https://wcjb.github.io/tags/svd/"/>
    
    <category term="特征分解" scheme="https://wcjb.github.io/tags/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/"/>
    
    <category term="特征向量" scheme="https://wcjb.github.io/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>卡尔曼滤波</title>
    <link href="https://wcjb.github.io/posts/b654ab75/"/>
    <id>https://wcjb.github.io/posts/b654ab75/</id>
    <published>2021-03-12T14:21:19.000Z</published>
    <updated>2025-02-17T07:00:11.891Z</updated>
    
    
    <summary type="html">&lt;p&gt;  &lt;strong&gt;卡尔曼滤波&lt;/strong&gt;（Kalman filter）是一种高效率的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/递归滤波器&quot;&gt;递归滤波器&lt;/a&gt;（&lt;a href=&quot;https://www.wikiwand.com/zh-hans/自迴歸模型&quot;&gt;自回归&lt;/a&gt;滤波器），它能够从一系列的不完全及包含&lt;a href=&quot;https://www.wikiwand.com/zh-hans/雜訊_(通訊學)&quot;&gt;杂讯&lt;/a&gt;的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/测量&quot;&gt;测量&lt;/a&gt;中，估计&lt;a href=&quot;https://www.wikiwand.com/zh-hans/动态系统&quot;&gt;动态系统&lt;/a&gt;的状态。卡尔曼滤波会根据各测量量在不同时间下的值，考虑各时间下的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/联合分布&quot;&gt;联合分布&lt;/a&gt;，再产生对未知变数的估计，因此会比只以单一测量量为基础的估计方式要准。卡尔曼滤波得名自主要贡献者之一的&lt;a href=&quot;https://www.wikiwand.com/zh-hans/鲁道夫·卡尔曼&quot;&gt;鲁道夫·卡尔曼&lt;/a&gt;，最早用于解决阿波罗计划的轨道预测问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="卡尔曼滤波" scheme="https://wcjb.github.io/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/"/>
    
    <category term="马尔可夫链" scheme="https://wcjb.github.io/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/"/>
    
  </entry>
  
</feed>
